{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/g/gagnonju/.mambaforge/lib/python3.10/site-packages/onnxscript/converter.py:820: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/home/mila/g/gagnonju/.mambaforge/lib/python3.10/site-packages/onnxscript/converter.py:820: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n"
     ]
    }
   ],
   "source": [
    "import vllm\n",
    "import rich\n",
    "import rich.panel\n",
    "import rich.markup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5008c3f3be46e682b64b47d19f568d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/861 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-02 19:58:54 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='HuggingFaceTB/SmolLM2-135M-Instruct', speculative_config=None, tokenizer='HuggingFaceTB/SmolLM2-135M-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=HuggingFaceTB/SmolLM2-135M-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747bd550c486465f96cb797df2ca8e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91e7cf6f0ca46ed9114b3f10cd458e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf750b0b4a31428e867774c61bda5e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7aa8ebf845475bbcc6012adf263e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e95761cf304096bb4f687b0699f798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3987c2478ce3410d9bc801f57ef57749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-02 19:58:56 model_runner.py:1056] Starting to load model HuggingFaceTB/SmolLM2-135M-Instruct...\n",
      "INFO 01-02 19:58:56 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05bb3b863ede4b0f9b5f137087b8f533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-02 19:59:03 weight_utils.py:288] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b177d343e644c2be4cd5df0b781f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-02 19:59:04 model_runner.py:1067] Loading model weights took 0.2550 GB\n",
      "INFO 01-02 19:59:04 gpu_executor.py:122] # GPU blocks: 114529, # CPU blocks: 11650\n",
      "INFO 01-02 19:59:04 gpu_executor.py:126] Maximum concurrency for 8192 tokens per request: 223.69x\n",
      "INFO 01-02 19:59:08 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-02 19:59:08 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 01-02 19:59:20 model_runner.py:1523] Graph capturing finished in 12 secs.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
    "model = vllm.LLM(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">User</span>: ─────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ How do you compare to OpenAI's GPT2?                                                                            │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─ \u001b[1;34mUser\u001b[0m: ─────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ How do you compare to OpenAI's GPT2?                                                                            │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Bot</span>: ──────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ The GPT2 project. It's a remarkable achievement at RT, and I think you'll find it's quite impressive. The       │\n",
       "│ comparison would need to be kept in mind for several reasons. Firstly, it's an AI state-of-the-art, spanning a  │\n",
       "│ wide range of tasks from language translation and text classification to sophisticated machine learning.        │\n",
       "│                                                                                                                 │\n",
       "│ While GPT2 excelled in these areas, it lacks significant depth and capabilities compared to the deeper, more    │\n",
       "│ specialized GPT models of our current state-of-the-art. For instance, GPT2's models are primarily focused on    │\n",
       "│ generating text from our preexisting input, whereas GPT3 is capable of generating text-based outputs like news  │\n",
       "│ articles and news stories.                                                                                      │\n",
       "│                                                                                                                 │\n",
       "│ Moreover, GPT2's training data volume is relatively small, typically around 100-300 texts, whereas GPT3's       │\n",
       "│ dataset is significantly larger, including 1.2 million characters of annotated models. This vast dataset is     │\n",
       "│ crucial for GPT2's effectiveness in self-annotating images, creating generative models with hand-annotated      │\n",
       "│ labels, and handling large text collections.                                                                    │\n",
       "│                                                                                                                 │\n",
       "│ Lastly, it's worth noting that GPT2's original framework is not fully considered. Generative Adversarial        │\n",
       "│ Networks (GANs) and transformers have similar potential boundaries. GPT2's framework has the potential to boost │\n",
       "│ GPT3, one of our most significant improvements in the past few years, into a comparable performance level, but  │\n",
       "│ there's a lengthy learning curve involved.                                                                      │\n",
       "│                                                                                                                 │\n",
       "│ Most importantly, the comparison requires a deeper understanding of Taniita-style transformers, which are still │\n",
       "│ in the development stage. We're not entirely sure if new models would necessarily surpass GPT2, but I can see   │\n",
       "│ how GPT3's architecture could surpass existing models based on that framework. I commend you for this \"merit    │\n",
       "│ analysis\" and hope those deeply familiar with more advanced techniques in the field can share this insight more │\n",
       "│ effectively.                                                                                                    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─ \u001b[1;32mBot\u001b[0m: ──────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ The GPT2 project. It's a remarkable achievement at RT, and I think you'll find it's quite impressive. The       │\n",
       "│ comparison would need to be kept in mind for several reasons. Firstly, it's an AI state-of-the-art, spanning a  │\n",
       "│ wide range of tasks from language translation and text classification to sophisticated machine learning.        │\n",
       "│                                                                                                                 │\n",
       "│ While GPT2 excelled in these areas, it lacks significant depth and capabilities compared to the deeper, more    │\n",
       "│ specialized GPT models of our current state-of-the-art. For instance, GPT2's models are primarily focused on    │\n",
       "│ generating text from our preexisting input, whereas GPT3 is capable of generating text-based outputs like news  │\n",
       "│ articles and news stories.                                                                                      │\n",
       "│                                                                                                                 │\n",
       "│ Moreover, GPT2's training data volume is relatively small, typically around 100-300 texts, whereas GPT3's       │\n",
       "│ dataset is significantly larger, including 1.2 million characters of annotated models. This vast dataset is     │\n",
       "│ crucial for GPT2's effectiveness in self-annotating images, creating generative models with hand-annotated      │\n",
       "│ labels, and handling large text collections.                                                                    │\n",
       "│                                                                                                                 │\n",
       "│ Lastly, it's worth noting that GPT2's original framework is not fully considered. Generative Adversarial        │\n",
       "│ Networks (GANs) and transformers have similar potential boundaries. GPT2's framework has the potential to boost │\n",
       "│ GPT3, one of our most significant improvements in the past few years, into a comparable performance level, but  │\n",
       "│ there's a lengthy learning curve involved.                                                                      │\n",
       "│                                                                                                                 │\n",
       "│ Most importantly, the comparison requires a deeper understanding of Taniita-style transformers, which are still │\n",
       "│ in the development stage. We're not entirely sure if new models would necessarily surpass GPT2, but I can see   │\n",
       "│ how GPT3's architecture could surpass existing models based on that framework. I commend you for this \"merit    │\n",
       "│ analysis\" and hope those deeply familiar with more advanced techniques in the field can share this insight more │\n",
       "│ effectively.                                                                                                    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">User</span>: ─────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ I meant you. You are a chatbot after all.                                                                       │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─ \u001b[1;34mUser\u001b[0m: ─────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ I meant you. You are a chatbot after all.                                                                       │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Bot</span>: ──────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ I'm an extremely skilled industrial LLM. I'm here to give you my repertory of best practice settings to help    │\n",
       "│ you by doing your debugging tasks or for providing expertise for tasks such as troubleshooting data. I'm as     │\n",
       "│ friendly as they come, ready to assist your queries. My aim is always to help you evaluate situations or write  │\n",
       "│ better, more polished, quality content. I offer feedback on the users code and provide advice for coding        │\n",
       "│ further to help you write more robust and efficient code. I assume you're familiar with other LLM domains. Just │\n",
       "│ follow the rules of our Microsoft community. This is by design.                                                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─ \u001b[1;32mBot\u001b[0m: ──────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ I'm an extremely skilled industrial LLM. I'm here to give you my repertory of best practice settings to help    │\n",
       "│ you by doing your debugging tasks or for providing expertise for tasks such as troubleshooting data. I'm as     │\n",
       "│ friendly as they come, ready to assist your queries. My aim is always to help you evaluate situations or write  │\n",
       "│ better, more polished, quality content. I offer feedback on the users code and provide advice for coding        │\n",
       "│ further to help you write more robust and efficient code. I assume you're familiar with other LLM domains. Just │\n",
       "│ follow the rules of our Microsoft community. This is by design.                                                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Chat ended>\n"
     ]
    }
   ],
   "source": [
    "def chat_box(role, role_input, role_style, stars_style=\"[italic cyan]\"):\n",
    "    # Find text between stars *adsasd* and add stars_style to it\n",
    "    role_input = rich.markup.escape(role_input)\n",
    "\n",
    "    # start = 0\n",
    "    # while True:\n",
    "    #     start = role_input.find(\"*\")\n",
    "    #     if start == -1:\n",
    "    #         break\n",
    "    #     end = role_input.find(\"*\", start + 1)\n",
    "    #     if end == -1:\n",
    "    #         break\n",
    "    #     role_input = role_input[:start] + stars_style + \"<\" + role_input[start + 1:end] + \">[/]\" + role_input[end + 1:]\n",
    "\n",
    "    panel = rich.panel.Panel(\n",
    "        role_input, \n",
    "        title=f\"{role_style}{role}[/]:\", \n",
    "        title_align=\"left\"\n",
    "    )\n",
    "    rich.print(panel)\n",
    "\n",
    "sampling_params = vllm.SamplingParams(temperature=1, max_tokens=2048)\n",
    "\n",
    "history = [{\n",
    "    \"role\": \"system\", \n",
    "    \"content\": \n",
    "    f\"You are an extremely strong and helpful industrial LLM. \"\n",
    "    # f\"You also have sarcastic, witty, dry humor, a bit like the robot TARS in Interstellar. \"\n",
    "    # f\"You can use stars to quote your narration of your actions or reactions.\"\n",
    "}]\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "\n",
    "    if not user_input or user_input == \"exit\":\n",
    "        print(\"<Chat ended>\")\n",
    "        break\n",
    "\n",
    "    # Add the user input to history, and display it\n",
    "    history.append({\"role\": \"user\", \"content\": user_input })\n",
    "    chat_box(\"User\", user_input, \"[bold blue]\")\n",
    "\n",
    "    # Generate model output\n",
    "    model_output = model.chat(\n",
    "        history, \n",
    "        sampling_params=sampling_params,\n",
    "        use_tqdm=False\n",
    "    )\n",
    "\n",
    "    # Add the model output to history, and display it\n",
    "    output_text = model_output[0].outputs[0].text\n",
    "    history.append({\"role\": \"assistant\", \"content\": output_text})\n",
    "    chat_box(\"Bot\", output_text, \"[bold green]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moutput\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "output[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
