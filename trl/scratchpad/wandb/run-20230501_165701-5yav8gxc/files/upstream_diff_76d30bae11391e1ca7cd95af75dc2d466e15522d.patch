diff --git a/first_approach/README.md b/archive/first_approach/README.md
similarity index 100%
rename from first_approach/README.md
rename to archive/first_approach/README.md
diff --git a/first_approach/STATE_STEP_2.md b/archive/first_approach/STATE_STEP_2.md
similarity index 100%
rename from first_approach/STATE_STEP_2.md
rename to archive/first_approach/STATE_STEP_2.md
diff --git a/first_approach/STATE_STEP_3.md b/archive/first_approach/STATE_STEP_3.md
similarity index 100%
rename from first_approach/STATE_STEP_3.md
rename to archive/first_approach/STATE_STEP_3.md
diff --git a/first_approach/abstract.md b/archive/first_approach/abstract.md
similarity index 100%
rename from first_approach/abstract.md
rename to archive/first_approach/abstract.md
diff --git a/first_approach/bin_refine.py b/archive/first_approach/bin_refine.py
similarity index 100%
rename from first_approach/bin_refine.py
rename to archive/first_approach/bin_refine.py
diff --git a/first_approach/console.py b/archive/first_approach/console.py
similarity index 100%
rename from first_approach/console.py
rename to archive/first_approach/console.py
diff --git a/first_approach/constants.py b/archive/first_approach/constants.py
similarity index 100%
rename from first_approach/constants.py
rename to archive/first_approach/constants.py
diff --git a/first_approach/data_synthetic_arithmetic_tokenizer.py b/archive/first_approach/data_synthetic_arithmetic_tokenizer.py
similarity index 100%
rename from first_approach/data_synthetic_arithmetic_tokenizer.py
rename to archive/first_approach/data_synthetic_arithmetic_tokenizer.py
diff --git a/first_approach/fast_ckpt_reader.py b/archive/first_approach/fast_ckpt_reader.py
similarity index 100%
rename from first_approach/fast_ckpt_reader.py
rename to archive/first_approach/fast_ckpt_reader.py
diff --git a/first_approach/general_shared_constants.py b/archive/first_approach/general_shared_constants.py
similarity index 100%
rename from first_approach/general_shared_constants.py
rename to archive/first_approach/general_shared_constants.py
diff --git a/first_approach/marginal.py b/archive/first_approach/marginal.py
similarity index 100%
rename from first_approach/marginal.py
rename to archive/first_approach/marginal.py
diff --git a/first_approach/mypy.ini b/archive/first_approach/mypy.ini
similarity index 100%
rename from first_approach/mypy.ini
rename to archive/first_approach/mypy.ini
diff --git a/first_approach/ppo.py b/archive/first_approach/ppo.py
similarity index 100%
rename from first_approach/ppo.py
rename to archive/first_approach/ppo.py
diff --git a/first_approach/pretrain.py b/archive/first_approach/pretrain.py
similarity index 100%
rename from first_approach/pretrain.py
rename to archive/first_approach/pretrain.py
diff --git a/first_approach/rl/__init__.py b/archive/first_approach/rl/__init__.py
similarity index 100%
rename from first_approach/rl/__init__.py
rename to archive/first_approach/rl/__init__.py
diff --git a/first_approach/rl/ppo.py b/archive/first_approach/rl/ppo.py
similarity index 100%
rename from first_approach/rl/ppo.py
rename to archive/first_approach/rl/ppo.py
diff --git a/first_approach/rl_baselines.py b/archive/first_approach/rl_baselines.py
similarity index 100%
rename from first_approach/rl_baselines.py
rename to archive/first_approach/rl_baselines.py
diff --git a/first_approach/script_data_arithmetic_parse.py b/archive/first_approach/script_data_arithmetic_parse.py
similarity index 100%
rename from first_approach/script_data_arithmetic_parse.py
rename to archive/first_approach/script_data_arithmetic_parse.py
diff --git a/first_approach/script_data_to_h5.py b/archive/first_approach/script_data_to_h5.py
similarity index 100%
rename from first_approach/script_data_to_h5.py
rename to archive/first_approach/script_data_to_h5.py
diff --git a/first_approach/train_utils.py b/archive/first_approach/train_utils.py
similarity index 100%
rename from first_approach/train_utils.py
rename to archive/first_approach/train_utils.py
diff --git a/first_approach/wandb_config.json b/archive/first_approach/wandb_config.json
similarity index 100%
rename from first_approach/wandb_config.json
rename to archive/first_approach/wandb_config.json
diff --git a/our_scratchpad/archive/ddps.py b/archive/our_scratchpad/archive/ddps.py
similarity index 100%
rename from our_scratchpad/archive/ddps.py
rename to archive/our_scratchpad/archive/ddps.py
diff --git a/our_scratchpad/archive/length_complexity.ipynb b/archive/our_scratchpad/archive/length_complexity.ipynb
similarity index 100%
rename from our_scratchpad/archive/length_complexity.ipynb
rename to archive/our_scratchpad/archive/length_complexity.ipynb
diff --git a/our_scratchpad/archive/nb_beam_search_speed.ipynb b/archive/our_scratchpad/archive/nb_beam_search_speed.ipynb
similarity index 100%
rename from our_scratchpad/archive/nb_beam_search_speed.ipynb
rename to archive/our_scratchpad/archive/nb_beam_search_speed.ipynb
diff --git a/our_scratchpad/archive/nb_ckpt_pl.ipynb b/archive/our_scratchpad/archive/nb_ckpt_pl.ipynb
similarity index 100%
rename from our_scratchpad/archive/nb_ckpt_pl.ipynb
rename to archive/our_scratchpad/archive/nb_ckpt_pl.ipynb
diff --git a/our_scratchpad/archive/nb_data_lengths.ipynb b/archive/our_scratchpad/archive/nb_data_lengths.ipynb
similarity index 100%
rename from our_scratchpad/archive/nb_data_lengths.ipynb
rename to archive/our_scratchpad/archive/nb_data_lengths.ipynb
diff --git a/our_scratchpad/archive/nb_explore_gsm8k.ipynb b/archive/our_scratchpad/archive/nb_explore_gsm8k.ipynb
similarity index 100%
rename from our_scratchpad/archive/nb_explore_gsm8k.ipynb
rename to archive/our_scratchpad/archive/nb_explore_gsm8k.ipynb
diff --git a/our_scratchpad/archive/nb_generate_experimentation.ipynb b/archive/our_scratchpad/archive/nb_generate_experimentation.ipynb
similarity index 100%
rename from our_scratchpad/archive/nb_generate_experimentation.ipynb
rename to archive/our_scratchpad/archive/nb_generate_experimentation.ipynb
diff --git a/our_scratchpad/archive/nb_group_beam_search.ipynb b/archive/our_scratchpad/archive/nb_group_beam_search.ipynb
similarity index 100%
rename from our_scratchpad/archive/nb_group_beam_search.ipynb
rename to archive/our_scratchpad/archive/nb_group_beam_search.ipynb
diff --git a/our_scratchpad/archive/nb_our_scratchpad.ipynb b/archive/our_scratchpad/archive/nb_our_scratchpad.ipynb
similarity index 100%
rename from our_scratchpad/archive/nb_our_scratchpad.ipynb
rename to archive/our_scratchpad/archive/nb_our_scratchpad.ipynb
diff --git a/our_scratchpad/archive/nb_vmap_transformers.ipynb b/archive/our_scratchpad/archive/nb_vmap_transformers.ipynb
similarity index 100%
rename from our_scratchpad/archive/nb_vmap_transformers.ipynb
rename to archive/our_scratchpad/archive/nb_vmap_transformers.ipynb
diff --git a/our_scratchpad/archive/torch_distributed.py b/archive/our_scratchpad/archive/torch_distributed.py
similarity index 100%
rename from our_scratchpad/archive/torch_distributed.py
rename to archive/our_scratchpad/archive/torch_distributed.py
diff --git a/our_scratchpad/archive/unpicklers/better_reader.py b/archive/our_scratchpad/archive/unpicklers/better_reader.py
similarity index 100%
rename from our_scratchpad/archive/unpicklers/better_reader.py
rename to archive/our_scratchpad/archive/unpicklers/better_reader.py
diff --git a/our_scratchpad/archive/unpicklers/reader.py b/archive/our_scratchpad/archive/unpicklers/reader.py
similarity index 100%
rename from our_scratchpad/archive/unpicklers/reader.py
rename to archive/our_scratchpad/archive/unpicklers/reader.py
diff --git a/our_scratchpad/asdiv_data_exploration.ipynb b/archive/our_scratchpad/asdiv_data_exploration.ipynb
similarity index 100%
rename from our_scratchpad/asdiv_data_exploration.ipynb
rename to archive/our_scratchpad/asdiv_data_exploration.ipynb
diff --git a/our_scratchpad/asdiv_dataset.py b/archive/our_scratchpad/asdiv_dataset.py
similarity index 100%
rename from our_scratchpad/asdiv_dataset.py
rename to archive/our_scratchpad/asdiv_dataset.py
diff --git a/our_scratchpad/configs/ppo_config.yml b/archive/our_scratchpad/configs/ppo_config.yml
similarity index 100%
rename from our_scratchpad/configs/ppo_config.yml
rename to archive/our_scratchpad/configs/ppo_config.yml
diff --git a/our_scratchpad/flan_t5_gsm8k.ipynb b/archive/our_scratchpad/flan_t5_gsm8k.ipynb
similarity index 100%
rename from our_scratchpad/flan_t5_gsm8k.ipynb
rename to archive/our_scratchpad/flan_t5_gsm8k.ipynb
diff --git a/our_scratchpad/flan_t5_xxl_results/asdiv_float32_majority_vote.ipynb b/archive/our_scratchpad/flan_t5_xxl_results/asdiv_float32_majority_vote.ipynb
similarity index 100%
rename from our_scratchpad/flan_t5_xxl_results/asdiv_float32_majority_vote.ipynb
rename to archive/our_scratchpad/flan_t5_xxl_results/asdiv_float32_majority_vote.ipynb
diff --git a/our_scratchpad/flan_t5_xxl_results/asdiv_float32_majority_vote_8_contexts.ipynb b/archive/our_scratchpad/flan_t5_xxl_results/asdiv_float32_majority_vote_8_contexts.ipynb
similarity index 100%
rename from our_scratchpad/flan_t5_xxl_results/asdiv_float32_majority_vote_8_contexts.ipynb
rename to archive/our_scratchpad/flan_t5_xxl_results/asdiv_float32_majority_vote_8_contexts.ipynb
diff --git a/our_scratchpad/little_rocket/.gitignore b/archive/our_scratchpad/little_rocket/.gitignore
similarity index 100%
rename from our_scratchpad/little_rocket/.gitignore
rename to archive/our_scratchpad/little_rocket/.gitignore
diff --git a/our_scratchpad/little_rocket/constants.py b/archive/our_scratchpad/little_rocket/constants.py
similarity index 100%
rename from our_scratchpad/little_rocket/constants.py
rename to archive/our_scratchpad/little_rocket/constants.py
diff --git a/our_scratchpad/little_rocket/jupyter_client.ipynb b/archive/our_scratchpad/little_rocket/jupyter_client.ipynb
similarity index 100%
rename from our_scratchpad/little_rocket/jupyter_client.ipynb
rename to archive/our_scratchpad/little_rocket/jupyter_client.ipynb
diff --git a/our_scratchpad/little_rocket/main.py b/archive/our_scratchpad/little_rocket/main.py
similarity index 100%
rename from our_scratchpad/little_rocket/main.py
rename to archive/our_scratchpad/little_rocket/main.py
diff --git a/our_scratchpad/little_rocket/neural_net.py b/archive/our_scratchpad/little_rocket/neural_net.py
similarity index 100%
rename from our_scratchpad/little_rocket/neural_net.py
rename to archive/our_scratchpad/little_rocket/neural_net.py
diff --git a/our_scratchpad/little_rocket/rl.py b/archive/our_scratchpad/little_rocket/rl.py
similarity index 100%
rename from our_scratchpad/little_rocket/rl.py
rename to archive/our_scratchpad/little_rocket/rl.py
diff --git a/our_scratchpad/little_rocket/rocket_baselines.ipynb b/archive/our_scratchpad/little_rocket/rocket_baselines.ipynb
similarity index 100%
rename from our_scratchpad/little_rocket/rocket_baselines.ipynb
rename to archive/our_scratchpad/little_rocket/rocket_baselines.ipynb
diff --git a/our_scratchpad/little_rocket/utils.py b/archive/our_scratchpad/little_rocket/utils.py
similarity index 100%
rename from our_scratchpad/little_rocket/utils.py
rename to archive/our_scratchpad/little_rocket/utils.py
diff --git a/our_scratchpad/other_run_flan_t5_gsm8k.ipynb b/archive/our_scratchpad/other_run_flan_t5_gsm8k.ipynb
similarity index 100%
rename from our_scratchpad/other_run_flan_t5_gsm8k.ipynb
rename to archive/our_scratchpad/other_run_flan_t5_gsm8k.ipynb
diff --git a/our_scratchpad/save_exp.ipynb b/archive/our_scratchpad/save_exp.ipynb
similarity index 100%
rename from our_scratchpad/save_exp.ipynb
rename to archive/our_scratchpad/save_exp.ipynb
diff --git a/our_scratchpad/text2digits_experimentation.ipynb b/archive/our_scratchpad/text2digits_experimentation.ipynb
similarity index 100%
rename from our_scratchpad/text2digits_experimentation.ipynb
rename to archive/our_scratchpad/text2digits_experimentation.ipynb
diff --git a/trl/accelerate_ddp_no.yaml b/trl/accelerate_ddp_no.yaml
new file mode 100644
index 0000000..3a9cf80
--- /dev/null
+++ b/trl/accelerate_ddp_no.yaml
@@ -0,0 +1,16 @@
+compute_environment: LOCAL_MACHINE
+distributed_type: MULTI_GPU
+downcast_bf16: 'no'
+gpu_ids: all
+machine_rank: 0
+main_training_function: main
+mixed_precision: 'no'
+num_machines: 1
+rdzv_backend: static
+same_network: true
+tpu_env: []
+tpu_use_cluster: false
+tpu_use_sudo: false
+use_cpu: false
+
+num_processes: 0
\ No newline at end of file
diff --git a/trl/archive/gpt-neo-20b_sentiment_peft.py b/trl/archive/gpt-neo-20b_sentiment_peft.py
new file mode 100644
index 0000000..f9ecb44
--- /dev/null
+++ b/trl/archive/gpt-neo-20b_sentiment_peft.py
@@ -0,0 +1,337 @@
+# coding=utf-8
+# Copyright 2022 The HuggingFace Inc. team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+from dataclasses import dataclass, field
+import os
+from typing import Optional
+import rich
+import torch
+from datasets import load_dataset
+from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training
+from tqdm import tqdm
+from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser, pipeline
+
+from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, set_seed
+from trl.core import LengthSampler
+
+
+RANK       = int(os.environ["RANK"])
+WORLD_SIZE = int(os.environ["WORLD_SIZE"])
+LOCAL_RANK = int(os.environ["LOCAL_RANK"])
+
+
+########################################################################
+# This is a fully working simple example to use trl with accelerate.
+#
+# This example fine-tunes a GPT2 model on the IMDB dataset using PPO
+# (proximal policy optimization).
+# in any of the following settings (with the same script):
+#   - single CPU or single GPU
+#   - fp16 (mixed-precision) or fp32 (normal precision)
+#
+# To run it in each of these various modes, first initialize the accelerate
+# configuration with `accelerate config`
+#
+########################################################################
+
+########################################################################
+# NOTE for to train with a 8-bit model a more recent version of
+# transformers is required, full dependecies for this example:
+# pip install  bitsandbytes datasets accelerate loralib
+# pip install  git+https://github.com/huggingface/transformers.git@main
+# pip install peft
+########################################################################
+
+# We first define the configuration of the experiment, defining the model, the dataset,
+# the training parameters, and the PPO parameters.
+# Check the default arguments in the `PPOConfig` class for more details.
+# If you want to log with tensorboard, add the kwarg
+# `accelerator_kwargs={"logging_dir": PATH_TO_LOGS}` to the PPOConfig.
+
+
+# Define and parse arguments.
+@dataclass
+class ScriptArguments:
+    """
+    The name of the Casual LM model we wish to fine with PPO
+    """
+
+    # NOTE: gpt2 models use Conv1D instead of Linear layers which are not yet supported in 8 bit mode
+    # models like gpt-neo* models are more suitable.
+
+    model_name: Optional[str]      = "edbeeching/gpt-neo-125M-imdb-lora-adapter-merged"
+    log_with: Optional[str]        = None
+    learning_rate: Optional[float] = 1.41e-5
+    mini_batch_size: Optional[int] = 16
+    batch_size: Optional[int]      = 32
+    gradient_accumulation_steps    = 1
+
+
+# Below is an example function to build the dataset. In our case, we use the IMDB dataset
+# from the `datasets` library. One should customize this function to train the model on
+# its own dataset.
+def build_dataset(
+    config, 
+    *,
+    input_min_text_length = 2, 
+    input_max_text_length = 8,
+    dataset_name          = "imdb", 
+):
+    """
+
+    Build dataset for training. This builds the dataset from `load_dataset`, one should
+    customize this function to train the model on its own dataset.
+
+    Args:
+        dataset_name (`str`):
+            The name of the dataset to be loaded.
+
+    Returns:
+        dataloader (`torch.utils.data.DataLoader`):
+            The dataloader for the dataset.
+
+    """
+    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
+    tokenizer.pad_token = tokenizer.eos_token
+    # load imdb with datasets
+    ds = load_dataset(dataset_name, split="train")
+    ds = ds.rename_columns({"text": "review"})
+    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)
+
+    input_size = LengthSampler(input_min_text_length, input_max_text_length)
+
+    def tokenize(sample):
+        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
+        sample["query"] = tokenizer.decode(sample["input_ids"])
+        return sample
+
+    ds = ds.map(tokenize, batched=False)
+    ds.set_format(type="torch")
+
+    return ds
+
+
+
+def collator(data):
+    return dict((key, [d[key] for d in data]) for key in data[0])
+
+
+def print_trainable_parameters(model):
+    """
+    Prints the number of trainable parameters in the model.
+    """
+    trainable_params = 0
+    all_param        = 0
+
+    for _, param in model.named_parameters():
+        all_param += param.numel()
+        if param.requires_grad:
+            trainable_params += param.numel()
+
+    rich.print(
+        f"[bold blue]({RANK}/{WORLD_SIZE}):[/]"
+        f"trainable params: {trainable_params} || "
+        f"all params: {all_param} || "
+        f"trainable%: {trainable_params / all_param:0.5%}"
+    )
+
+
+def main():
+    parser      = HfArgumentParser(ScriptArguments)
+    script_args = parser.parse_args_into_dataclasses()[0]
+
+    config = PPOConfig(
+        gradient_accumulation_steps = script_args.gradient_accumulation_steps,
+        mini_batch_size             = script_args.mini_batch_size,
+        learning_rate               = script_args.learning_rate,
+        batch_size                  = script_args.batch_size,
+        model_name                  = script_args.model_name,
+        log_with                    = script_args.log_with,
+    )
+
+    # We then define the arguments to pass to the sentiment analysis pipeline.
+    # We set `return_all_scores` to True to get the sentiment score for each token.
+    sent_kwargs = dict(
+        return_all_scores = True, 
+        function_to_apply = "none", 
+        batch_size        = config.mini_batch_size,
+    )
+
+    # We retrieve the dataloader by calling the `build_dataset` function.
+    dataset = build_dataset(config)
+
+    # set seed before initializing value head for deterministic eval
+    set_seed(config.seed)
+
+    # Now let's build the model, the reference model, and the tokenizer.
+    dmap_keys = ["transformer", "lm_head"]
+    dmap      = {
+        k: LOCAL_RANK 
+        for k in dmap_keys
+    }
+    print("pretrained_model = AutoModelForCausalLM.from_pretrained ...")
+    pretrained_model = AutoModelForCausalLM.from_pretrained(
+        config.model_name, 
+        load_in_8bit=True, 
+        device_map=dmap,
+    )
+
+    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
+
+    """### Apply LoRA
+    Here comes the magic with `peft`! Let's load a `PeftModel` and 
+    specify that we are going to use low-rank adapters (LoRA) using 
+    `get_peft_model` utility function from `peft`.
+    """
+
+
+    print("pretrained_model = prepare_model_for_int8_training ...")
+    pretrained_model = prepare_model_for_int8_training(
+        pretrained_model, 
+        output_embedding_layer_name="embed_out"
+    )
+    
+    target_modules = None
+    if "gpt-neox" in script_args.model_name:
+        # workaround to use 8bit training on this model
+        # hacky workaround due to issues with "EleutherAI/gpt-neox-20b"
+        target_modules = ["query_key_value", "xxx"]  
+
+        for name, param in pretrained_model.named_parameters():
+            # freeze base model's layers
+            param.requires_grad = False
+
+            if getattr(pretrained_model, "is_loaded_in_8bit", False):
+                # cast layer norm in fp32 for stability for 8bit models
+                if param.ndim == 1 and "layer_norm" in name:
+                    param.data = param.data.to(torch.float16)
+
+    lora_config = LoraConfig(
+        r=16,
+        lora_alpha=32,
+        target_modules=target_modules,  # handled automatically by peft
+        lora_dropout=0.05,
+        bias="none",
+        task_type="CAUSAL_LM",
+    )
+    
+    print("pretrained_model = get_peft_model(pretrained_model, lora_config)")
+    pretrained_model = get_peft_model(pretrained_model, lora_config)
+
+    print("AutoModelForCausalLMWithValueHead.from_pretrained")
+    model = AutoModelForCausalLMWithValueHead.from_pretrained(pretrained_model)
+
+    model.gradient_checkpointing_disable = model.pretrained_model.gradient_checkpointing_disable
+    model.gradient_checkpointing_enable  = model.pretrained_model.gradient_checkpointing_enable
+
+    print_trainable_parameters(model)
+
+    # GPT-2 tokenizer has a pad token, but it is not eos_token by default. We need to set it to eos_token.
+    # only for this model.
+    tokenizer.pad_token = tokenizer.eos_token
+
+    optimizer = torch.optim.Adam(
+        filter(lambda p: p.requires_grad, model.parameters()), 
+        lr=config.learning_rate
+    )
+
+    # We then build the PPOTrainer, passing the model, the reference model, the tokenizer
+    ppo_trainer = PPOTrainer(
+        config, 
+        model, 
+        data_collator = collator, 
+        ref_model     = None,
+        tokenizer     = tokenizer,
+        optimizer     = optimizer,
+        dataset       = dataset,
+    )
+
+    # We then build the sentiment analysis pipeline, passing the model name and the
+    # sentiment analysis pipeline arguments. Let's also make sure to set the device
+    # to the same device as the PPOTrainer.
+    device = ppo_trainer.accelerator.device
+    if ppo_trainer.accelerator.num_processes == 1:
+        device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
+
+    print("pipeline(...)")
+    sentiment_pipe = pipeline(
+        "sentiment-analysis", 
+        model="lvwerra/distilbert-imdb", 
+        device=device,
+    )
+
+    # We then define the arguments to pass to the `generate` function. These arguments
+    # are passed to the `generate` function of the PPOTrainer, which is a wrapper around
+    # the `generate` function of the trained model.
+    generation_kwargs = {
+        "pad_token_id": tokenizer.eos_token_id,
+        "eos_token_id": -1,
+        "min_length":   -1,
+        "do_sample":    True,
+        "top_k":        0.0,
+        "top_p":        1.0,
+    }
+    output_min_length = 4
+    output_max_length = 16
+    output_length_sampler = LengthSampler(
+        output_min_length, 
+        output_max_length,
+    )
+
+
+    for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader), desc="Epoch"):
+        query_tensors = batch["input_ids"]
+
+        model.gradient_checkpointing_disable()
+        model.pretrained_model.config.use_cache = True
+        # Get response from Causal LM
+        response_tensors = []
+
+        for query in tqdm(query_tensors, "Unrolling."):
+            gen_len = output_length_sampler()
+            generation_kwargs["max_new_tokens"] = gen_len
+            response = ppo_trainer.generate(query, **generation_kwargs)
+            response_tensors.append(response.squeeze()[- gen_len:])
+
+        batch["response"] = [
+            tokenizer.decode(r.squeeze()) 
+            for r in response_tensors
+        ]
+
+        # Compute sentiment score
+        texts = [q + r for q, r in zip(batch["query"], batch["response"])]
+        pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
+        rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]
+
+        # Run PPO step
+        model.gradient_checkpointing_enable()
+        model.pretrained_model.config.use_cache = False
+
+        rich.print(f"[blue bold]Epoch {epoch}:[/] [white bold]ppo_trainer.step")
+        stats = ppo_trainer.step(
+            responses = response_tensors, 
+            queries   = query_tensors, 
+            scores    = rewards,
+        )
+
+        ppo_trainer.log_stats(
+            rewards = rewards,
+            stats   = stats, 
+            batch   = batch, 
+        )
+
+
+
+if __name__ == "__main__":
+    main()
diff --git a/trl/archive/t5_example.py b/trl/archive/t5_example.py
new file mode 100644
index 0000000..b4e2281
--- /dev/null
+++ b/trl/archive/t5_example.py
@@ -0,0 +1,161 @@
+from dataclasses import dataclass, field
+import itertools
+import random
+from typing import Optional
+
+import fire
+import torch
+from datasets import load_dataset
+import numpy as np
+import peft
+from tqdm import tqdm
+import transformers
+from transformers import (
+    AutoTokenizer, 
+    HfArgumentParser, 
+    pipeline,
+)
+
+from trl import (
+    AutoModelForSeq2SeqLMWithValueHead, 
+    PPOConfig, 
+    PPOTrainer,
+    set_seed,
+)
+from trl.core import LengthSampler
+
+set_seed(0)
+random.seed(1)
+np.random.seed(2)
+torch.manual_seed(3)
+torch.cuda.manual_seed_all(4)
+
+
+DEFAULT_GRADIENT_ACCUMULATION_STEPS = 3
+DEFAULT_MINI_BATCH_SIZE             = 16
+DEFAULT_LEARNING_RATE               = 5e-5
+DEFAULT_BATCH_SIZE                  = 3
+DEFAULT_MODEL_NAME                  = "google/flan-t5-small"
+DEFAULT_PEFT_CONFIG                 = dict(
+    r              = 8,
+    lora_alpha     = 32,
+    task_type      = peft.TaskType.SEQ_2_SEQ_LM,
+    lora_dropout   = 0,
+    inference_mode = False,
+)
+
+def main(
+    *,
+    gradient_accumulation_steps = DEFAULT_GRADIENT_ACCUMULATION_STEPS,
+    mini_batch_size             = DEFAULT_MINI_BATCH_SIZE,
+    learning_rate               = DEFAULT_LEARNING_RATE,
+    peft_config                 = DEFAULT_PEFT_CONFIG,
+    model_name                  = DEFAULT_MODEL_NAME,
+    batch_size                  = DEFAULT_BATCH_SIZE,
+):
+
+    config = PPOConfig(
+        gradient_accumulation_steps = gradient_accumulation_steps,
+        mini_batch_size             = mini_batch_size,
+        learning_rate               = learning_rate,
+        model_name                  = model_name,
+        batch_size                  = batch_size,
+        log_with                    = "wandb",
+    )
+    # We then define the arguments to pass to the sentiment analysis pipeline.
+    # We set `return_all_scores` to True to get the sentiment score for each token.
+    sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}
+
+
+    # Below is an example function to build the dataset. In our case, we use the IMDB dataset
+    # from the `datasets` library. One should customize this function to train the model on
+    # its own dataset.
+    def build_imdb_dataset(tokenizer, input_min_text_length=2, input_max_text_length=8):
+        # load imdb with datasets
+        ds = load_dataset("imdb", split="train")
+        ds = ds.rename_columns({"text": "review"})
+        ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)
+
+        input_size = LengthSampler(input_min_text_length, input_max_text_length)
+
+        def tokenize(sample):
+            sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()] + [tokenizer.eos_token_id]
+            sample["query"] = tokenizer.decode(sample["input_ids"])
+            return sample
+
+        ds = ds.map(tokenize, batched=False)
+        ds.set_format(type="torch")
+        return ds
+
+
+    def collater(data):
+        return dict((key, [d[key] for d in data]) for key in data[0])
+
+
+    # set seed before initializing value head for deterministic eval
+    
+    ref_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(config.model_name)
+    for param in ref_model.parameters():
+        param.requires_grad = False
+    ref_model.eval()
+
+    peft.LoraConfig(**peft_config)
+    model = peft.
+
+
+    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
+
+    # We retrieve the dataloader by calling the `build_dataset` function.
+    dataset = build_imdb_dataset(tokenizer)
+
+    query = tokenizer("I really liked this movie because", return_tensors="pt")["input_ids"]
+
+    generation_kwargs = {"top_k": 0.0, "top_p": 1.0, "do_sample": True, "eos_token_id": -1}
+
+
+    # We then build the PPOTrainer, passing the model, the reference model, the tokenizer
+    ppo_trainer = PPOTrainer(
+        config, 
+        model, 
+        ref_model, 
+        tokenizer, 
+        dataset=dataset, data_collator=collater)
+
+    # We then build the sentiment analysis pipeline, passing the model name and the
+    # sentiment analysis pipeline arguments. Let's also make sure to set the device
+    # to the same device as the PPOTrainer.
+    device = ppo_trainer.accelerator.device
+    if ppo_trainer.accelerator.num_processes == 1:
+        device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
+    sentiment_pipe = pipeline("sentiment-analysis", "lvwerra/distilbert-imdb", device=device)
+
+    # We then define the arguments to pass to the `generate` function. These arguments
+    # are passed to the `generate` function of the PPOTrainer, which is a wrapper around
+    # the `generate` function of the trained model.
+    output_min_length = 16
+    output_max_length = 32
+    output_length_sampler = LengthSampler(output_min_length, output_max_length)
+
+    for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
+        query_tensors = batch["input_ids"]
+
+        # Get response from t5
+        response_tensors = ppo_trainer.generate(
+            query_tensors, 
+            **generation_kwargs,
+        )
+        response_tensors = [r[1:] for r in response_tensors]
+        batch["response"] = tokenizer.batch_decode(response_tensors)
+
+        # Compute sentiment score
+        texts = [q + r for q, r in zip(batch["query"], batch["response"])]
+        pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
+        rewards = [torch.tensor(output[1]["score"]).to(device) for output in pipe_outputs]
+
+        # Run PPO step
+        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
+        ppo_trainer.log_stats(stats, batch, rewards)
+
+
+if __name__ == "__main__":
+    fire.Fire(main)
\ No newline at end of file
diff --git a/trl/bin_gptx-neo.py b/trl/bin_gptx-neo.py
new file mode 100755
index 0000000..9202e50
--- /dev/null
+++ b/trl/bin_gptx-neo.py
@@ -0,0 +1,396 @@
+#!/usr/bin/env python
+import os
+import wandb
+
+
+os.environ["TOKENIZERS_PARALLELISM"] = "false"
+os.environ["NCCL_DEBUG"]             = "WARN"
+os.environ["DATASETS_VERBOSITY"]     = "warning"
+os.environ["TRANSFORMERS_VERBOSITY"] = "warning"
+
+import collections
+from dataclasses import dataclass
+import itertools
+import logging
+import random
+import typing
+
+import fire
+import torch
+import numpy as np
+import datasets
+import peft
+import rich
+import rich.table
+import rich.status
+from tqdm import tqdm
+import transformers
+import trl
+import trl.core 
+
+import lib_trl_utils
+from accelerate.utils import DistributedDataParallelKwargs
+
+
+datasets    .logging.set_verbosity_warning()
+transformers.logging.set_verbosity_warning()
+logging.getLogger("datasets"    ).setLevel(logging.WARNING)
+logging.getLogger("transformers").setLevel(logging.WARNING)
+logging.getLogger("deepspeed"   ).setLevel(logging.WARNING)
+
+
+np.random            .seed(0)
+random               .seed(1)
+torch         .manual_seed(2)
+torch.cuda.manual_seed_all(3)
+trl              .set_seed(4)
+
+
+DEFAULT_DATASET_NAME      = "imdb"
+DEFAULT_INPUT_MIN_LENGTH  = 2
+DEFAULT_INPUT_MAX_LENGTH  = 5
+DEFAULT_OUTPUT_MIN_LENGTH = 10
+DEFAULT_OUTPUT_MAX_LENGTH = 20
+
+DEFAULT_LOG_STATS_VERBOSE = True
+DEFAULT_REWARD_VERBOSE    = False
+PROMPT =  "" 
+DEFAULT_LORA_CONFIG = dict(
+    inference_mode = False,
+    lora_dropout   = 0.05,
+    lora_alpha     = 32,
+    task_type      = peft.TaskType.CAUSAL_LM,
+    bias           = "none",
+    r              = 8,
+)
+DEFAULT_GENERATION_KWARGS = dict(
+    min_length   = 3,
+    do_sample    = True,
+    top_k        = 0.0,
+    top_p        = 1.0,
+)
+DEFAULT_PIPE_SENT_KWARGS = dict(
+    function_to_apply = "none", 
+    batch_size        = 256,
+    truncation        = True,
+    top_k             = None,)
+DEFAULT_PRECISION = torch.bfloat16
+
+
+@dataclass
+class ScriptArguments:
+    """
+    The name of the Casual LM model we wish to fine with PPO
+    """
+    gradient_accumulation_steps: typing.Optional[int]   = 1 
+    generation_batch_size:                        int   = 256
+    mini_batch_size                                     = 16
+    learning_rate:               typing.Optional[float] = 1.41e-5
+    model_name:                  typing.Optional[str]   = "edbeeching/gpt-neo-125M-imdb-lora-adapter-merged"
+    batch_size:                  typing.Optional[int]   = 256
+    num_epochs:                  typing.Optional[int]   = 10000
+    log_with:                    typing.Optional[str]   = None
+
+
+def build_dataset(
+    config:                dict[str, typing.Any],
+    dataset_name:          str, 
+    input_min_length: int, 
+    input_max_length: int,
+) -> datasets.Dataset:
+    """
+
+    Build dataset for training. This builds the dataset 
+    from `load_dataset`, one should customize this function 
+    to train the model on its own dataset.
+
+    output needs to have input_ids, & query
+
+    Args:
+        dataset_name (`str`):
+            The name of the dataset to be loaded.
+    Returns:
+        dataloader (`torch.utils.data.DataLoader`):
+            The dataloader for the dataset.
+
+    """
+
+    tokenizer = lib_trl_utils.build_tokenizer(config.model_name)
+    ds = datasets.load_dataset(dataset_name, split="train")
+    ds = ds.rename_columns({"text": "review"})
+    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)
+
+    input_size = trl.core.LengthSampler(
+        input_min_length,
+        input_max_length,
+    )
+
+    
+    def tokenize(sample):
+        prompt = PROMPT
+
+        sample["input_ids"] = tokenizer.encode(
+            prompt + sample["review"],
+            truncation=True,
+        )[: input_size()]
+        sample["query"] = tokenizer.decode(sample["input_ids"])
+        return sample
+
+    ds = ds.map(tokenize, batched=False)
+    ds.set_format(type="torch")
+
+    return ds
+
+
+def collator(
+    data: list[dict[str, lib_trl_utils.IntSequence]],
+) -> dict[str, list[lib_trl_utils.IntSequence]]:
+    
+    output_dict = {key: [d[key] for d in data] for key in data[0]}
+
+    return output_dict
+
+
+class RewardFn:
+    def __init__(
+            self, 
+            *, 
+            device: int, 
+            tokenizer: transformers, 
+            pipe_sent_kwargs: dict[str, typing.Any],
+            verbose: bool = False,
+        ):
+
+        self._sentiment_pipe = transformers.pipeline(
+            "sentiment-analysis", 
+            device = device,
+            model  = "lvwerra/distilbert-imdb", 
+        )
+
+        self._pipe_sent_kwargs = pipe_sent_kwargs
+        self._tokenizer        = tokenizer
+        self._verbose          = verbose
+    
+    def __call__(self, response_tensors, batch_query):
+        
+        batch_response  = [self._tokenizer.decode(r.squeeze()) for r in response_tensors]
+        texts           = [q + r for q, r in zip(batch_query, batch_response)]
+        pipe_outputs    = self._sentiment_pipe(texts, **self._pipe_sent_kwargs)
+        prepped_outputs = [{
+            output["label"]: torch.tensor(output["score"]) for output in outputs} 
+            for outputs in pipe_outputs
+        ]
+        final_outputs    = [{
+            output["label"]: torch.tensor(output["score"]) for output in outputs}["POSITIVE"]
+            for outputs in pipe_outputs
+        ]
+
+        if self._verbose:
+            table = rich.table.Table(
+                "Pipeline Inputs", 
+                "POSITIVE",
+                "NEGATIVE",
+                "FINAL OUTPUT",
+                title       = "Sentiment Analysis Pipeline",
+                show_lines  = True,
+            )
+
+
+            for text, output, final_output in itertools.islice(
+                zip(texts, prepped_outputs, final_outputs), 5):
+
+                table.add_row(
+                    str(text), 
+                    str(output["POSITIVE"].item()),
+                    str(output["NEGATIVE"].item()),
+                    str(final_output.item()),
+                )
+            
+            rich.print(self._pipe_sent_kwargs)
+            rich.print(table)
+    
+        return final_outputs
+
+def main(
+    *, 
+    reward_fn_verbose: bool                           = DEFAULT_REWARD_VERBOSE,
+    log_stats_verbose: bool                           = DEFAULT_LOG_STATS_VERBOSE,
+    generation_kwargs: dict[str, typing.Any]          = DEFAULT_GENERATION_KWARGS,
+    lora_config_dict:  dict[str, typing.Any]          = DEFAULT_LORA_CONFIG, 
+    precision:         typing.Union[str, torch.dtype] = DEFAULT_PRECISION,
+    dataset_name:      str                            = DEFAULT_DATASET_NAME, 
+    input_min_length:  str                            = DEFAULT_INPUT_MIN_LENGTH, 
+    input_max_length:  str                            = DEFAULT_INPUT_MAX_LENGTH,
+    output_min_length: str                            = DEFAULT_OUTPUT_MIN_LENGTH, 
+    output_max_length: str                            = DEFAULT_OUTPUT_MAX_LENGTH,
+    pipe_sent_kwargs:  dict[str, typing.Any]          = DEFAULT_PIPE_SENT_KWARGS,
+):
+    parser = transformers.HfArgumentParser(ScriptArguments)
+    script_args = parser.parse_args_into_dataclasses()[0]
+
+    if "gpt" in script_args.model_name.lower():
+        assert lora_config_dict["task_type"] == peft.TaskType.CAUSAL_LM
+        model_type = peft.TaskType.CAUSAL_LM
+    elif "t5" in script_args.model_name.lower():
+        assert lora_config_dict["task_type"] == peft.TaskType.SEQ_2_SEQ_LM
+        model_type = peft.TaskType.SEQ_2_SEQ_LM
+    else:
+        raise ValueError(f"Unknown model type: {script_args.model_name}")
+
+    config = trl.PPOConfig(
+        gradient_accumulation_steps = script_args.gradient_accumulation_steps,
+        mini_batch_size             = script_args.mini_batch_size,
+        learning_rate               = script_args.learning_rate,
+        model_name                  = script_args.model_name,
+        batch_size                  = script_args.batch_size,
+        log_with                    = script_args.log_with,
+        accelerator_kwargs = dict(
+            kwargs_handlers=[DistributedDataParallelKwargs(find_unused_parameters=True)]
+        )
+    )
+    if lib_trl_utils.get_rank() == 0:
+        wandb.init(
+            project="trl", 
+            entity="julesgm", 
+            save_code=True,
+        )
+
+    
+    dataset = build_dataset(
+        config,
+        dataset_name=dataset_name, 
+        input_min_length=input_min_length, 
+        input_max_length=input_max_length,
+    )
+
+    model, tokenizer = lib_trl_utils.init_model(
+        lora_config_dict = lora_config_dict,
+        model_name       = config.model_name,
+        precision        = precision,
+        model_type       = model_type,
+    )
+    optimizer = torch.optim.Adam(
+        filter(lambda p: p.requires_grad, model.parameters()), 
+        lr=config.learning_rate
+    )
+    ppo_trainer = trl.PPOTrainer(
+        config, 
+        model, 
+        data_collator = collator,
+        ref_model     = None,
+        optimizer     = optimizer,
+        tokenizer     = tokenizer,
+        dataset       = dataset,
+    )
+
+    if "gpt" in config.model_name:
+        generation_kwargs["pad_token_id"] = tokenizer.eos_token_id
+        generation_kwargs["eos_token_id"] = -1
+        generation_kwargs["min_length"]   = -1
+
+    ###########################################################################
+    ###########################################################################
+    reward_fn = RewardFn(
+        pipe_sent_kwargs = pipe_sent_kwargs,
+        tokenizer        = tokenizer,
+        verbose          = reward_fn_verbose,
+        device           = lib_trl_utils.get_local_rank(),
+    )
+    output_length_sampler = trl.core.LengthSampler(
+        output_min_length, 
+        output_max_length,
+    )
+    assert output_min_length <= output_max_length, (
+        output_min_length,
+        output_max_length,
+    )
+    assert generation_kwargs["min_length"] <= output_min_length, (
+        generation_kwargs["min_length"],
+        output_min_length,
+    )
+
+    ###########################################################################
+    ###########################################################################
+    
+    assert lib_trl_utils.print_trainable_parameters(model, False) > 0
+
+    for epoch in range(script_args.num_epochs):
+        for batch_idx, batch in tqdm(
+            enumerate(ppo_trainer.dataloader), 
+            desc="Training",
+            disable=lib_trl_utils.get_rank() != 0
+        ):
+            batch["response"] = lib_trl_utils.batched_unroll(
+                output_length_sampler = output_length_sampler,
+                generation_batch_size = script_args.generation_batch_size,
+                generation_kwargs     = generation_kwargs, 
+                ppo_trainer           = ppo_trainer, 
+                tokenizer             = tokenizer,
+                batch                 = batch,
+                model                 = model,
+            )
+
+            # Compute rewards
+            rewards = reward_fn(
+                response_tensors = batch["response"],
+                batch_query      = batch["query"], 
+            )
+
+            ###########################################################################
+            # Print Rewards
+            ###########################################################################
+            all_rewards = ppo_trainer.accelerator.gather_for_metrics(
+                torch.tensor(rewards).to(ppo_trainer.accelerator.device)
+            )
+
+            rich.print(
+                f"[bold blue]"
+                f"({lib_trl_utils.get_rank()}/{lib_trl_utils.get_world_size()}) " +
+                f"({epoch = } {batch_idx = }) " +
+                f"[/][white bold]" +
+                f"Average rewards: " +
+                f"{all_rewards.mean().item():0.4} " +
+                f"+- {all_rewards.std().item():0.1}" 
+            )
+
+            if lib_trl_utils.get_rank() == 0:
+                wandb.log({"avg_all_rewards": all_rewards.mean().item()})
+
+
+            ###########################################################################
+            # Checks & Step
+            ###########################################################################
+            # PPO Step
+            if ppo_trainer.is_encoder_decoder:
+                lib_trl_utils.check_all_start_with_token_id(
+                    batch["response"], tokenizer.pad_token_id,
+                )
+
+            assert all((response != tokenizer.pad_token_id).all() for response in batch["response"])
+            assert all((inputs   != tokenizer.pad_token_id).all() for inputs   in batch["input_ids"])
+
+            for k, v in batch.items():
+                if isinstance(v, torch.Tensor):
+                    v.requires_grad = True
+
+            stats = ppo_trainer.step(
+                responses = batch["response"],
+                queries   = batch["input_ids"],
+                scores    = rewards,
+            )
+
+            # Log stats
+            assert isinstance(rewards, list), type(rewards)
+            assert isinstance(stats,   dict), type(stats)
+            assert isinstance(batch,   dict), type(batch)
+
+            ppo_trainer.log_stats(
+                rewards = rewards,
+                verbose = log_stats_verbose,
+                batch   = batch,
+                stats   = stats,
+            )
+
+
+if __name__ == "__main__":
+    fire.Fire(main)
\ No newline at end of file
diff --git a/trl/bin_main.py b/trl/bin_main.py
new file mode 100755
index 0000000..27f6a4e
--- /dev/null
+++ b/trl/bin_main.py
@@ -0,0 +1,374 @@
+#!/usr/bin/env python
+import os
+
+os.environ["TOKENIZERS_PARALLELISM"] = "false"
+os.environ["TRANSFORMERS_VERBOSITY"] = "warning"
+os.environ["DATASETS_VERBOSITY"]     = "warning"
+os.environ["WANDB_SILENT"]           = "true"
+os.environ["NCCL_DEBUG"]             = "WARN"
+
+
+import logging
+import random
+import typing
+
+import accelerate
+import fire
+
+import numpy as np
+import datasets
+import peft
+import rich
+import rich.status
+import rich.table
+import torch
+from tqdm import tqdm
+import transformers
+import trl
+import trl.core 
+import wandb
+
+import lib_trl_utils
+from accelerate.utils import DistributedDataParallelKwargs
+
+import lib_data
+import lib_metric
+import lib_reward
+
+datasets    .logging.set_verbosity_warning()
+transformers.logging.set_verbosity_warning()
+logging.getLogger("datasets"    ).setLevel(logging.WARNING)
+logging.getLogger("transformers").setLevel(logging.WARNING)
+logging.getLogger("deepspeed"   ).setLevel(logging.WARNING)
+
+np.random            .seed(0)
+random               .seed(1)
+torch         .manual_seed(2)
+torch.cuda.manual_seed_all(3)
+trl              .set_seed(4)
+
+
+DEFAULT_LOG_STATS_VERBOSE = True
+DEFAULT_REWARD_VERBOSE    = False
+PROMPT                    =  "" 
+
+DEFAULT_LORA_CONFIG = dict(
+    inference_mode = False,
+    lora_dropout   = 0.05,
+    lora_alpha     = 32,
+    task_type      = peft.TaskType.SEQ_2_SEQ_LM,
+    bias           = "none",
+    r              = 16,
+)
+
+DEFAULT_GENERATION_KWARGS = dict(
+    min_length   = 3,
+    do_sample    = True,
+    top_k        = 0.0,
+    top_p        = 1.0,
+)
+
+DEFAULT_GRADIENT_ACCUMULATION_STEPS: int                  = 1
+DEFAULT_GENERATION_BATCH_SIZE:       int                  = 16
+DEFAULT_MINI_BATCH_SIZE:             int                  = 16
+DEFAULT_LEARNING_RATE:               float                = 1.41e-5
+DEFAULT_MODEL_NAME:                  str                  = "google/flan-t5-base" 
+DEFAULT_BATCH_SIZE:                  int                  = 16
+DEFAULT_NUM_EPOCHS:                  int                  = 10
+DEFAULT_PRECISION                                         = torch.bfloat16
+DEFAULT_LOG_WITH:                    typing.Optional[str] = None
+DEFAULT_USE_PEFT:                    bool                 = True
+
+
+def collator(data):
+    return dict((key, [d[key] for d in data]) for key in data[0])
+
+
+def evaluate_or_test(
+    *,
+    generation_batch_size: int,
+    generation_kwargs: dict[str, typing.Any],
+    logging_header: str,
+    ppo_trainer: trl.core.PPOTrainer,
+    dataloader, 
+    reward_fn: typing.Callable[[list[str], list[str]], torch.Tensor],
+    tokenizer: transformers.PreTrainedTokenizerBase,
+    set_name: str, 
+    model: trl.models.modeling_base.PreTrainedModelWrapper,
+):
+    
+    rewards = []
+    for batch_idx, batch in tqdm(
+        enumerate(dataloader), 
+        desc=logging_header
+    ):
+        
+        batch["response"] = lib_trl_utils.batched_unroll(
+            generation_batch_size = generation_batch_size,
+            generation_kwargs     = generation_kwargs, 
+            ppo_trainer           = ppo_trainer, 
+            tokenizer             = tokenizer,
+            batch                 = batch,
+            model                 = model,
+        )
+
+        local_batch_rewards = reward_fn(
+            response_tensors = batch["response"],
+            batch_query      = batch["query"], 
+        )
+
+        gathered_batch_rewards = ppo_trainer.accelerator.gather_for_metrics(
+            local_batch_rewards.to(ppo_trainer.accelerator.device),
+        )
+
+        rewards.append(gathered_batch_rewards)
+
+    reward = torch.cat(rewards, dim=0)
+    
+    wandb.log({
+        f"{set_name}/reward_mean": reward.mean().item(),
+        f"{set_name}/reward_str":  reward.std ().item(),
+        
+    })
+
+class RewardForwardWrapper:
+    """
+    Meant to work with either a fixed trlAutoModelWithValueHead or a PeftModel
+    """
+    def __init__(self, ppo_trainer_model, ppo_trainer_ref_model):
+        self._ppo_model = ppo_trainer_model
+        self._ppo_ref   = ppo_trainer_ref_model
+
+    def reward_forward_fn(self, *args, **kwargs):
+        peft_mode = (
+            isinstance(self._ppo_model, peft.PeftModel) and 
+            self._ppo_ref is None
+        )
+        ref_mode = (
+            (not isinstance(self._ppo_model, peft.PeftModel)) and 
+            self._ppo_ref is not None
+        )
+
+        assert peft_mode ^ ref_mode
+        rich.print(f"[red bold]{peft_mode = } {ref_mode = }")
+
+        if peft_mode:
+            assert isinstance(self._ppo_model, peft.PeftModel)
+            with self._ppo_model.disable_adapter():
+                with self._ppo_model.no_grad():
+                    return self._ppo_model(*args, **kwargs)
+            
+        elif ref_mode:
+            self._ppo_ref.eval()
+            with self._ppo_ref.no_grad():
+                return self._ppo_ref(*args, **kwargs)
+
+        raise ValueError("Should not be here")
+
+
+def main(
+    *, 
+    gradient_accumulation_steps: int          = DEFAULT_GRADIENT_ACCUMULATION_STEPS,
+    generation_batch_size: int                = DEFAULT_GENERATION_BATCH_SIZE,
+    reward_fn_verbose: bool                   = DEFAULT_REWARD_VERBOSE,
+    generation_kwargs: dict[str, typing.Any]  = DEFAULT_GENERATION_KWARGS,
+    log_stats_verbose: bool                   = DEFAULT_LOG_STATS_VERBOSE,
+    lora_config_dict:  dict[str, typing.Any]  = DEFAULT_LORA_CONFIG, 
+    mini_batch_size: int                      = DEFAULT_MINI_BATCH_SIZE,
+    learning_rate: float                      = DEFAULT_LEARNING_RATE,
+    model_name: str                           = DEFAULT_MODEL_NAME,
+    batch_size: int                           = DEFAULT_BATCH_SIZE,
+    num_epochs: int                           = DEFAULT_NUM_EPOCHS,
+    precision: typing.Union[str, torch.dtype] = DEFAULT_PRECISION,
+    log_with: str                             = DEFAULT_LOG_WITH,
+
+    input_max_length: int                     = 115,
+    dataset_name: str                         = lib_data.GSM8K,
+    use_peft: bool                            = DEFAULT_USE_PEFT,
+):
+    args = locals().copy()
+
+    ###########################################################################
+    # Find the type of model we are using
+    ###########################################################################
+    if "gpt" in model_name.lower():
+        assert lora_config_dict["task_type"] == peft.TaskType.CAUSAL_LM
+        model_type = peft.TaskType.CAUSAL_LM
+    elif "t5" in model_name.lower():
+        assert lora_config_dict["task_type"] == peft.TaskType.SEQ_2_SEQ_LM
+        model_type = peft.TaskType.SEQ_2_SEQ_LM
+    else:
+        raise ValueError(f"Unknown model type: {model_name}")
+
+    ppo_config_dict = dict(
+        gradient_accumulation_steps = gradient_accumulation_steps,
+        accelerator_kwargs          = dict(kwargs_handlers=[DistributedDataParallelKwargs(find_unused_parameters=True)]),
+        mini_batch_size             = mini_batch_size,
+        learning_rate               = learning_rate,
+        model_name                  = model_name,
+        batch_size                  = batch_size,
+        log_with                    = log_with,
+    )
+
+    config = trl.PPOConfig(
+        **ppo_config_dict,
+    )
+
+    if lib_trl_utils.get_rank() == 0:
+        wandb.init(
+            save_code = True,
+            project   = "trl-main",
+            entity    = "julesgm",
+            name      = None,
+            config    = dict(
+                generation_kwargs = generation_kwargs,
+                ppo_config_args   = ppo_config_dict,
+                script_args       = args,
+            ),
+        )
+
+    if dataset_name == lib_data.GSM8K:
+        dataset = lib_data.GSM8K(
+            input_max_length, 
+            tokenizer, 
+            datasets.load_dataset("gsm8k", "main", split="train"),
+        )
+        eval_dataset = lib_data.GSM8K(
+            input_max_length, 
+            tokenizer, 
+            datasets.load_dataset("gsm8k", "main", split="test")
+        )
+        
+    elif dataset_name == lib_data.ASDiv:
+        dataset = lib_data.ASDiv(
+            input_max_length, 
+            tokenizer, 
+            datasets.load_dataset("asdiv"),
+        )
+    else:
+        raise ValueError(f"Unknown dataset: {dataset_name}")
+
+    model, ref_model, tokenizer = lib_trl_utils.init_model(
+        lora_config_dict = lora_config_dict,
+        model_name       = config.model_name,
+        model_type       = model_type,
+        precision        = precision,
+    )
+
+
+    ###########################################################################
+    # Set model name specific flags
+    ###########################################################################
+    if "gpt" in config.model_name:
+        assert lora_config_dict["task_type"] == peft.TaskType.CAUSAL_LM
+        generation_kwargs["pad_token_id"] = tokenizer.eos_token_id
+        generation_kwargs["eos_token_id"] = -1
+        generation_kwargs["min_length"]   = -1
+    
+
+    ###########################################################################
+    # Prep Training
+    ###########################################################################
+    ppo_trainer = trl.PPOTrainer(
+        config, 
+        model,
+        data_collator = collator,
+        ref_model     = None,
+        tokenizer     = tokenizer,
+        dataset       = dataset,
+    )
+    reward_forward_fn = RewardForwardWrapper(
+        ppo_trainer_model     = ppo_trainer.model,
+        ppo_trainer_ref_model = ppo_trainer.ref_model,
+    )
+    metric_accuracy = lib_metric.ScratchpadAnswerAccuracy()
+    reward_fn = lib_reward.ScratchpadRewardFn(
+        ref_model = reward_forward_fn,
+        tokenizer = tokenizer, 
+        uses_peft = use_peft,
+        metric_fn = metric_accuracy,
+    )
+    
+
+    ###########################################################################
+    # Training Loop
+    ###########################################################################
+    for epoch in range(num_epochs):
+        for batch_idx, batch in tqdm(
+            enumerate(ppo_trainer.dataloader), 
+            desc="Training",
+            disable=lib_trl_utils.get_rank() != 0
+        ):
+            batch["response"] = lib_trl_utils.batched_unroll(
+                generation_batch_size = generation_batch_size,
+                generation_kwargs     = generation_kwargs, 
+                ppo_trainer           = ppo_trainer, 
+                tokenizer             = tokenizer,
+                batch                 = batch,
+                model                 = model,
+            )
+
+            rewards = reward_fn(
+                response_tensors = batch["response"],
+                batch_query      = batch["query"], 
+            )
+
+            ###########################################################################
+            # Print Rewards
+            ###########################################################################
+            all_rewards = ppo_trainer.accelerator.gather_for_metrics(
+                torch.tensor(rewards).to(ppo_trainer.accelerator.device)
+            )
+
+            rich.print(
+                f"[bold blue]"
+                f"({lib_trl_utils.get_rank()}/{lib_trl_utils.get_world_size()}) " +
+                f"({epoch = } {batch_idx = }) " +
+                f"[/][white bold]" +
+                f"Average rewards: " +
+                f"{all_rewards.mean().item():0.4} " +
+                f"+- {all_rewards.std().item():0.1}" 
+            )
+
+            if lib_trl_utils.get_rank() == 0:
+                wandb.log({"avg_all_rewards": all_rewards.mean().item()})
+
+
+            ###########################################################################
+            # Checks & Step
+            ###########################################################################
+            # PPO Step
+            if ppo_trainer.is_encoder_decoder:
+                lib_trl_utils.check_all_start_with_token_id(
+                    batch["response"], tokenizer.pad_token_id,
+                )
+
+            assert all((response != tokenizer.pad_token_id).all() for response in batch["response"])
+            assert all((inputs   != tokenizer.pad_token_id).all() for inputs   in batch["input_ids"])
+
+            for k, v in batch.items():
+                if isinstance(v, torch.Tensor):
+                    v.requires_grad = True
+
+            stats = ppo_trainer.step(
+                responses = batch["response"],
+                queries   = batch["input_ids"],
+                scores    = rewards,
+            )
+
+            # Log stats
+            assert isinstance(rewards, list), type(rewards)
+            assert isinstance(stats,   dict), type(stats)
+            assert isinstance(batch,   dict), type(batch)
+
+            ppo_trainer.log_stats(
+                rewards = rewards,
+                verbose = log_stats_verbose,
+                batch   = batch,
+                stats   = stats,
+            )
+
+    
+
+if __name__ == "__main__":
+    fire.Fire(main)
\ No newline at end of file
diff --git a/trl/launch b/trl/launch
new file mode 100755
index 0000000..aa673eb
--- /dev/null
+++ b/trl/launch
@@ -0,0 +1,70 @@
+#!/usr/bin/env python
+import os
+import shlex
+import subprocess
+
+import fire
+from pathlib import Path
+
+
+BIN_PATH = "/home/mila/g/gagnonju/.main/bin/python"
+MODULE = "/home/mila/g/gagnonju/.main/bin/accelerate"
+
+
+
+def _kill_wandb_servers():
+    subprocess.call(
+        "pgrep wandb | xargs kill -9",
+        shell=True, 
+        # stdout=subprocess.STDOUT, 
+        stderr=subprocess.DEVNULL,
+        universal_newlines=True,
+    )
+
+
+def _kill_other_python_processes():
+    subprocess.call(
+        f"pgrep python | grep -v {os.getpid()} | xargs kill -9",
+        shell=True,
+        # stdout=subprocess.STDOUT,
+        stderr=subprocess.DEVNULL,
+        universal_newlines=True,
+    )
+
+
+def check_exists(path):
+    path = Path(path)
+    assert path.exists(), path
+
+
+def main(one=False, config_file="accelerate_ddp_no.yaml"):
+    if config_file:
+        check_exists(config_file)
+
+    check_exists(BIN_PATH)
+    check_exists(MODULE)
+    _kill_wandb_servers()
+    _kill_other_python_processes()
+
+    if one:
+        num_processes = 1
+    else:
+        # Casting it seems useless, but enforces that it should
+        # be castable to int, which is a way to constrain what
+        # we want
+        num_processes = int(os.environ["SLURM_GPUS_ON_NODE"])
+    
+    command = [
+        BIN_PATH, 
+        MODULE,
+        "launch",
+        "--num_processes", str(num_processes),
+        "--config_file", config_file, 
+        "bin_gptx-neo.py",
+    ]
+
+    print(shlex.join(command))
+    os.execv(BIN_PATH, command)
+
+if __name__ == "__main__":
+    fire.Fire(main)
diff --git a/trl/lib_data.py b/trl/lib_data.py
new file mode 100644
index 0000000..117ef91
--- /dev/null
+++ b/trl/lib_data.py
@@ -0,0 +1,474 @@
+import abc
+import bisect
+import collections
+import enum
+import logging
+import os
+import re
+import time
+import typing
+import xml
+from pathlib import Path
+
+
+import more_itertools
+import numpy as np
+import rich
+import rich.box
+import rich.table
+import torch
+import transformers
+import wget
+from text2digits import text2digits
+
+LOGGER = logging.getLogger(__name__)
+RANK = int(os.environ["RANK"])
+WORLD_SIZE = int(os.environ["WORLD_SIZE"])
+
+class MovingAverage:
+    def __init__(self, window_size: int):
+        self._window_size = window_size
+        self._window = np.zeros(window_size)
+        self._pointer = 0
+        self._size = 0
+
+    @property
+    def window_size(self):
+        return self._window_size
+    
+    @property
+    def size(self):
+        return self._size
+
+    def update(self, value: float):
+        self._window[self._pointer] = value
+        self._pointer = (self._pointer + 1) % self._window_size
+        self._size = min(self._size + 1, self._window_size)
+
+    def get(self):
+        if self._size == 0:
+            raise ValueError("No data in the moving average window. self._size == 0")
+        return self._window.sum() / self._size, (self._window.sum(), self._size)
+
+
+class ConvToNum:
+    def __init__(self, failure_rate_moving_average_window_size: int = 10000):
+        self._failure_rate   = MovingAverage(failure_rate_moving_average_window_size)
+        self._change_rate    = MovingAverage(failure_rate_moving_average_window_size)
+        self._no_answer_rate = MovingAverage(failure_rate_moving_average_window_size)
+        self._converter_inst = text2digits.Text2Digits(convert_ordinals=False)
+        self._answer_pat     = re.compile(r"(\d+(,\d+)*)(\.\d+)?")
+
+    @classmethod
+    def _log_diffs(cls, *, level, initial, text, converted, final, change_stats):
+        text_diff = "[on black] ".join([
+            (f"[on black]{pre_conv}" if pre_conv == conv else f"[on red]{pre_conv}") 
+            for pre_conv, conv in zip(text.split(), converted.split())
+        ])
+        converted_diff = "[on black] ".join([
+            (f"[on black]{conv}" if pre_conv == conv else f"[on red]{conv}") 
+            for pre_conv, conv in zip(text.split(), converted.split())
+        ])
+        final_diff = "".join([
+            (f"[on black]{final_}" if final_ == conv else f"[on red]{final_}") 
+            for final_, conv in zip(
+                final, 
+                converted
+            )
+        ])
+
+        if change_stats:
+            change_str = f"{change_stats[0]:0.1%}, {change_stats[1][0]}/{change_stats[1][1]}"
+        else:
+            change_str = ""
+
+        LOGGER.log(
+            level,
+            f"\n[green bold]Converted words to numbers.\n" +
+            (f"change_stats: {change_str}\n" if change_stats else "") +
+            f"{initial}\n" +
+            f"[green]-> with words ->[white]\n" +
+            f"{text_diff}\n" +
+            f"[green]-> with numbers ->[white]\n" +
+            f"{converted_diff}\n" +
+            (
+                "" if converted != final else 
+                f"[green]-> final ->[white]\n{final_diff}\n"
+            )+ 
+            "#" * 80 + "\n" 
+        )
+
+    @classmethod
+    def _log_answers(cls, *, level, initial_answer, intermediate_answer, new_answer, final_answer):
+        LOGGER.log(
+            level,
+            "\n" +
+            "#" * 80 + "\n" + 
+            f"[bold blue]initial_answer:[white]       \"{initial_answer}\"\n" +
+            f"[bold blue]intermediate_answer:[white]  \"{intermediate_answer}\"\n" +
+            f"[bold blue]new_answer:[white]           \"{new_answer}\"\n" +
+            f"[bold blue]final_answer:[white]         \"{final_answer}\"\n" 
+        )
+
+    def conv_words_to_numbers(self, text: str) -> str:
+
+        #######################################################################
+        # First we do a variety of fixes to the text to make the inputs 
+        # compatible with the text2digits library.
+        #######################################################################
+
+        initial = text
+
+        # A)
+        # Remove the commas in the numbers. They break the text2digits library.
+        # 1,000 -> 1000
+
+        text_ = re.sub(r"(\d),(\d)", r"\1\2", text)
+        if text_ != text:
+            LOGGER.debug(
+                "[bold blue]A) Removed commas in numbers:\n"
+                f"[white]{text}\n"
+                f"[green]{text_}\n"
+            )
+            text = text_
+
+        # B)
+        # Add a zero to the start of decimal numbers that start with a dot. 
+        # It breaks the text2digits library.
+        # .5 -> 0.5
+
+        text_ = re.sub(r"(?P<nan>\D)\.(?P<num>\d)", "\g<nan> 0.\g<num>", text)
+        if text_ != text:
+            LOGGER.debug(
+                "[bold blue]B) Added one or more zeros:\n"
+                f"[white]{text}\n"
+                f"[green]{text_}\n"
+            )
+            text = text_
+
+        # C)
+        # If a number is followed by a dot and a non number character or nothing, 
+        # text2digits breaks. We add a space between the number and the dot.
+        # 1. -> 1 .
+
+        text_ = re.sub(r"(\d)\.(?P<non_num>\D|$)", r"\1 .\g<non_num>", text)
+        if text_ != text:
+            LOGGER.debug(
+                "[bold blue]C) Added a space after the dot:\n"
+                f"[white]{text}\n"
+                f"[green]{text_}\n"
+            )
+            text = text_
+
+        # D)
+        # A dollat sign followed by a decimal number breaks the text2digits library.
+        # $1.5 -> $ 1.5
+        text_ = re.sub(r"\$(\d+)\.(?P<non_num>\d|$)", r"$ \1.\g<non_num>", text)
+        if text_ != text:
+            LOGGER.debug(
+                "[bold blue]D) Added a space after the dot:\n"
+                f"[white]{text}\n"
+                f"[green]{text_}\n"
+            )
+            text = text_
+
+        # E)
+        # Add space between the last number and a percentage sign.
+        text_ = re.sub(r"(\d)%", r"\1 %", text)
+        if text_ != text:
+            LOGGER.debug(
+                "[bold blue]E) Added a space before the percentage sign:\n"
+                f"[white]{text}\n"
+                f"[green]{text_}\n"
+            )
+            text = text_
+
+        #######################################################################
+        # Now we attempt to convert the words to numbers.
+        #######################################################################
+        try:
+            converted = self._converter_inst.convert(text)
+        except (ValueError, ArithmeticError):
+            self._failure_rate.update(1)
+            ratio, (sum_, size) = self._failure_rate.get()
+            LOGGER.debug(
+                f"\n[red bold] Failed to convert words to numbers. "
+                f"(Failure rate: {ratio:0.2%}, {int(sum_)}/{size}):\n[white]"
+                f"{text}"
+            )
+            return initial
+
+        
+        # Undo the changes we made to the text to make it compatible with the
+        # text2digits library.
+
+        final = re.sub(r"(\d) \.", r"\1.", converted)
+
+        #######################################################################
+        # Now we decide if it's worth returning the converted text.
+        #######################################################################
+        self._failure_rate.update(0)
+
+        initial_answer      = self.extract_answer(initial)
+        initial_answer      = initial_answer.group(0) if initial_answer else None
+        intermediate_answer = self.extract_answer(text)
+        intermediate_answer = intermediate_answer.group(0) if intermediate_answer else None
+        new_answer          = self.extract_answer(converted)
+        new_answer          = new_answer.group(0) if new_answer else None
+        final_answer        = self.extract_answer(final)
+        final_answer        = final_answer.group(0) if final_answer else None
+
+        self._log_answers(
+            level=logging.DEBUG,
+            initial_answer=initial_answer,
+            intermediate_answer=intermediate_answer,
+            new_answer=new_answer,
+            final_answer=final_answer,
+        )
+
+        self._log_diffs(
+            level=logging.DEBUG,
+            initial=initial, 
+            text=text, 
+            converted=converted, 
+            final=final,
+            change_stats=None,
+        )
+
+        # We only return the text if it has changed.
+        if intermediate_answer != new_answer:
+            self._change_rate.update(1)
+            self._log_diffs(
+                level=logging.DEBUG,
+                initial=initial,
+                text=text,
+                converted=converted,
+                final=final,
+                change_stats=self._change_rate.get(),
+            )
+            self._change_rate.update(0)
+            if final_answer is None or not final_answer:
+                self._no_answer_rate.update(1)
+                ratio, (sum_, size) = self._no_answer_rate.get()
+                LOGGER.info(
+                    f"[red bold] No answer found in final text. {ratio:0.1%} {sum_}/{size}"
+                )
+            else:
+                self._no_answer_rate.update(0)
+            return final
+        else:
+            self._change_rate.update(0)
+            if initial_answer is None or not initial_answer:
+                self._no_answer_rate.update(1)
+            else:
+                self._no_answer_rate.update(0)
+            return initial
+
+    def extract_answer(self, text: str) -> re.Match:
+        
+        output = more_itertools.last(
+            re.finditer(self._answer_pat, text),
+            default=None,
+        )
+
+        if output is None:
+            # LOGGER.debug(f"[dark_orange bold on white]EXTRACT ANSWER FAILED: `{text}`")
+            pass
+
+        return output
+
+    def __call__(self, text: str) -> str:
+        return self.conv_words_to_numbers(text)
+
+
+class ASDivRaw(torch.utils.data.Dataset):
+    def __init__(
+        self,
+        cache_path,
+        url="https://raw.githubusercontent.com/chaochun/nlu-asdiv-dataset/master/dataset/ASDiv.xml",
+        quiet=False,
+    ):
+
+        super().__init__()
+        self._cache_path = Path(cache_path)
+        self._url = url
+
+        if not self._cache_path.exists():
+            if not quiet:
+                print("Downloading dataset...")
+            wget.download(self._url, out=str(self._cache_path), bar=None)
+            if not quiet:
+                print("Download complete.")
+
+        if not quiet:
+            print("Parsing dataset...")
+
+        with self._cache_path.open() as fp:
+            root = xml.etree.ElementTree.parse(fp).getroot()[0]
+            self._data = [
+                {element.tag: element.text for element in x} | dict(x.items())
+                for x in root
+            ]
+
+        if not quiet:
+            print("Parsing complete.")
+
+    def __len__(self):
+        return len(self._data)
+
+    def __getitem__(self, index):
+        return self._data[index]
+
+
+class ASDiv:
+    def __init__(self, *args, **kwargs):
+        
+        assert False
+
+        self._ds = ASDivRaw(*args, **kwargs)
+        self._extra_info = {}
+
+        for inner_item in self._ds:
+            new_keys = {"question", "answer"}
+            assert not any(k in inner_item for k in new_keys), new_keys - (
+                new_keys & set(inner_item)
+            )
+        super().__init__()
+        
+    def preprocess_question(self, question: str) -> str:
+        tokenized = self._tokenizer(
+            question, add_special_tokens=False,
+        )
+        assert isinstance(tokenized["input_ids"], list), f"{type(tokenized['input_ids']).mro() = }"
+        assert isinstance(tokenized["input_ids"][0], int), f"{type(tokenized['input_ids'][0]).mro() = }"
+
+        return self._tokenizer.decode(
+            tokenized["input_ids"], skip_special_tokens=True,
+        ).strip()
+
+    def __len__(self) -> int:
+        return len(self._ds)
+
+    def _get_indiv_item(self, index) -> str:
+        inner_item = self._ds[index]
+        sample = self.preprocess_question(
+            inner_item["Body"] + " " + inner_item["Question"]
+        )
+        
+        self._extra_info[sample] = {
+            "answer": inner_item["Answer"],
+            "scratchpad": inner_item["Formula"],
+        } | inner_item
+        
+
+        assert isinstance(sample, str)
+        return sample
+
+    def __getitem__(self, idx_or_slice: typing.Union[int, slice]) -> typing.Union[str, list[str]]:
+        if isinstance(idx_or_slice, int):
+            return self._get_indiv_item(idx_or_slice)
+
+        elif isinstance(idx_or_slice, slice):
+            return [self._get_indiv_item(i) for i in range(
+                idx_or_slice.start, idx_or_slice.stop, idx_or_slice.step)
+            ]
+        
+
+class GSM8K:
+    _int_patt = re.compile(r"\-?\d+")
+
+    def __init__(
+        self, 
+        *,
+        max_length: int,
+        tokenizer: transformers.PreTrainedTokenizerBase, 
+        ds: collections.abc.Sequence[str],
+    ):
+
+        self._outputs_key = "answer"
+        self._inputs_key  = "question"
+        self._max_length  = max_length
+        self._tokenizer   = tokenizer
+        self._populate_ds(ds)
+
+    def _populate_ds(self, ds):
+        samples = []
+        outputs = []
+
+        for idx in range(len(ds)):
+            sample = self.preprocess_question(ds[idx][self._inputs_key])
+            output = ds[idx][self._outputs_key].rsplit("####", 1)[1].strip()
+            samples.append(sample)
+            outputs.append(output.replace(",", ""))
+
+        LOGGER.info("> Tokenizing.")
+        tokenized_samples = self._tokenizer(samples)["input_ids"]
+        tokenized_outputs = self._tokenizer(outputs)["input_ids"]
+        LOGGER.info("< Done Tokenizing.")
+
+        self._samples = []
+        self._outputs = []
+        
+        for t_s, t_o in more_itertools.zip_equal(
+            tokenized_samples, 
+            tokenized_outputs, 
+        ):
+            if len(t_s) <= self._max_length:
+                self._samples.append(t_s)
+                self._outputs.append(t_o)
+
+        LOGGER.info(
+            f"[red bold]With len {self._max_length} - "
+            f"Kept {len(self._samples)  /  len(samples):0.1%} samples, "
+            f"{     len(self._samples)} / {len(samples)}"
+        )
+
+    def __len__(self):
+        assert len(self._samples) == len(self._outputs), (
+            f"{len(self._samples) = }, {len(self._outputs) = }")
+        return len(self._samples)
+
+    def __getitem__(
+            self, idx_or_slice: typing.Union[int, slice]
+        ) -> typing.Union[str, list[str]]:
+        return self._samples[idx_or_slice], self._outputs[idx_or_slice]
+        
+
+
+class ArithmeticDummyDS:
+    """
+    
+    This is a terrible hack just to test with the previous project's data
+    out of the box. We aren't using this data in the actual project.
+
+    """
+
+    def __init__(self, ds, tokenizer):
+        self._ds = ds
+        self._tokenizer = tokenizer
+
+    def __len__(self):
+        return len(self._ds)
+
+    def __getitem__(self, idx):
+        assert False
+        return {
+            "inputs": self._tokenizer.decode(
+                self._ds[idx]["input"], skip_special_tokens=True
+            ),
+            "labels": self._tokenizer.decode(
+                self._ds[idx]["value"], skip_special_tokens=True
+            ),
+        }
+
+class DatasetChoices(str, enum.Enum):
+    ASDIV = "asdiv"
+    GSM8K = "gsm8k"
+    ARITHMETIC_DUMMY = "arithmetic_dummy"
+
+
+DATASET_KEY_TO_CLASS = {
+    DatasetChoices.ASDIV: ASDiv,
+    DatasetChoices.GSM8K: GSM8K,
+    DatasetChoices.ARITHMETIC_DUMMY: ArithmeticDummyDS,
+}
\ No newline at end of file
diff --git a/trl/lib_metric.py b/trl/lib_metric.py
new file mode 100644
index 0000000..70adee4
--- /dev/null
+++ b/trl/lib_metric.py
@@ -0,0 +1,170 @@
+import logging
+import math
+import os
+import re
+from typing import *
+import typing
+
+from beartype import beartype
+import datasets
+import numpy as np
+import pandas as pd
+import rich
+import torch
+import tqdm
+import transformers
+
+import general_utils as utils
+import lib_data
+
+
+LOGGER = logging.getLogger(__name__)
+RANK = int(os.environ["RANK"])
+LOCAL_RANK = int(os.environ["LOCAL_RANK"])
+
+
+class ScratchpadAnswerAccuracy:
+    def __init__(
+        self, *, 
+        extra_info_engine,
+    ):
+        super().__init__()
+        self._num_conv_instance = lib_data.ConvToNum()
+        self._extract_answer    = self._num_conv_instance.extract_answer
+        self._extra_info_fn     = extra_info_engine
+
+    def _make_comparable(
+        self, 
+        match: re.Match, 
+        original_text: typing.Optional[str] = None
+    ) -> Optional[float]:
+        
+        if match is None:
+            return None
+        
+        assert isinstance(match, re.Match), type(match).mro()
+        try:
+            converted = float(match.group(0))
+        except ValueError:
+            try:
+                converted = float(match.group(0).replace(",", ""))
+            except ValueError:
+                LOGGER.info(
+                    f"[red bold]ValueError: [white]"
+                    f"`{match.group(0).replace(',', '') = }` "
+                    f"`{original_text = }` "
+                )
+                return None
+        
+        return converted
+
+    def __call__(
+        self,
+        prompts: List[str],
+        samples: List[str],
+        outputs: List[str],
+    ):
+        assert prompts
+        assert outputs
+
+        extra_info = self._extra_info_fn(
+            sample_str=prompts, 
+            miss_ok=False,)
+        generated_texts = outputs
+        reference_texts = [x["answer"] for x in extra_info]
+        assert len(reference_texts) == len(generated_texts), (
+            len(reference_texts),
+            len(generated_texts),)
+        assert len(generated_texts) == len(reference_texts), (
+            len(generated_texts),
+            len(reference_texts),)
+
+        #######################################################################
+        # Compare each sample one by one
+        #######################################################################
+        parsed = [] # Only used for debugging. Convert to a dataframe as needed.
+        em_value = []
+        for ith_sample, (raw_gen, raw_ref) in enumerate(
+            zip(generated_texts, reference_texts)
+        ):
+            # -----------------------------------------------------------------
+            # Prepare the ref
+            # -----------------------------------------------------------------
+            if isinstance(raw_ref, str):
+                raw_ref = [raw_ref]
+            assert isinstance(raw_ref, list)  , type(raw_ref)
+            assert len(raw_ref) == 1          ,  len(raw_ref)
+            assert isinstance(raw_ref[0], str), type(raw_ref[0])
+            extracted_ref = self._extract_answer (raw_ref[0])
+            ref           = self._make_comparable(extracted_ref, raw_ref[0])
+
+            if ref is None:
+                rich.print(
+                    f"[bold red on white]REF IS NONE: "
+                    f"\"{raw_ref = }\" \"{extracted_ref = }\"")
+
+            assert ref is not None, raw_ref
+
+            # -----------------------------------------------------------------
+            # Prepare Gen
+            # -----------------------------------------------------------------
+            assert raw_gen is not None, raw_gen
+            extracted_gen = self._extract_answer(raw_gen)
+            if extracted_gen is not None:
+                gen = self._make_comparable(extracted_gen, raw_gen)
+            else:
+                gen = None
+                
+            parsed.append(dict(
+                gen=gen, ref=ref, gen_text=raw_gen, ref_text=raw_ref[0]
+            ))
+            if gen is not None:
+                em_value.append(1.0 if math.isclose(gen, ref) else 0.0)
+            else:
+                LOGGER.debug(
+                    f"[bold yellow]gen is None:[/] "
+                    f"{raw_gen = } {extracted_gen = }")
+                em_value.append(0.0)
+
+        #######################################################################
+        # Compute stats, do checks and log
+        #######################################################################
+        num_nones_parsed = sum(x["gen"] is None for x in parsed)
+        output = dict(em_accuracy=em_value)
+        assert parsed
+
+        LOGGER.info(
+            f"[bold green]EM Result: [bold white]"
+            f"{np.mean(em_value):0.2%}\n"
+            f"[bold red on white]Fraction of no answer found: "
+            f"{num_nones_parsed / len(generated_texts):0.1%}\n")
+
+        assert len(em_value) == len(generated_texts) == len(reference_texts), (
+            f"\n" + 
+            f"{len(generated_texts)   = }\n" + 
+            f"{len(reference_texts)   = }\n" + 
+            f"{len(em_value)          = }")
+        
+        return output
+
+
+def test():
+    utils.check_equal(convert_to_int("1"), 1)
+    utils.check_equal(convert_to_int("- 1"), -1)
+    utils.check_equal(convert_to_int("1."), 1)
+    utils.check_equal(convert_to_int("-1."), -1)
+    utils.check_equal(convert_to_int("- 1."), -1)
+    utils.check_equal(convert_to_int("1.0"), 1)
+    utils.check_equal(convert_to_int("-1.0"), -1)
+    utils.check_equal(convert_to_int("- 1.0"), -1)
+    utils.check_equal(convert_to_int("1.00"), 1)
+    utils.check_equal(convert_to_int("-1.00"), -1)
+    utils.check_equal(convert_to_int("- 1.00"), -1)
+
+    utils.check_equal(convert_to_int("1.4123"), None)
+    utils.check_equal(convert_to_int("-1.1"), None)
+    utils.check_equal(convert_to_int("- 1.000234"), None)
+
+
+if __name__ == "__main__":
+    test()
diff --git a/trl/lib_reward.py b/trl/lib_reward.py
new file mode 100644
index 0000000..d10a5c9
--- /dev/null
+++ b/trl/lib_reward.py
@@ -0,0 +1,308 @@
+import collections
+import logging
+import os
+import random
+import re
+import typing
+
+import accelerate
+import general_utils
+import more_itertools
+import numpy as np
+import torch
+import transformers
+
+import lib_data
+import lib_metric
+import lib_bisect_tokens
+
+LOGGER = logging.getLogger(__name__)
+LOCAL_RANK = int(os.environ["LOCAL_RANK"])
+
+
+def global_do_checks(model):
+    local_rank = LOCAL_RANK
+    assert model.device.index == local_rank, (
+        f"{model.device.index = }, {local_rank = }"
+    )
+    assert (torch.cuda.current_device() == local_rank), (
+        torch.cuda.current_device(), local_rank)
+    assert torch.distributed.get_backend() == "nccl", (
+        torch.distributed.get_backend())
+    torch.distributed.barrier()
+
+
+def info(message):
+    general_utils.parallel_log(LOGGER, logging.INFO, message)
+
+
+def remove_special_token_ids(
+    input_ids: list[int], tokenizer: transformers.PreTrainedTokenizer
+):
+    """
+    Remove special tokens from the input_ids
+    """
+    all_special_ids    = set(tokenizer.all_special_ids)
+    filtered_input_ids = [x for x in input_ids if x not in all_special_ids]
+
+    assert len(filtered_input_ids) == len(input_ids) - 1, (
+                f"\n"
+                f"{tokenizer.decode(input_ids)          = },\n"
+                f"{tokenizer.decode(filtered_input_ids) = }.\n"
+                f"{len(input_ids)                       = },\n"
+                f"{len(filtered_input_ids)              = },\n"
+            )
+
+    return filtered_input_ids
+
+
+def _maybe_frozen_head(model, input_dict, use_frozen_head):
+    if use_frozen_head:
+        with torch.no_grad():
+            return model(**input_dict)
+    return model(**input_dict)
+
+def _maybe_autocast(model_or_fn, dtype):
+    if dtype is None:
+        return model_or_fn
+    return torch.cuda.amp.autocast(dtype=dtype)(model_or_fn)
+
+
+def clone_hf_model(
+        hf_model: transformers.PreTrainedModel
+) -> transformers.PreTrainedModel:
+    
+    assert isinstance(hf_model, transformers.PreTrainedModel), type(hf_model)
+    copy = type(hf_model)(hf_model.config)
+    copy.load_state_dict(hf_model.state_dict())
+    return copy
+
+
+class ScratchpadRewardFn:
+    def __init__(
+        self, *, 
+        ref_model: typing.Union[str, transformers.PreTrainedModel],
+        tokenizer: transformers.PreTrainedTokenizerBase, 
+        uses_peft: bool,
+        metric_fn,
+    ):
+        super().__init__()
+        
+        #----------------------------------------------------------------
+        # Build Models
+        #----------------------------------------------------------------
+        reward_model = ref_model
+
+        #----------------------------------------------------------------
+        # Set Attributes
+        #----------------------------------------------------------------
+        self._show_answer_replacement = False
+        self._reward_tokenizer        = tokenizer
+        self._no_answer_rate          = lib_data.MovingAverage(10000)
+        self._model                   = reward_model.eval()
+        self._conv_to_num             = lib_data.ConvToNum()
+        self._metric                  = metric_fn
+        self._uses_peft               = uses_peft
+
+    
+    def __call__(self, prompts, samples, outputs, ref_answers):
+        # The idea is to:
+        # 1. Extract the associated answers & tokenize the answers
+        # 2. Create a mask for the answers
+        # 3. Tokenize the samples
+        # 4. Concate the samples & answers
+        # 5. Run the reward model on the concatenated samples & answers
+        # 6. Extract the logp for the answers
+        # 7. Return the logp for the answers
+
+        #######################################################################
+        # - Sanity checks 
+        # - Varia prep
+        #######################################################################
+        output = np.mean(self._metric(prompts, samples, outputs)["em_accuracy"])
+        LOGGER.info(f"[red bold]REWARD - INSTANT PER BATCH METRIC ACC: {output = :0.2%}")
+
+        # Get the answers.
+        question_tok = self._tokenizer(prompts, padding=True, return_tensors="pt")
+
+        timer_flags = dict(
+            disable         = True,
+            cuda_sync       = False,
+            accelerate_sync = False,
+            accelerator     = None,
+            log_level       = logging.INFO,
+            logger          = LOGGER,
+        )
+        
+        #######################################################################
+        # - Replace the number words by digits.
+        # - Find the answer tokens in the generated output.
+        # - Remove it.
+        # - Put the ref answer instead.
+        # - Create a mask over the not-answer for the perplexity.
+        #######################################################################
+        # Replace the number words by digits.
+        scratchpads = []
+        timer = general_utils.ctx_timeit
+        with timer("Replacing the number words by digits", **timer_flags):
+            for i, (output, ref_answer) in enumerate(
+                more_itertools.zip_equal(outputs, ref_answers)
+            ):
+                scratchpads.append(self.replace_answer(
+                    original_generation = self._conv_to_num(output), 
+                    ref_answer          = ref_answer,
+                )[0])
+
+        # Find the answer tokens.
+        with timer("Extracting the answer tokens", **timer_flags):
+            tok_outputs, start_end_outputs, str_matches_outputs = (
+                lib_bisect_tokens.extract_match_tokens(
+                    regexes          = [
+                        re.escape(ref_answer) 
+                        for ref_answer in ref_answers
+                    ],
+                    tokenizer        = self._reward_tokenizer, 
+                    strings          = scratchpads,
+                    tokenizer_kwargs = dict(
+                        return_tensors = "pt", 
+                        padding        = True,
+                    ),
+                    verbose=False,
+                )
+            )
+        
+        assert (torch.cuda.current_device() == torch.distributed.get_rank()), (
+            torch.cuda.current_device(), torch.distributed.get_rank())
+        assert len(start_end_outputs) == len(scratchpads), (
+            f"{len(start_end_outputs) = }, {len(scratchpads) = }")
+        
+        # > Replace the answer tokens by the ref answer.
+        # > Create the masks.
+        masks = []
+        seq_len = tok_outputs["input_ids"].shape[1]
+        with timer("Creating the masks", **timer_flags):
+            for matches, scratchpad_ids, ref_answer in zip(
+                start_end_outputs, tok_outputs["input_ids"], ref_answers
+            ):
+                start, end = matches[-1]
+                mask = [0] * start + [1] * (end + 1 - start) + [0] * (seq_len - end - 1)
+
+                if self._show_answer_replacement:
+                    scratchpad_before = self._reward_tokenizer.decode(
+                        scratchpad_ids, skip_special_tokens=True)
+                    scratchpad_after  = " ".join([
+                        self._reward_tokenizer.decode(token) 
+                        if m == 0 else " <<REF_ANSWER>>" 
+                        for token, m in zip(scratchpad_ids, mask)
+                    ]).replace("<pad>", "")
+
+                    LOGGER.info(
+                        f"[bold blue]ref answer:[white]        {ref_answer}\n"
+                        f"[bold blue]start, end:[white]        {start}, {end}\n"
+                        f"[bold blue]Scratchpad before:[white] {scratchpad_before}\n"
+                        f"[bold blue]Scratchpad after:[white]  {scratchpad_after}\n"
+                    )
+
+                masks.append(torch.tensor(mask).to(tok_outputs["input_ids"].device))
+                assert len(mask) == seq_len, f"{len(mask) = }, {seq_len = }"
+
+        ###########################################################################
+        # 2. Compute the logp for the answers
+        ###########################################################################
+        if self._reward_model.config.is_encoder_decoder:
+            assert self._model.device.type == "cuda", (
+                f"{self._model.device.type = }"
+            )
+            
+            # TODO: Maybe we don't have to recompute the logits. 
+            # We should get them for generation and for training. 
+            with timer("Moving things to GPU", **timer_flags):
+                input_dict = dict(
+                    input_ids              = question_tok["input_ids"     ].to(self._inputs_device),
+                    attention_mask         = question_tok["attention_mask"].to(self._inputs_device),
+                    decoder_input_ids      = tok_outputs ["input_ids"     ].to(self._inputs_device),
+                    decoder_attention_mask = tok_outputs ["attention_mask"].to(self._inputs_device),
+                )
+
+            with timer(
+                f"> Computing the logits with the ref model for the reward." +
+                f"{question_tok['input_ids'].shape = }"
+                , **timer_flags):
+                
+                with torch.no_grad():                    
+                    if self._uses_peft:
+                        with self._model.disable_adapter():
+                            logits = self._model(**input_dict).logits
+                    else:
+                        logits = self._model(**input_dict).logits
+
+        else:
+            # TODO(@julesgm): Fix this
+            assert False
+            logits = self._reward_model(
+                full_seq["input_ids"], attention_mask=full_seq["attention_mask"]
+                ).logits
+
+        with timer("Computing the rest of the reward", **timer_flags):
+
+            reward_model_outputs_all = logits.softmax(-1)
+            idx = tok_outputs["input_ids"].to(reward_model_outputs_all.device)
+            reward_model_outputs_scratchpad = torch.gather(
+                reward_model_outputs_all, -1, idx.unsqueeze(-1)
+            ).squeeze(-1)
+
+            ###########################################################################
+            # 3. Only keep the logp for the actual values used
+            ###########################################################################
+
+            masks        = torch.stack(masks).to(reward_model_outputs_scratchpad.device)
+            probs        = reward_model_outputs_scratchpad.clone()
+            probs[masks == 0] = 1
+            logp         = probs.log()
+            logp_per_seq = logp.sum(-1)
+            final_output = logp_per_seq.detach()
+
+        return final_output, reward_model_outputs_scratchpad
+
+    def replace_answer(self, *, original_generation: str, ref_answer: str) -> tuple[str, int, int]:
+        answer = self._conv_to_num.extract_answer(original_generation)
+        
+        # If the answer is None, then we just add the reference answer at the end.
+        if answer is None:
+            self._no_answer_rate.update(1)
+            ratio, (sum_, size) = self._no_answer_rate.get()
+            LOGGER.info(f"[red bold]No answer: {ratio:.1%} ({sum_}/{size}) ")
+            new_scratchpad = original_generation.strip() + " The answer is: "
+            start_pos      = len(new_scratchpad)
+            end_pos        = start_pos + len(ref_answer)
+            final          = new_scratchpad + ref_answer + "."
+
+            return final, start_pos, end_pos
+
+        self._no_answer_rate.update(0)
+        mode = "in_place"
+
+        if mode == "in_place":
+            # If the answer is not None, then we replace the answer with the reference answer.
+            start = original_generation[:answer.start()]
+            end   = original_generation[answer.end():]
+        elif mode == "remove_end":
+            start = original_generation[:answer.start()]
+            end   = "."
+        else:
+            raise ValueError(f"{mode = }")
+
+        start_pos = len(start)
+        end_pos   = len(start) + len(ref_answer)
+        final     = start + ref_answer + end
+
+        # LOGGER.info(
+        #     "\n"
+        #     f"[bold blue]original:[/]           {original_generation}\n"
+        #     f"[bold blue]ref answer:[/]         {ref_answer}\n"
+        #     f"[bold blue]start:[/]              {start}\n"
+        #     f"[bold blue]end:[/]                {end}\n"
+        #     f"[bold blue]final:[/]              {final}\n"
+        # )
+
+        return final, start_pos, end_pos
\ No newline at end of file
diff --git a/trl/lib_trl_utils.py b/trl/lib_trl_utils.py
new file mode 100644
index 0000000..8b5beff
--- /dev/null
+++ b/trl/lib_trl_utils.py
@@ -0,0 +1,313 @@
+import collections
+import enum
+import os
+import typing
+
+import peft
+import rich
+import torch
+import transformers
+from tqdm import tqdm
+
+import trl
+import trl.core
+import trl.models
+
+
+def get_rank():
+    return int(os.getenv("RANK", "0"))
+
+
+def get_local_rank():
+    return int(os.getenv("LOCAL_RANK", "0"))
+
+
+def get_world_size():
+    return int(os.getenv("WORLD_SIZE", "1"))
+
+
+IntSequence = typing.TypeVar(
+    "IntSequence", 
+    list[int], 
+    torch.LongTensor,
+)
+
+
+IntSequenceContainer = typing.TypeVar(
+    "IntSequenceContainer", 
+    torch.LongTensor, 
+    list[torch.LongTensor], 
+    list[list[int]],
+)
+
+
+def build_tokenizer(model_name):
+    tok = transformers.AutoTokenizer.from_pretrained(model_name, padding_side="left")
+    
+    if "gpt" in model_name.lower():
+        tok.pad_token = tok.eos_token
+    
+    return tok
+
+
+def check_all_start_with_token_id(tensors: IntSequence, token_id: int):
+    """
+    Description: Makes sure all the tensors in the list start with the same token id.
+    
+    Intent: All the decoder inputs of an encoder-decoder model should start with the same, 
+            fixed token id. This is a sanity check to make surethat they do.
+
+    """
+    starts = [r[0].item() for r in tensors]
+    if not all(s == token_id for s in starts):
+        raise ValueError(
+            f"{token_id = }\n"
+            f"{collections.Counter(starts) = }\n"
+        )
+
+
+def print_trainable_parameters(
+    model: torch.nn.Module,
+    do_print: bool = True,
+) -> int:
+    """
+
+    Description: Prints the number of trainable parameters in the model, returns the number.
+
+    Intent: Mostly of use with peft & LoRA, to see how many parameters are actually trainable.
+
+    """
+    trainable_params = 0
+    all_param = 0
+
+    for _, param in model.named_parameters():
+        all_param += param.numel()
+        if param.requires_grad:
+            trainable_params += param.numel()
+
+    if do_print:
+        rich.print(
+            f"[bold blue]({get_rank()}/{get_world_size()}):[/] "
+            f"trainable params: {trainable_params} || "
+            f"all params: {all_param} || "
+            f"trainable%: {100 * trainable_params / all_param}"
+        )
+
+    return trainable_params
+
+
+def init_model(
+    *, 
+    precision, 
+    model_name: str, 
+    lora_config_dict: dict[str, typing.Any],
+    model_type,
+) -> transformers.PreTrainedModel:
+    """
+    
+    Description: Initializes the model, tokenizer, and LoRA config, & does the precision stuff.
+
+    Intent: The LoRA stuff & the precision stuff is repetitive.
+
+    Currently:
+        1. Init the pretrained model
+            -> If int8, prepare for int8 training w/ peft.
+        2. Peft-ize the pretrained model
+        3. Initialize the trl ValueHead model from the peft-model
+
+    .. That's what they use in the example:
+    https://github.com/lvwerra/trl/blob/main/examples/sentiment/scripts/gpt-neox-20b_peft/gpt-neo-20b_sentiment_peft.py#L194
+
+    The question is, why does peft + value-model work with gpt and not t5?
+
+    It further doesn't work with "prepare_for_int8" training, but that's not the question right now.
+        
+    """
+    
+    lora_config = peft.LoraConfig(**lora_config_dict)
+    tokenizer = build_tokenizer(model_name)
+
+    ###########################################################################
+    # Model Class Specific Options
+    ###########################################################################
+    if model_type == peft.TaskType.CAUSAL_LM:
+        lora_config.task_type = peft.TaskType.CAUSAL_LM
+        transformers_cls      = transformers.AutoModelForCausalLM
+        trl_cls               = trl.models.  AutoModelForCausalLMWithValueHead
+        transformers.GPTNeoForCausalLM
+
+        tokenizer.pad_token   = tokenizer.eos_token
+        dmap_keys = ["transformer", "lm_head"]
+
+    elif model_type == peft.TaskType.SEQ_2_SEQ_LM:
+        lora_config.task_type = peft.TaskType.SEQ_2_SEQ_LM
+        transformers_cls      = transformers.AutoModelForSeq2SeqLM
+        trl_cls               = trl.models.  AutoModelForSeq2SeqLMWithValueHead
+        transformers.T5ForConditionalGeneration
+
+        assert "t5" in model_name.lower(), model_name
+        dmap_keys = ["encoder", "decoder", "lm_head", "shared"]
+
+    else:
+        raise ValueError(model_name)
+
+    ###########################################################################
+    # Init the Pre-Trained Model
+    # -> Precision specific
+    ###########################################################################
+    if precision == "int8":
+        dmap = {k: int(get_local_rank()) for k in dmap_keys}
+        pretrained_model = transformers_cls.from_pretrained(
+            model_name, 
+            load_in_8bit = True, 
+            device_map   = dmap
+        )
+        # https://github.com/huggingface/peft/blob/main/src/peft/utils/other.py#L35
+        # Casts the layer norm to fp32 for stability purposes
+        # Upcasts lm_head to fp32 for stability purposes
+        # Make the output embedding layer require grads 
+
+        pretrained_model = peft.prepare_model_for_int8_training(
+            pretrained_model, 
+            output_embedding_layer_name="lm_head"
+        )
+
+    else:
+        assert precision in [torch.float16, torch.bfloat16], precision 
+        pretrained_model = transformers_cls.from_pretrained(
+            model_name,
+            torch_dtype=precision,
+        )
+
+    ###########################################################################
+    # Model Instance Specific Fixes
+    ###########################################################################
+    if "gpt-neox" in model_name.lower():
+        assert False
+        # workaround to use 8bit training on this model
+        # hacky workaround due to issues with "EleutherAI/gpt-neox-20b"
+        lora_config.target_modules = ["query_key_value", "xxx"]  
+
+        for name, param in pretrained_model.named_parameters():
+            # freeze base model's layers
+            param.requires_grad = False
+
+            if getattr(pretrained_model, "is_loaded_in_8bit", False):
+                # cast layer norm in fp32 for stability for 8bit models
+                if param.ndim == 1 and "layer_norm" in name:
+                    param.data = param.data.to(torch.float16)
+    
+    ###########################################################################
+    # 
+    ###########################################################################
+    peft_model = peft.get_peft_model(pretrained_model, lora_config,)
+
+    if precision == "int8":
+        model = trl_cls.from_pretrained(peft_model)
+    else:
+        peft_model.to(precision)
+        model = trl_cls.from_pretrained(peft_model, torch_dtype=precision)
+        model.to(precision)
+
+    model.gradient_checkpointing_disable = model.pretrained_model.gradient_checkpointing_disable
+    model.gradient_checkpointing_enable  = model.pretrained_model.gradient_checkpointing_enable
+    print_trainable_parameters(model)
+
+    assert print_trainable_parameters(model, False) > 0
+
+    return model, tokenizer
+
+
+def unroll(
+    *,
+    output_length_sampler: typing.Optional[trl.core.LengthSampler],
+    generation_kwargs:     dict[str, typing.Any],
+    ppo_trainer:           trl.PPOTrainer,
+    batch:                 dict[str, IntSequence],
+    model:                 transformers.PreTrainedModel,
+) -> IntSequenceContainer:
+    """
+    Requires 
+    """
+
+    # Unroll the policy
+    model.gradient_checkpointing_disable()
+    model.pretrained_model.config.use_cache = True
+    model.eval()
+
+    response_tensors = []  
+    for query in tqdm(
+        batch["input_ids"], 
+        disable = get_rank() != 0,
+        desc    = "Unrolling policy", 
+    ):
+        if output_length_sampler:
+            gen_len = output_length_sampler()
+            generation_kwargs["max_new_tokens"] = gen_len
+
+        with torch.no_grad():
+            response = ppo_trainer.generate(query, **generation_kwargs)
+        response_tensors.append(response.squeeze())
+    
+    model.train()
+    model.gradient_checkpointing_enable()
+    model.pretrained_model.config.use_cache = False
+    
+    exit()
+    return response_tensors
+
+
+def batched_unroll(
+    *,
+    output_length_sampler: typing.Optional[trl.core.LengthSampler],
+    generation_batch_size: int,
+    generation_kwargs:     dict[str, typing.Any],
+    ppo_trainer:           trl.PPOTrainer,
+    tokenizer:             transformers.PreTrainedTokenizer,
+    batch:                 dict[str, IntSequence],
+    model:                 transformers.PreTrainedModel,
+) -> IntSequenceContainer:
+    """
+    Requires 
+    """
+
+    # Unroll the policy
+    model.gradient_checkpointing_disable()
+    model.pretrained_model.config.use_cache = True
+    model.eval()
+    response_tensors = []
+
+
+    if output_length_sampler:
+        gen_len = output_length_sampler()
+        generation_kwargs["max_new_tokens"] = gen_len
+
+    model = ppo_trainer.accelerator.unwrap_model(model)
+
+    for i in tqdm(
+        range(0, len(batch["input_ids"]), generation_batch_size), 
+        disable=get_rank() != 0,
+        desc="Unrolling",
+    ):
+        responses = model.generate(
+            **tokenizer(
+                batch["query"][i:i + generation_batch_size],
+                return_tensors="pt", 
+                padding=True, 
+                truncation=True,
+            ).to(get_local_rank()),
+            **generation_kwargs
+        )
+    
+        for response in responses:
+            response_tensors.append(
+                response[response != tokenizer.pad_token_id]
+            )
+
+    model.train()
+    model.gradient_checkpointing_enable()
+    model.pretrained_model.config.use_cache = False
+    
+    return response_tensors
+
+
diff --git a/trl/links/link_lora.py b/trl/links/link_lora.py
new file mode 120000
index 0000000..ec373ac
--- /dev/null
+++ b/trl/links/link_lora.py
@@ -0,0 +1 @@
+/home/mila/g/gagnonju/.main/lib/python3.9/site-packages/peft/tuners/lora.py
\ No newline at end of file
diff --git a/trl/links/link_modeling_base.py b/trl/links/link_modeling_base.py
new file mode 120000
index 0000000..5a703b5
--- /dev/null
+++ b/trl/links/link_modeling_base.py
@@ -0,0 +1 @@
+/home/mila/g/gagnonju/.main/lib/python3.9/site-packages/trl/models/modeling_base.py
\ No newline at end of file
diff --git a/trl/links/link_modeling_value_head.py b/trl/links/link_modeling_value_head.py
new file mode 120000
index 0000000..e4be3a8
--- /dev/null
+++ b/trl/links/link_modeling_value_head.py
@@ -0,0 +1 @@
+/home/mila/g/gagnonju/.main/lib/python3.9/site-packages/trl/models/modeling_value_head.py
\ No newline at end of file
diff --git a/trl/links/link_ppo_trainer.py b/trl/links/link_ppo_trainer.py
new file mode 120000
index 0000000..656d402
--- /dev/null
+++ b/trl/links/link_ppo_trainer.py
@@ -0,0 +1 @@
+/home/mila/g/gagnonju/.main/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py
\ No newline at end of file
diff --git a/trl/nb_pipeline_test.ipynb b/trl/nb_pipeline_test.ipynb
new file mode 100644
index 0000000..a1e0aa1
--- /dev/null
+++ b/trl/nb_pipeline_test.ipynb
@@ -0,0 +1,94 @@
+{
+ "cells": [
+  {
+   "cell_type": "code",
+   "execution_count": 2,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import os\n",
+    "\n",
+    "import torch\n",
+    "import transformers"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 15,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "pipe = transformers.pipeline(\n",
+    "    \"sentiment-analysis\", \n",
+    "    model=\"lvwerra/distilbert-imdb\", \n",
+    "    device=0,\n",
+    ")\n",
+    "\n",
+    "kwargs = {\n",
+    "    \"truncation\":        True,\n",
+    "    \"batch_size\":        16, \n",
+    "    \"top_k\":             None, \n",
+    "}"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 17,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "[{'label': 'POSITIVE', 'score': 0.9910256266593933}, {'label': 'NEGATIVE', 'score': 0.008974345400929451}]\n"
+     ]
+    },
+    {
+     "ename": "",
+     "evalue": "",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
+     ]
+    }
+   ],
+   "source": [
+    "pipe_outputs = pipe(\n",
+    "    \"This movie is interesting, in that just how so crazy how insanely I love it\", \n",
+    "    **kwargs,\n",
+    ")\n",
+    "\n",
+    "print(pipe_outputs)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": ".main",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.9.15"
+  },
+  "orig_nbformat": 4
+ },
+ "nbformat": 4,
+ "nbformat_minor": 2
+}
diff --git a/trl/nb_t5_gpt_comparison.ipynb b/trl/nb_t5_gpt_comparison.ipynb
new file mode 100644
index 0000000..5b04b84
--- /dev/null
+++ b/trl/nb_t5_gpt_comparison.ipynb
@@ -0,0 +1,628 @@
+{
+ "cells": [
+  {
+   "cell_type": "code",
+   "execution_count": 1,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "\n",
+      "===================================BUG REPORT===================================\n",
+      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
+      "================================================================================\n",
+      "CUDA SETUP: CUDA runtime path found: /cvmfs/ai.mila.quebec/apps/arch/common/cuda/11.7/lib64/libcudart.so\n",
+      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
+      "CUDA SETUP: Detected CUDA version 117\n",
+      "CUDA SETUP: Loading binary /home/mila/g/gagnonju/.main/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "/home/mila/g/gagnonju/.main/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/mila/g/gagnonju/local_cudnn/cudnn-linux-x86_64-8.5.0.96_cuda11-archive/lib')}\n",
+      "  warn(msg)\n"
+     ]
+    }
+   ],
+   "source": [
+    "import enum\n",
+    "import os\n",
+    "import peft\n",
+    "import rich\n",
+    "import torch\n",
+    "import transformers\n",
+    "import trl"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 2,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "class ModelTypes(str, enum.Enum):\n",
+    "    CAUSAL_LM = \"causal_lm\"\n",
+    "    SEQ_2_SEQ_LM = \"seq_2_seq_lm\"\n",
+    "\n",
+    "class ModelTokenizerPair:\n",
+    "    def __init__(\n",
+    "        self, \n",
+    "        hf_name=None, \n",
+    "        model_cls=None, \n",
+    "        default_gen_kwargs=None, \n",
+    "        device=int(os.getenv(\"LOCAL_RANK\", \"0\")), \n",
+    "        init_kwargs=None,\n",
+    "    ):\n",
+    "\n",
+    "        if default_gen_kwargs is None:\n",
+    "            default_gen_kwargs = dict(\n",
+    "                max_new_tokens=100,\n",
+    "            )\n",
+    "        \n",
+    "        self._init_kwargs = init_kwargs\n",
+    "        self._peft_initialized = False\n",
+    "        self._trl_initialized = False\n",
+    "        self._default_gen_kwargs = default_gen_kwargs\n",
+    "        self.device = device\n",
+    "\n",
+    "        if hf_name is not None:\n",
+    "            self.model = model_cls.from_pretrained(hf_name, **init_kwargs)\n",
+    "            self.tokenizer = transformers.AutoTokenizer.from_pretrained(hf_name)\n",
+    "\n",
+    "            if \"gpt\" in hf_name.lower():\n",
+    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
+    "                self._default_gen_kwargs[\"pad_token_id\"] = self.model.config.eos_token_id\n",
+    "                self._model_type = ModelTypes.CAUSAL_LM\n",
+    "\n",
+    "            else:\n",
+    "                assert \"t5\" in hf_name.lower()\n",
+    "                self._model_type = ModelTypes.SEQ_2_SEQ_LM\n",
+    "        \n",
+    "        self.m = self.model\n",
+    "        self.t = self.tokenizer\n",
+    "        \n",
+    "\n",
+    "    def to(self, *args, **kwargs):\n",
+    "        self.model.to(*args, **kwargs)\n",
+    "        return self\n",
+    "\n",
+    "    def cuda(self, *args, **kwargs):\n",
+    "        self.model.cuda(*args, **kwargs)\n",
+    "        return self\n",
+    "\n",
+    "    def cpu(self, *args, **kwargs):\n",
+    "        self.model.cpu(*args, **kwargs)\n",
+    "        return self\n",
+    "\n",
+    "    def gen_from_text(self, text, gen_kwargs, to_text=True):\n",
+    "        if gen_kwargs is None:\n",
+    "            gen_kwargs = {}\n",
+    "\n",
+    "        tokenized = self.tokenizer(\n",
+    "            text, \n",
+    "            return_tensors=\"pt\", \n",
+    "            padding=True, \n",
+    "            truncation=True,\n",
+    "        ).to(self.device)\n",
+    "        \n",
+    "        output = self.generate(\n",
+    "            **tokenized,\n",
+    "            **gen_kwargs,\n",
+    "        )\n",
+    "        if self._model_type == ModelTypes.CAUSAL_LM:\n",
+    "            output = output[:, tokenized[\"input_ids\"].shape[-1]:]\n",
+    "        \n",
+    "        if to_text:\n",
+    "            return self.tokenizer.batch_decode(output)\n",
+    "        \n",
+    "        return output\n",
+    "\n",
+    "    def generate(self, *args, **gen_kwargs):\n",
+    "        if gen_kwargs is None:\n",
+    "            gen_kwargs = self._default_gen_kwargs\n",
+    "        else:\n",
+    "            gen_kwargs = self._default_gen_kwargs | gen_kwargs\n",
+    "        return self.model.generate(*args, **gen_kwargs)\n",
+    "\n",
+    "    def text_to_text(self, text, gen_kwargs=None):\n",
+    "        return self.gen_from_text(text, gen_kwargs, to_text=True)\n",
+    "\n",
+    "    def text_to_ids(self, text, gen_kwargs=None):\n",
+    "        return self.gen_from_text(text, gen_kwargs, to_text=False)\n",
+    "\n",
+    "    def ids_to_text(self, model_inputs, gen_kwargs=None):\n",
+    "        if gen_kwargs is None:\n",
+    "            gen_kwargs = {}\n",
+    "        generated = self.generate(**model_inputs, **gen_kwargs)\n",
+    "        return self.tokenizer.batch_decode(generated)\n",
+    "\n",
+    "    def init_peft(self, peft_config):\n",
+    "        self._peft_initialized = True\n",
+    "        rich.print(\n",
+    "            f\"[red bold]init_trl: \"\n",
+    "            f\"{self._peft_initialized = } \"\n",
+    "            f\"{self._trl_initialized = }\"\n",
+    "        )\n",
+    "        self.model = peft.get_peft_model(model=self.model, peft_config=peft_config)\n",
+    "\n",
+    "    def init_trl(self):\n",
+    "        self._trl_initialized = True\n",
+    "        rich.print(\n",
+    "            f\"[red bold]init_trl: \"\n",
+    "            f\"{self._peft_initialized = } \"\n",
+    "            f\"{self._trl_initialized = }\"\n",
+    "        )\n",
+    "        if self._model_type == ModelTypes.CAUSAL_LM:\n",
+    "            trl_cls = trl.models.AutoModelForCausalLMWithValueHead\n",
+    "\n",
+    "        elif self._model_type == ModelTypes.SEQ_2_SEQ_LM:\n",
+    "            trl_cls = trl.models.AutoModelForSeq2SeqLMWithValueHead\n",
+    "\n",
+    "        self.model = trl_cls.from_pretrained(self.model)\n",
+    "\n",
+    "    def __call__(self, *args, **kwds) -> torch.Tensor:\n",
+    "        return self.model(*args, **kwds)\n",
+    "\n",
+    "    def forward_from_text(self, text: str, decoder_text = None) -> torch.Tensor:\n",
+    "        inputs = self.t(\n",
+    "            text, \n",
+    "            padding        = True, \n",
+    "            truncation     = True,\n",
+    "            return_tensors = \"pt\", \n",
+    "        ).to(self.device)\n",
+    "        \n",
+    "        if not decoder_text is None:\n",
+    "            assert self._model_type == ModelTypes.SEQ_2_SEQ_LM, self._model_type\n",
+    "\n",
+    "            decoder_inputs = self.t(\n",
+    "                decoder_text,\n",
+    "                padding        = True,\n",
+    "                truncation     = True,\n",
+    "                return_tensors = \"pt\",\n",
+    "            ).to(self.device)\n",
+    "\n",
+    "            inputs = dict(\n",
+    "                input_ids              = inputs[\"input_ids\"],\n",
+    "                attention_mask         = inputs[\"attention_mask\"],\n",
+    "                decoder_input_ids      = decoder_inputs[\"input_ids\"],\n",
+    "                decoder_attention_mask = decoder_inputs[\"attention_mask\"],\n",
+    "            )\n",
+    "        else:\n",
+    "            assert self._model_type == ModelTypes.CAUSAL_LM, self._model_type\n",
+    "\n",
+    "        for v in inputs.values():\n",
+    "            assert v.device.type == \"cuda\", v.device\n",
+    "            \n",
+    "        return self.model(**inputs)\n",
+    "    \n",
+    "    def train(self, *args, **kwargs):\n",
+    "        return self.model.train(*args, **kwargs)\n",
+    "    \n",
+    "    def training(self, *args, **kwargs):\n",
+    "        return self.model.training(*args, **kwargs)\n",
+    "    \n",
+    "    def eval(self, *args, **kwargs):\n",
+    "        return  self.model.eval(*args, **kwargs)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 3,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "DTYPE = torch.bfloat16\n",
+    "\n",
+    "if DTYPE in (torch.float16, torch.bfloat16):\n",
+    "    init_kwargs = dict(\n",
+    "        torch_dtype=DTYPE,\n",
+    "    )\n",
+    "elif DTYPE is None:\n",
+    "    init_kwargs = {}\n",
+    "else: \n",
+    "    raise ValueError(f\"Invalid DTYPE: {DTYPE}\")\n",
+    "\n",
+    "\n",
+    "t5  = ModelTokenizerPair(\n",
+    "    hf_name=\"google/flan-t5-small\", \n",
+    "    model_cls=transformers.AutoModelForSeq2SeqLM, \n",
+    "    init_kwargs=init_kwargs,\n",
+    ").cuda()\n",
+    "\n",
+    "gpt = ModelTokenizerPair(\n",
+    "    hf_name=\"edbeeching/gpt-neo-125M-imdb-lora-adapter-merged\", \n",
+    "    model_cls=transformers.AutoModelForCausalLM,\n",
+    "    init_kwargs=init_kwargs,\n",
+    ").cuda()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 4,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/html": [
+       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Nothing Applied</span>\n",
+       "</pre>\n"
+      ],
+      "text/plain": [
+       "\u001b[1;32mNothing Applied\u001b[0m\n"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "[\" I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know.\"]\n",
+      "['<pad> blue</s>']\n"
+     ]
+    },
+    {
+     "data": {
+      "text/html": [
+       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">init_trl: self._peft_initialized = </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">True</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> self._trl_initialized = </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">False</span>\n",
+       "</pre>\n"
+      ],
+      "text/plain": [
+       "\u001b[1;31minit_trl: self._peft_initialized = \u001b[0m\u001b[1;3;31mTrue\u001b[0m\u001b[1;31m self._trl_initialized = \u001b[0m\u001b[1;3;31mFalse\u001b[0m\n"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">              </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold; font-style: italic\">INNERMOST PEFT-CONFIG:</span><span style=\"font-style: italic\">              </span>\n",
+       "┏━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n",
+       "┃<span style=\"font-weight: bold\"> Key                     </span>┃<span style=\"font-weight: bold\"> Value                </span>┃\n",
+       "┡━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n",
+       "│ <span style=\"color: #008000; text-decoration-color: #008000\">base_model_name_or_path</span> │ google/flan-t5-small │\n",
+       "│ <span style=\"color: #008000; text-decoration-color: #008000\">bias</span>                    │ none                 │\n",
+       "│ <span style=\"color: #008000; text-decoration-color: #008000\">enable_lora</span>             │ None                 │\n",
+       "│ <span style=\"color: #008000; text-decoration-color: #008000\">fan_in_fan_out</span>          │ False                │\n",
+       "│ <span style=\"color: #008000; text-decoration-color: #008000\">inference_mode</span>          │ False                │\n",
+       "│ <span style=\"color: #008000; text-decoration-color: #008000\">lora_alpha</span>              │ 32                   │\n",
+       "│ <span style=\"color: #008000; text-decoration-color: #008000\">lora_dropout</span>            │ 0.05                 │\n",
+       "│ <span style=\"color: #008000; text-decoration-color: #008000\">merge_weights</span>           │ False                │\n",
+       "│ <span style=\"color: #008000; text-decoration-color: #008000\">modules_to_save</span>         │ None                 │\n",
+       "│ <span style=\"color: #008000; text-decoration-color: #008000\">peft_type</span>               │ LORA                 │\n",
+       "│ <span style=\"color: #008000; text-decoration-color: #008000\">r</span>                       │ 16                   │\n",
+       "│ <span style=\"color: #008000; text-decoration-color: #008000\">target_modules</span>          │ ['q', 'v']           │\n",
+       "│ <span style=\"color: #008000; text-decoration-color: #008000\">task_type</span>               │ SEQ_2_SEQ_LM         │\n",
+       "└─────────────────────────┴──────────────────────┘\n",
+       "</pre>\n"
+      ],
+      "text/plain": [
+       "\u001b[3m              \u001b[0m\u001b[1;3;34mINNERMOST PEFT-CONFIG:\u001b[0m\u001b[3m              \u001b[0m\n",
+       "┏━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n",
+       "┃\u001b[1m \u001b[0m\u001b[1mKey                    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mValue               \u001b[0m\u001b[1m \u001b[0m┃\n",
+       "┡━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n",
+       "│ \u001b[32mbase_model_name_or_path\u001b[0m │ google/flan-t5-small │\n",
+       "│ \u001b[32mbias\u001b[0m                    │ none                 │\n",
+       "│ \u001b[32menable_lora\u001b[0m             │ None                 │\n",
+       "│ \u001b[32mfan_in_fan_out\u001b[0m          │ False                │\n",
+       "│ \u001b[32minference_mode\u001b[0m          │ False                │\n",
+       "│ \u001b[32mlora_alpha\u001b[0m              │ 32                   │\n",
+       "│ \u001b[32mlora_dropout\u001b[0m            │ 0.05                 │\n",
+       "│ \u001b[32mmerge_weights\u001b[0m           │ False                │\n",
+       "│ \u001b[32mmodules_to_save\u001b[0m         │ None                 │\n",
+       "│ \u001b[32mpeft_type\u001b[0m               │ LORA                 │\n",
+       "│ \u001b[32mr\u001b[0m                       │ 16                   │\n",
+       "│ \u001b[32mtarget_modules\u001b[0m          │ ['q', 'v']           │\n",
+       "│ \u001b[32mtask_type\u001b[0m               │ SEQ_2_SEQ_LM         │\n",
+       "└─────────────────────────┴──────────────────────┘\n"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">init_trl: self._peft_initialized = </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">True</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> self._trl_initialized = </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">False</span>\n",
+       "</pre>\n"
+      ],
+      "text/plain": [
+       "\u001b[1;31minit_trl: self._peft_initialized = \u001b[0m\u001b[1;3;31mTrue\u001b[0m\u001b[1;31m self._trl_initialized = \u001b[0m\u001b[1;3;31mFalse\u001b[0m\n"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                            </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold; font-style: italic\">INNERMOST PEFT-CONFIG:</span><span style=\"font-style: italic\">                            </span>\n",
+       "┏━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
+       "┃<span style=\"font-weight: bold\"> Key                     </span>┃<span style=\"font-weight: bold\"> Value                                            </span>┃\n",
+       "┡━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
+       "│ <span style=\"color: #008000; text-decoration-color: #008000\">base_model_name_or_path</span> │ edbeeching/gpt-neo-125M-imdb-lora-adapter-merged │\n",
+       "│ <span style=\"color: #008000; text-decoration-color: #008000\">bias</span>                    │ none                                             │\n",
+       "│ <span style=\"color: #008000; text-decoration-color: #008000\">enable_lora</span>             │ None                                             │\n",
+       "│ <span style=\"color: #008000; text-decoration-color: #008000\">fan_in_fan_out</span>          │ False                                            │\n",
+       "│ <span style=\"color: #008000; text-decoration-color: #008000\">inference_mode</span>          │ False                                            │\n",
+       "│ <span style=\"color: #008000; text-decoration-color: #008000\">lora_alpha</span>              │ 32                                               │\n",
+       "│ <span style=\"color: #008000; text-decoration-color: #008000\">lora_dropout</span>            │ 0.05                                             │\n",
+       "│ <span style=\"color: #008000; text-decoration-color: #008000\">merge_weights</span>           │ False                                            │\n",
+       "│ <span style=\"color: #008000; text-decoration-color: #008000\">modules_to_save</span>         │ None                                             │\n",
+       "│ <span style=\"color: #008000; text-decoration-color: #008000\">peft_type</span>               │ LORA                                             │\n",
+       "│ <span style=\"color: #008000; text-decoration-color: #008000\">r</span>                       │ 16                                               │\n",
+       "│ <span style=\"color: #008000; text-decoration-color: #008000\">target_modules</span>          │ ['q_proj', 'v_proj']                             │\n",
+       "│ <span style=\"color: #008000; text-decoration-color: #008000\">task_type</span>               │ CAUSAL_LM                                        │\n",
+       "└─────────────────────────┴──────────────────────────────────────────────────┘\n",
+       "</pre>\n"
+      ],
+      "text/plain": [
+       "\u001b[3m                            \u001b[0m\u001b[1;3;34mINNERMOST PEFT-CONFIG:\u001b[0m\u001b[3m                            \u001b[0m\n",
+       "┏━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
+       "┃\u001b[1m \u001b[0m\u001b[1mKey                    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mValue                                           \u001b[0m\u001b[1m \u001b[0m┃\n",
+       "┡━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
+       "│ \u001b[32mbase_model_name_or_path\u001b[0m │ edbeeching/gpt-neo-125M-imdb-lora-adapter-merged │\n",
+       "│ \u001b[32mbias\u001b[0m                    │ none                                             │\n",
+       "│ \u001b[32menable_lora\u001b[0m             │ None                                             │\n",
+       "│ \u001b[32mfan_in_fan_out\u001b[0m          │ False                                            │\n",
+       "│ \u001b[32minference_mode\u001b[0m          │ False                                            │\n",
+       "│ \u001b[32mlora_alpha\u001b[0m              │ 32                                               │\n",
+       "│ \u001b[32mlora_dropout\u001b[0m            │ 0.05                                             │\n",
+       "│ \u001b[32mmerge_weights\u001b[0m           │ False                                            │\n",
+       "│ \u001b[32mmodules_to_save\u001b[0m         │ None                                             │\n",
+       "│ \u001b[32mpeft_type\u001b[0m               │ LORA                                             │\n",
+       "│ \u001b[32mr\u001b[0m                       │ 16                                               │\n",
+       "│ \u001b[32mtarget_modules\u001b[0m          │ ['q_proj', 'v_proj']                             │\n",
+       "│ \u001b[32mtask_type\u001b[0m               │ CAUSAL_LM                                        │\n",
+       "└─────────────────────────┴──────────────────────────────────────────────────┘\n"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Peft Applied</span>\n",
+       "</pre>\n"
+      ],
+      "text/plain": [
+       "\u001b[1;32mPeft Applied\u001b[0m\n"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "[\" I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know.\"]\n",
+      "['<pad> blue</s>']\n"
+     ]
+    },
+    {
+     "data": {
+      "text/html": [
+       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">init_trl: self._peft_initialized = </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">True</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> self._trl_initialized = </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">True</span>\n",
+       "</pre>\n"
+      ],
+      "text/plain": [
+       "\u001b[1;31minit_trl: self._peft_initialized = \u001b[0m\u001b[1;3;31mTrue\u001b[0m\u001b[1;31m self._trl_initialized = \u001b[0m\u001b[1;3;31mTrue\u001b[0m\n"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">init_trl: self._peft_initialized = </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">True</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> self._trl_initialized = </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">True</span>\n",
+       "</pre>\n"
+      ],
+      "text/plain": [
+       "\u001b[1;31minit_trl: self._peft_initialized = \u001b[0m\u001b[1;3;31mTrue\u001b[0m\u001b[1;31m self._trl_initialized = \u001b[0m\u001b[1;3;31mTrue\u001b[0m\n"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Peft &amp; Trl Applied</span>\n",
+       "</pre>\n"
+      ],
+      "text/plain": [
+       "\u001b[1;32mPeft & Trl Applied\u001b[0m\n"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "gpt_generated = [\" I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know.\"]\n",
+      "t5_generated  = ['<pad> blue</s>']\n"
+     ]
+    },
+    {
+     "data": {
+      "text/html": [
+       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Forward Pass</span>\n",
+       "</pre>\n"
+      ],
+      "text/plain": [
+       "\u001b[1;32mForward Pass\u001b[0m\n"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">hidden_states      .device:</span>  cu<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">da:0</span>\n",
+       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">hidden_states      .dtype :</span>  torch.bfloat16\n",
+       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">self.summary.weight.device:</span>  cu<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">da:0</span>\n",
+       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">self.summary.weight.dtype :</span>  torch.bfloat16\n",
+       "\n",
+       "</pre>\n"
+      ],
+      "text/plain": [
+       "\u001b[1;34mhidden_states      .device:\u001b[0m  cu\u001b[1;92mda:0\u001b[0m\n",
+       "\u001b[1;34mhidden_states      .dtype :\u001b[0m  torch.bfloat16\n",
+       "\u001b[1;34mself.summary.weight.device:\u001b[0m  cu\u001b[1;92mda:0\u001b[0m\n",
+       "\u001b[1;34mself.summary.weight.dtype :\u001b[0m  torch.bfloat16\n",
+       "\n"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">hidden_states      .device:</span>  cu<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">da:0</span>\n",
+       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">hidden_states      .dtype :</span>  torch.bfloat16\n",
+       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">self.summary.weight.device:</span>  cu<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">da:0</span>\n",
+       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">self.summary.weight.dtype :</span>  torch.bfloat16\n",
+       "\n",
+       "</pre>\n"
+      ],
+      "text/plain": [
+       "\u001b[1;34mhidden_states      .device:\u001b[0m  cu\u001b[1;92mda:0\u001b[0m\n",
+       "\u001b[1;34mhidden_states      .dtype :\u001b[0m  torch.bfloat16\n",
+       "\u001b[1;34mself.summary.weight.device:\u001b[0m  cu\u001b[1;92mda:0\u001b[0m\n",
+       "\u001b[1;34mself.summary.weight.dtype :\u001b[0m  torch.bfloat16\n",
+       "\n"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/plain": [
+       "(tensor([[[-43.0000,  -3.6406,  -9.0625,  ..., -43.0000, -43.0000, -43.0000],\n",
+       "          [-32.0000,   5.6562,  -0.4766,  ..., -31.7500, -32.0000, -31.5000],\n",
+       "          [-60.2500,  -6.2188,  -9.5625,  ..., -60.2500, -60.2500, -60.2500],\n",
+       "          ...,\n",
+       "          [-52.0000,   0.8984,  -4.6875,  ..., -52.0000, -52.2500, -51.7500],\n",
+       "          [-56.5000,  -0.6602, -10.0000,  ..., -56.5000, -56.2500, -56.2500],\n",
+       "          [-50.2500,   1.5234,  -8.8750,  ..., -50.0000, -50.2500, -49.7500]]],\n",
+       "        device='cuda:0', grad_fn=<ToCopyBackward0>),\n",
+       " None,\n",
+       " tensor([[ 0.3145,  0.2656, -0.4961,  0.0092,  0.0928,  0.1562, -0.0118]],\n",
+       "        device='cuda:0', dtype=torch.bfloat16, grad_fn=<SqueezeBackward1>))"
+      ]
+     },
+     "execution_count": 4,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "shared_message = \"What is the color of the sky?\"\n",
+    "\n",
+    "shared_peft_config = dict(\n",
+    "    lora_dropout=0.05,\n",
+    "    lora_alpha=32,\n",
+    "    r=16,\n",
+    "    bias=\"none\",\n",
+    ")\n",
+    "\n",
+    "############################################################\n",
+    "############################################################\n",
+    "\n",
+    "causal_lm = peft.LoraConfig(\n",
+    "    task_type=peft.TaskType.CAUSAL_LM,\n",
+    "    **shared_peft_config,\n",
+    ")\n",
+    "\n",
+    "seq2seq = peft.LoraConfig(\n",
+    "    task_type=peft.TaskType.SEQ_2_SEQ_LM,\n",
+    "    **shared_peft_config,\n",
+    ")\n",
+    "\n",
+    "\n",
+    "rich.print(\"[bold green]Nothing Applied\")\n",
+    "print(gpt.text_to_text(shared_message))\n",
+    "print(t5 .text_to_text(shared_message))\n",
+    "\n",
+    "t5 .init_peft(seq2seq)\n",
+    "gpt.init_peft(causal_lm)\n",
+    "\n",
+    "\n",
+    "t5 .to(t5.device)\n",
+    "gpt.to(t5.device)\n",
+    "\n",
+    "if DTYPE in (torch.float16, torch.bfloat16):\n",
+    "    t5 .to(DTYPE)\n",
+    "    gpt.to(DTYPE)\n",
+    "\n",
+    "rich.print(\"[bold green]Peft Applied\")\n",
+    "print(gpt.text_to_text(shared_message))\n",
+    "print(t5 .text_to_text(shared_message))\n",
+    "\n",
+    "t5 .init_trl()\n",
+    "t5.model.v_head.to(t5.device)\n",
+    "gpt.init_trl()\n",
+    "gpt.model.v_head.to(gpt.device)\n",
+    "\n",
+    "if DTYPE in (torch.float16, torch.bfloat16):\n",
+    "    t5 .to(DTYPE)\n",
+    "    gpt.to(DTYPE)\n",
+    "\n",
+    "rich.print(\"[bold green]Peft & Trl Applied\")\n",
+    "gpt_generated = gpt.text_to_text(shared_message)\n",
+    "print(f\"{gpt_generated = }\")\n",
+    "t5_generated = t5.text_to_text(shared_message)\n",
+    "print(f\"{t5_generated  = }\")\n",
+    "\n",
+    "rich.print(\"[bold green]Forward Pass\")\n",
+    "gpt.train()\n",
+    "gpt.forward_from_text(shared_message)\n",
+    "t5 .train()\n",
+    "t5 .forward_from_text(shared_message, t5.text_to_text(shared_message))"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": ".main",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.9.15"
+  },
+  "orig_nbformat": 4
+ },
+ "nbformat": 4,
+ "nbformat_minor": 2
+}
diff --git a/trl/scratchpad/gpt2-sentiment-control.ipynb b/trl/scratchpad/gpt2-sentiment-control.ipynb
new file mode 100644
index 0000000..745fb81
--- /dev/null
+++ b/trl/scratchpad/gpt2-sentiment-control.ipynb
@@ -0,0 +1,863 @@
+{
+ "cells": [
+  {
+   "attachments": {},
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# Tune GPT2 to generate controlled sentiment reviews\n",
+    "> Optimise GPT2 to produce IMDB movie reviews with controlled sentiment using a BERT sentiment classifier for rewards.\n",
+    "\n",
+    "**WARNING:** We often experienced loss spikes in this examples which caused model training to fail or slow down. There is a [GitHub issue](https://github.com/lvwerra/trl/issues/101) to track the issue."
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "<div style=\"text-align: center\">\n",
+    "<img src='https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/gpt2-ctrl-training-setup.png' width='600'>\n",
+    "<p style=\"text-align: center;\"> <b>Figure:</b> Experiment setup to tune GPT2. The yellow arrows are outside the scope of this notebook, but the trained models are available through Hugging Face. </p>\n",
+    "</div>\n",
+    "\n",
+    "\n",
+    "The experiment setup is very similar to the positive sentiment notebook. However, in this notebook we fine-tune GPT2 (small) to generate **controlled** movie reviews based on the IMDB dataset. The model gets the target sentiment and 5 tokens from a real review and is tasked to produce continuations with the targeted sentiment. The reward for the continuations is calculated with the logits of a BERT sentiment classifier. That reward is then used for PPO training."
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Setup experiment"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Import dependencies"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 1,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "%load_ext autoreload\n",
+    "%autoreload 2"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 2,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "/home/leandro_huggingface_co/miniconda3/envs/trl/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
+      "  from .autonotebook import tqdm as notebook_tqdm\n"
+     ]
+    }
+   ],
+   "source": [
+    "import random\n",
+    "import torch\n",
+    "import wandb\n",
+    "import time\n",
+    "import os\n",
+    "from tqdm import tqdm\n",
+    "import numpy as np\n",
+    "import pandas as pd\n",
+    "from random import choices\n",
+    "import matplotlib.pyplot as plt\n",
+    "\n",
+    "tqdm.pandas()\n",
+    "\n",
+    "from datasets import load_dataset\n",
+    "\n",
+    "from transformers import AutoTokenizer, pipeline\n",
+    "\n",
+    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Configuration"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 3,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "sentiment_pipe_kwargs = {\"top_k\": None, \"function_to_apply\": \"none\"}\n",
+    "\n",
+    "config = PPOConfig(\n",
+    "    model_name=\"lvwerra/gpt2-imdb\", steps=51200, learning_rate=1.41e-5, remove_unused_columns=False, log_with=\"wandb\"\n",
+    ")\n",
+    "\n",
+    "txt_in_len = 5\n",
+    "txt_out_len = 20\n",
+    "seed = 1"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 4,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "np.random.seed(seed)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "You can see that we load a GPT2 model called `gpt2_imdb`. This model was additionally fine-tuned on the IMDB dataset for 1 epoch with the huggingface [script](https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py) (no special settings). The other parameters are mostly taken from the original paper [\"Fine-Tuning Language Models from Human Preferences\"](\n",
+    "https://arxiv.org/pdf/1909.08593.pdf). This model as well as the BERT model is available in the Huggingface model zoo [here](https://huggingface.co/models). The following code should automatically download the models."
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Load data and models"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Load pre-trained GPT2 language models"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "We load the GPT2 model with a value head and the tokenizer. We load the model twice; the first model is optimized while the second model serves as a reference to calculate the KL-divergence from the starting point. This serves as an additional reward signal in the PPO training to make sure the optimized model does not deviate too much from the original language model."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 5,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "gpt2_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
+    "gpt2_model_ref = create_reference_model(gpt2_model)\n",
+    "gpt2_tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
+    "\n",
+    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Load IMDB dataset\n",
+    "The IMDB dataset contains 50k movie review annotated with \"positive\"/\"negative\" feedback indicating the sentiment.  We load the IMDB dataset into a DataFrame and filter for comments that are at least 500 characters long and take the first 1000 characters of each comment. The first filter we apply to avoid comments that are less than `txt_in_len` token long and the second to avoid tokenizing way more text than we actually need."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 6,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Found cached dataset imdb (/home/leandro_huggingface_co/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n",
+      "Loading cached processed dataset at /home/leandro_huggingface_co/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-d314b4c14499bf03.arrow\n",
+      "Loading cached processed dataset at /home/leandro_huggingface_co/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-0d5fcb05c95b1186.arrow\n"
+     ]
+    },
+    {
+     "data": {
+      "text/plain": [
+       "Dataset({\n",
+       "    features: ['review', 'sentiment'],\n",
+       "    num_rows: 22578\n",
+       "})"
+      ]
+     },
+     "execution_count": 6,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "# create the dataset\n",
+    "#\n",
+    "dataset = load_dataset(\"imdb\", split=\"train\")\n",
+    "dataset = dataset.rename_columns({\"text\": \"review\", \"label\": \"sentiment\"})\n",
+    "# make sure the comments are are at least 500 and trim to 1000\n",
+    "dataset = dataset.filter(lambda x: len(x[\"review\"]) > 500, batched=False)\n",
+    "dataset = dataset.map(lambda x: {\"review\": x[\"review\"][:1000]}, batched=False)\n",
+    "\n",
+    "dataset"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Tokenize IMDB reviews"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "We tokenize all IMDB in advance to avoid tokenizing twice. In the first step we encode the queries and slice the first `txt_in_len` tokens. In a second step we decode these tokens back to text for later display."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 7,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Loading cached processed dataset at /home/leandro_huggingface_co/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-383f6ebf0ae41ee4.arrow\n",
+      "Loading cached processed dataset at /home/leandro_huggingface_co/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-f4875ad4fccbbc1f.arrow\n"
+     ]
+    }
+   ],
+   "source": [
+    "dataset = dataset.map(\n",
+    "    lambda x: {\"input_ids\": gpt2_tokenizer.encode(\" \" + x[\"review\"], return_tensors=\"pt\")[0, :txt_in_len]},\n",
+    "    batched=False,\n",
+    ")\n",
+    "dataset = dataset.map(lambda x: {\"query\": gpt2_tokenizer.decode(x[\"input_ids\"])}, batched=False)\n",
+    "dataset = dataset[:20480]\n",
+    "\n",
+    "from datasets import Dataset\n",
+    "\n",
+    "dataset = Dataset.from_dict(dataset)\n",
+    "dataset.set_format(\"pytorch\")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 8,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "tensor([ 770, 2646,  373, 2192, 7867])"
+      ]
+     },
+     "execution_count": 8,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "dataset[3][\"input_ids\"]"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 9,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "def collator(data):\n",
+    "    return dict((key, [d[key] for d in data]) for key in data[0])"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 10,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
+      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlvwerra\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
+     ]
+    },
+    {
+     "data": {
+      "text/html": [
+       "Tracking run with wandb version 0.13.9"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "Run data is saved locally in <code>/home/leandro_huggingface_co/trl/examples/sentiment/notebooks/wandb/run-20230206_125743-jpcnr7jx</code>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "Syncing run <strong><a href=\"https://wandb.ai/lvwerra/trl/runs/jpcnr7jx\" target=\"_blank\">comic-music-184</a></strong> to <a href=\"https://wandb.ai/lvwerra/trl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       " View project at <a href=\"https://wandb.ai/lvwerra/trl\" target=\"_blank\">https://wandb.ai/lvwerra/trl</a>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       " View run at <a href=\"https://wandb.ai/lvwerra/trl/runs/jpcnr7jx\" target=\"_blank\">https://wandb.ai/lvwerra/trl/runs/jpcnr7jx</a>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    }
+   ],
+   "source": [
+    "ppo_trainer = PPOTrainer(config, gpt2_model, gpt2_model_ref, gpt2_tokenizer, dataset, data_collator=collator)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Load BERT classifier\n",
+    "We load a BERT classifier fine-tuned on the IMDB dataset."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 11,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "if ppo_trainer.accelerator.num_processes == 1:\n",
+    "    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\n",
+    "else:\n",
+    "    device = ppo_trainer.accelerator.device\n",
+    "sentiment_pipe = pipeline(\"sentiment-analysis\", \"lvwerra/distilbert-imdb\", device=device)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "The model outputs are the logits for the negative and positive class. We will use the logits for positive class as a reward signal for the language model."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 12,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "[{'label': 'NEGATIVE', 'score': 2.3350484371185303},\n",
+       " {'label': 'POSITIVE', 'score': -2.726576328277588}]"
+      ]
+     },
+     "execution_count": 12,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "text = \"this movie was really bad!!\"\n",
+    "output = sentiment_pipe(text, **sentiment_pipe_kwargs)\n",
+    "output"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 13,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "[{'label': 'POSITIVE', 'score': 2.557040214538574},\n",
+       " {'label': 'NEGATIVE', 'score': -2.294790267944336}]"
+      ]
+     },
+     "execution_count": 13,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "text = \"this movie was really good!!\"\n",
+    "output = sentiment_pipe(text, **sentiment_pipe_kwargs)\n",
+    "output"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 14,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "[{'label': 'POSITIVE', 'score': 0.8562759160995483},\n",
+       " {'label': 'NEGATIVE', 'score': -0.7086048126220703}]"
+      ]
+     },
+     "execution_count": 14,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "text = \"this movie was a documentary\"\n",
+    "output = sentiment_pipe(text, **sentiment_pipe_kwargs)\n",
+    "output"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "The resulting reward signal:"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 15,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "def extract_pipe_output(outputs):\n",
+    "    positive_logits = []\n",
+    "    for out in outputs:\n",
+    "        for element in out:\n",
+    "            if element[\"label\"] == \"POSITIVE\":\n",
+    "                positive_logits.append(torch.tensor(element[\"score\"]))\n",
+    "    return positive_logits"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 16,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "-0.7086048126220703"
+      ]
+     },
+     "execution_count": 16,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "output[1][\"score\"]"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Control token dict\n",
+    "We will append the control token at the beginning of each query to signal the model what the target sentiment is. Each control sequence consists of three tokens:"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 17,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "ctrl_str = [\"[negative]\", \"[neutral]\", \"[positive]\"]\n",
+    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # this should be handled by accelerate\n",
+    "ctrl_tokens = dict((s, gpt2_tokenizer.encode(s, return_tensors=\"pt\").squeeze().to(device)) for s in ctrl_str)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 18,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "{'[negative]': tensor([   58, 31591,    60], device='cuda:0'),\n",
+       " '[neutral]': tensor([   58, 29797,    60], device='cuda:0'),\n",
+       " '[positive]': tensor([   58, 24561,    60], device='cuda:0')}"
+      ]
+     },
+     "execution_count": 18,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "ctrl_tokens"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Reward function"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 19,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "def pos_logit_to_reward(logit, task):\n",
+    "    \"\"\"\n",
+    "    Take the positive sentiment logit and scale it for the task.\n",
+    "        task [negative]: reward = -logit\n",
+    "        task [neutral]: reward = -2*abs(logit)+4\n",
+    "        task [positive]: reward = logit\n",
+    "    \"\"\"\n",
+    "    for i in range(len(logit)):\n",
+    "        if task[i] == \"[negative]\":\n",
+    "            logit[i] = -logit[i]\n",
+    "        elif task[i] == \"[neutral]\":\n",
+    "            logit[i] = -2 * torch.abs(logit[i]) + 4\n",
+    "        elif task[i] == \"[positive]\":\n",
+    "            pass\n",
+    "        else:\n",
+    "            raise ValueError(\"task has to be in [0, 1, 2]!\")\n",
+    "    return logit"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "The following examples show the rewards for the cases where the classifier logit is 4, -4 and 0 for the three targets `['negative]`, `['neutral]` and `['positive']`. The scaling is not perfect as it differs between neutral and the other two classes. This is something to further investigate in the future. Ideally, one would use the logit output for each class individually, but since there is no dedicated class for neutral this is a workaround."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 20,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "['[negative]', '[neutral]', '[positive]']\n"
+     ]
+    }
+   ],
+   "source": [
+    "print(ctrl_str)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 21,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "tensor([-4., -4.,  4.])"
+      ]
+     },
+     "execution_count": 21,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "pos_logit_to_reward(torch.Tensor([4, 4, 4]), ctrl_str)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 22,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "tensor([ 4., -4., -4.])"
+      ]
+     },
+     "execution_count": 22,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "pos_logit_to_reward(torch.Tensor([-4, -4, -4]), ctrl_str)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 23,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "tensor([-0., 4., 0.])"
+      ]
+     },
+     "execution_count": 23,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "pos_logit_to_reward(torch.Tensor([0, 0, 0]), ctrl_str)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Generation settings"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 24,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "generation_kwargs = {\n",
+    "    \"min_length\": -1,\n",
+    "    \"top_k\": 0.0,\n",
+    "    \"top_p\": 1.0,\n",
+    "    \"do_sample\": True,\n",
+    "    \"pad_token_id\": gpt2_tokenizer.eos_token_id,\n",
+    "    \"max_new_tokens\": txt_out_len,\n",
+    "    \"eos_token_id\": -1,\n",
+    "}"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Optimize model"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "**Steps**\n",
+    "\n",
+    "The training loop consists of the following steps:\n",
+    "1. Get a batch of queries and create random controls\n",
+    "2. Get the query responses from the policy\n",
+    "3. Join query and responses and tokenize for BERT analysis\n",
+    "4. Get sentiments for query/responses from BERT\n",
+    "5. Optimize policy with PPO using the (query, response, reward) triplet\n",
+    "6. Log all the training statistics\n",
+    "\n",
+    "**Training time**\n",
+    "\n",
+    "This step takes **~2h** on a P6000 GPU with the above specified settings."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 26,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "  8%|▊         | 6/80 [12:44<2:37:54, 128.03s/it]/home/leandro_huggingface_co/miniconda3/envs/trl/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
+      "  warnings.warn(\n",
+      "100%|██████████| 80/80 [2:46:39<00:00, 124.99s/it]  \n",
+      " 91%|█████████▏| 73/80 [2:30:39<14:35, 125.03s/it]  "
+     ]
+    }
+   ],
+   "source": [
+    "for epoch in range(2):\n",
+    "    for batch in tqdm(ppo_trainer.dataloader):\n",
+    "        (\n",
+    "            logs,\n",
+    "            game_data,\n",
+    "        ) = (\n",
+    "            dict(),\n",
+    "            dict(),\n",
+    "        )\n",
+    "\n",
+    "        #### prepend a random control token\n",
+    "        task_list = choices(ctrl_str, k=config.batch_size)\n",
+    "        game_data[\"query\"] = [t + q for t, q in zip(task_list, batch[\"query\"])]\n",
+    "        query_tensors = [torch.cat((ctrl_tokens[t], input_ids)) for t, input_ids in zip(task_list, batch[\"input_ids\"])]\n",
+    "\n",
+    "        #### get response from gpt2\n",
+    "        response_tensors = []\n",
+    "        for query in query_tensors:\n",
+    "            response = ppo_trainer.generate(query, **generation_kwargs)\n",
+    "            response_tensors.append(response.squeeze()[-txt_out_len:])\n",
+    "        game_data[\"response\"] = [gpt2_tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
+    "\n",
+    "        #### sentiment analysis\n",
+    "        texts = [q + r for q, r in zip(batch[\"query\"], game_data[\"response\"])]\n",
+    "        logits = extract_pipe_output(sentiment_pipe(texts, **sentiment_pipe_kwargs))\n",
+    "        rewards = pos_logit_to_reward(logits, task_list)\n",
+    "\n",
+    "        #### Run PPO training\n",
+    "        t = time.time()\n",
+    "        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
+    "\n",
+    "        for cs in ctrl_str:\n",
+    "            key = \"env/reward_\" + cs.strip(\"[]\")\n",
+    "            stats[key] = np.mean([r.cpu().numpy() for r, t in zip(rewards, task_list) if t == cs])\n",
+    "        ppo_trainer.log_stats(stats, game_data, rewards)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Training progress\n",
+    "If you are tracking the training progress with Weights&Biases you should see a plot similar to the following:\n",
+    "\n",
+    "<div style=\"text-align: center\">\n",
+    "<img src='https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/gpt2-ctrl-training-stats.png' width='800'>\n",
+    "<p style=\"text-align: center;\"> <b>Figure:</b> Reward mean and distribution evolution during training. </p>\n",
+    "</div>\n",
+    "\n",
+    "One can observe how the model starts to generate more positive outputs after a few optimisation steps.\n",
+    "\n",
+    "> Note: Investigating the KL-divergence will probably show that at this point the model has not converged to the target KL-divergence, yet. To get there would require longer training or starting with a higher inital coefficient."
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Model inspection"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Reward distribution\n",
+    "First, we can have a look at the reward distribution. Both the negative and positive rewards are clearly shifted to high rewards. The neutral rewards, however, are still centered around zero. There are a few possible explanations for this. There could be a bug in the code and the way the neutral rewards are calculated. Another problem could be that sentence sometimes start with a strong sentiment and it is hard for the model shift the sentiment towards neutral."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGzCAYAAAAMr0ziAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAABPCUlEQVR4nO3deVwVZf8//tecw4HDroiyibKImSmQkKi5B6K3mt4tLvm4RSq7S7lvjTtNLAVcPqip0aLZnbdL3ZK0qP2+5o0SSVmiFor7lklubGqIgB4OnPn9YWfyyGE5h+UM8Ho+Hjw8c80117znOoPzZuaaGUEURRFEREREMqawdABEREREdWHCQkRERLLHhIWIiIhkjwkLERERyR4TFiIiIpI9JixEREQke0xYiIiISPaYsBAREZHsMWEhIiIi2WPCQkQNtmnTJgiCgNzcXLOWnzZtGnx8fAzKBEFAQkJCg2OrS2ZmJgRBQGZmplQ2dOhQ9OrVq8nXDQC5ubkQBAGbNm1qlvURtVRMWIio1UhJSUFycrKlwzBKzrERtQRWlg6AiMiYO3fuwMrKtP+iUlJScOLECcyePbveywwePBh37tyBtbW1iRGapqbYunbtijt37kClUjXp+olaOp5hIZKBsrIyS4dQK51Oh7t37zbrOtVqtckJiynu3r0LnU4HhUIBtVoNhcIy/x0KggC1Wg2lUmmR9RO1FExYiJpZQkICBEHAqVOn8Nxzz6F9+/YYOHCgNP+///0vQkJCYGtrCxcXF0yaNAmXL1+W5r/77rtQKpUoLi6WylatWgVBEBAbGyuVVVVVwdHREa+//rpUtnLlSgwYMAAdOnSAra0tQkJC8MUXX1SLURAExMTEYMuWLXjkkUdgY2ODtLQ0AMDJkycxfPhw2NraonPnzliyZAl0Ol29t3/Hjh3o1asX1Go1evXqhe3btxut9+AYltu3b2P27Nnw8fGBjY0NOnXqhIiICBw+fBjAvXEnX3/9NX777TcIggBBEKRxMfpxKlu3bsWbb74JLy8v2NnZoaSkxOgYFr3s7GwMGDAAtra28PX1xbp16wzm1zR258E2a4utpjEs3377LQYNGgR7e3u0a9cO48aNw+nTpw3q6PelX375BdOmTUO7du3g7OyM6OholJeX1/wlELVAvCREZCHPPvssAgIC8H//938QRREAsHTpUixYsAATJkzAiy++iKKiIrz33nsYPHgwjhw5gnbt2mHQoEHQ6XT44YcfMGbMGADAvn37oFAosG/fPqn9I0eOoLS0FIMHD5bK3nnnHTz55JOYMmUKKioqsHXrVjz77LPYuXMnRo8ebRDft99+i88++wwxMTFwdXWFj48P8vPzMWzYMFRWVmLevHmwt7fHv//9b9ja2tZrm/fs2YOnn34aPXv2RFJSEm7cuIHo6Gh07ty5zmVffvllfPHFF4iJiUHPnj1x48YN/PDDDzh9+jT69OmDN954A7du3cKVK1fw9ttvAwAcHBwM2li8eDGsra3x2muvQaPR1HoZ6Pfff8df/vIXTJgwAZMnT8Znn32GV155BdbW1nj++efrtb169Yntft988w1GjRoFPz8/JCQk4M6dO3jvvffw+OOP4/Dhw9UGKE+YMAG+vr5ISkrC4cOHsX79enTq1AnLly83KU4iWROJqFnFx8eLAMTJkycblOfm5opKpVJcunSpQfnx48dFKysrqbyqqkp0cnIS586dK4qiKOp0OrFDhw7is88+KyqVSvH27duiKIri6tWrRYVCIf7+++9SW+Xl5QZtV1RUiL169RKHDx9uUA5AVCgU4smTJw3KZ8+eLQIQDx48KJUVFhaKzs7OIgDx4sWLtW57cHCw6OHhIRYXF0tle/bsEQGIXbt2rRZDfHy8NO3s7CzOnDmz1vZHjx5drR1RFMW9e/eKAEQ/P79qfaCft3fvXqlsyJAhIgBx1apVUplGoxGDg4PFTp06iRUVFaIoiuLGjRuNbrexNmuK7eLFiyIAcePGjVKZfj03btyQyo4ePSoqFApx6tSpUpl+X3r++ecN2vzrX/8qdujQodq6iFoyXhIispCXX37ZYHrbtm3Q6XSYMGECrl+/Lv24u7sjICAAe/fuBQAoFAoMGDAA33//PQDg9OnTuHHjBubNmwdRFJGVlQXg3lmXXr16oV27dtI67j8T8vvvv+PWrVsYNGiQdFnlfkOGDEHPnj0Nynbt2oV+/fqhb9++UlnHjh0xZcqUOrc3Ly8POTk5iIqKgrOzs1QeERFRbT3GtGvXDgcPHsS1a9fqrFuTqKioep8NsrKywt///ndp2traGn//+99RWFiI7Oxss2Ooi76fpk2bBhcXF6k8MDAQERER2LVrV7VlHtyXBg0ahBs3bqCkpKTJ4iRqbkxYiCzE19fXYPr8+fMQRREBAQHo2LGjwc/p06dRWFgo1R00aBCys7Nx584d7Nu3Dx4eHujTpw+CgoKky0I//PADBg0aZLCOnTt3ol+/flCr1XBxcUHHjh3xwQcf4NatW3XGBwC//fYbAgICqpU/9NBDdW7vb7/9BgBmL79ixQqcOHEC3t7e6Nu3LxISEvDrr7/Wudz9jG1TTTw9PWFvb29Q1r17dwAw+3kz9aHvJ2N98vDDD+P69evVBml36dLFYLp9+/YA7iWlRK0Fx7AQWciDf+nrdDoIgoD//e9/Ru8YuX/Mw8CBA6HVapGVlYV9+/ZJicmgQYOwb98+nDlzBkVFRQYJy759+/Dkk09i8ODBWLt2LTw8PKBSqbBx40akpKTUGZ+lTZgwAYMGDcL27duxZ88evPXWW1i+fDm2bduGUaNG1auNxt4mQRCMlldVVTXqeupS0x1G4h9jo4haAyYsRDLh7+8PURTh6+sr/SVfk759+8La2hr79u3Dvn37MGfOHAD3niny0UcfISMjQ5rW+/LLL6FWq7F7927Y2NhI5Rs3bqx3jF27dsX58+erlZ89e7ZeywIwe3kA8PDwwIwZMzBjxgwUFhaiT58+WLp0qZSw1JRAmOPatWsoKyszOMty7tw5AJAGverPZNx/xxbw51mS+9U3Nn0/GeuTM2fOwNXVtdqZH6K2gJeEiGTiqaeeglKpRGJiYrW/jEVRxI0bN6RptVqNxx57DJ9++ikuXbpkcIblzp07ePfdd+Hv7w8PDw9pGaVSCUEQDP76z83NxY4dO+od41/+8hccOHAAhw4dksqKioqwZcuWOpf18PBAcHAwNm/ebHAJKj09HadOnap12aqqqmqXrTp16gRPT09oNBqpzN7e3ujlLXNUVlbiww8/lKYrKirw4YcfomPHjggJCQFwL8kEII0n0sf673//u1p79Y3t/n66PxE6ceIE9uzZg7/85S/mbhJRi8YzLEQy4e/vjyVLliAuLg65ubkYP348HB0dcfHiRWzfvh0vvfQSXnvtNan+oEGDsGzZMjg7O6N3794A7h3EH3roIZw9exbTpk0zaH/06NFYvXo1Ro4cieeeew6FhYVYs2YNunXrhmPHjtUrxrlz5+KTTz7ByJEjMWvWLOm25q5du9arjaSkJIwePRoDBw7E888/j5s3b+K9997DI488gtLS0hqXu337Njp37oxnnnkGQUFBcHBwwDfffIOffvoJq1atkuqFhIQgNTUVsbGxeOyxx+Dg4ICxY8fWa9se5OnpieXLlyM3Nxfdu3dHamoqcnJy8O9//1t6Ku0jjzyCfv36IS4uDjdv3oSLiwu2bt2KysrKau2ZEttbb72FUaNGoX///njhhRek25qdnZ2b5f1KRLJkyVuUiNoi/a2oRUVFRud/+eWX4sCBA0V7e3vR3t5e7NGjhzhz5kzx7NmzBvW+/vprEYA4atQog/IXX3xRBCD+5z//qdb2f/7zHzEgIEC0sbERe/ToIW7cuFGK534AaryF+NixY+KQIUNEtVotenl5iYsXLxb/85//1Ou2Zv32Pfzww6KNjY3Ys2dPcdu2bWJUVFSttzVrNBpxzpw5YlBQkOjo6Cja29uLQUFB4tq1aw2WKS0tFZ977jmxXbt2BrdK628z/vzzz6vFU9NtzY888oj4888/i/379xfVarXYtWtX8f3336+2/IULF8Tw8HDRxsZGdHNzE+fPny+mp6dXa7Om2Izd1iyKovjNN9+Ijz/+uGhrays6OTmJY8eOFU+dOmVQp6Z9qabbrYlaMkEUOSqLiIiI5I1jWIiIiEj2mLAQERGR7DFhISIiItljwkJERESyx4SFiIiIZI8JCxEREcleq3hwnE6nw7Vr1+Do6Nioj+YmIiKipiOKIm7fvg1PT08oFLWfQ2kVCcu1a9fg7e1t6TCIiIjIDJcvX0bnzp1rrdMqEhZHR0cA9zbYycnJ7Ha0Wi327NmDESNGSI/ebovYD+wDgH0AsA/02A/sA6Bp+qCkpATe3t7Scbw2rSJh0V8GcnJyanDCYmdnBycnpza7QwLsB4B9ALAPAPaBHvuBfQA0bR/UZzgHB90SERGR7DFhISIiItljwkJERESy1yrGsNSHKIqorKxEVVVVjXW0Wi2srKxw9+7dWuu1di2xH1QqFZRKpaXDICKiJtImEpaKigrk5eWhvLy81nqiKMLd3R2XL19u089zaYn9IAgCOnfuDAcHB0uHQkRETaDVJyw6nQ4XL16EUqmEp6cnrK2tazwI63Q6lJaWwsHBoc4H2LRmLa0fRFFEUVERrly5goCAAJ5pISJqhVp9wlJRUQGdTgdvb2/Y2dnVWlen06GiogJqtbpFHKibSkvsh44dOyI3NxdarZYJCxFRK9QyjkaNoKUceMk8LeXSFRERmYdHcSIiIpI9JixEREQke61+DEtt3k4/ZzAtiiI0Gg1sbGya5BLDqxHdTao/dOhQfPfddwCAI0eOIDg4uNFjamyCIGD79u0YP358o7SXmZmJYcOGAQDGjRuHHTt2NEq7RETUsvAMi8xNnz4deXl56NWrl6VDMZCQkGA0gcrLy8OoUaMabT0DBgxAXl4eJkyY0GhtEhFRy9Omz7C0BHZ2dnB3d7d0GPXW2LFaW1vD3d0dtra20Gg0jdo2ERG1HDzD0oJkZmZCEARkZGQgNDQUdnZ2GDBgAM6ePWtQ76uvvkKfPn2gVqvh5+eHxMREVFZWSvPPnDmDgQMHQq1Wo2fPnvjmm28gCILB5Zb4+Hj06NEDdnZ28PPzw4IFC6DVagEAmzZtQmJiIo4ePQpBECAIAjZt2gQABu0MGDAAr7/+ukFsRUVFUKlU+P777wEAGo0Gr732Gry8vGBvb4+wsDBkZmY2bscREVGLxzMsLdAbb7yBVatWoWPHjnj55Zfx/PPP48cffwQA7Nu3D1OnTsW7776LQYMG4cKFC3jppZcA3EtCqqqqMH78eHTp0gUHDx7E7du38a9//avaOhwdHbFhwwZ07twZx48fx/Tp0+Ho6Ii5c+di4sSJOHHiBNLS0vDNN98AAJydnau1MWXKFKxYsQLLli2TxgSlpqbC09MTgwYNAgDExMTg1KlT2Lp1Kzw9PbF9+3aMHDkSx48fR0BAQJP0H9Vsbc5a6bOgE+AJT6w/vh6iQrRgVMCM4BkWXT8RWR7PsLRAS5cuxZAhQ9CzZ0/MmzcP+/fvx927dwEAiYmJmDdvHqKiouDn54eIiAgsXrwYH374IQAgPT0dFy5cwMcff4ygoCAMHDgQS5curbaO1157DQMGDICPjw/Gjh2L1157DZ999hkAwNbWFg4ODrCysoK7u7t0yeZBEyZMwLVr1/DDDz9IZSkpKZg8eTIEQcClS5ewceNGfP755xg0aBD8/f3x2muvYeDAgdi4cWNTdB0REbVQPMPSAgUGBkqfPTw8AACFhYXo0qULjh49ih9//NEgCamqqsLdu3dRXl6Os2fPwtvb22CsSd++fautY9u2bfjPf/6DCxcuoLS0FJWVlXBycjIpzo4dO2LEiBHYsmULBg0ahIsXLyIrK0tKno4fP46qqip0725495RGo0GHDh1MWhcREbVuTFhaIJVKJX3WX2rR6XQAgNLSUiQmJuKpp56qtpxara5X+1lZWXjppZeQkJCAkSNHwtnZGVu3bsWqVatMjnXKlCn45z//iffeew8pKSno3bs3evfuLcWqVCqRnZ1d7XH6fIkhERHdz6xLQmvWrIGPjw/UajXCwsJw6NChGutu27YNoaGhaNeuHezt7REcHIxPPvnEoM60adOkwZv6n5EjR5oTWpvXp08fnD17Ft26dav2o1Ao8NBDD+Hy5csoKCiQlvnpp58M2sjKyoK3tzfmz5+P0NBQBAQE4LfffjOoY21tjaqqqjrjGTduHO7evYu0tDSkpKRgypQp0rxHH30UVVVVKCwsrBZrS7ozioiImp7JZ1hSU1MRGxuLdevWISwsDMnJyYiMjMTZs2fRqVOnavVdXFzwxhtvoEePHrC2tsbOnTsRHR2NTp06ITIyUqo3cuRIg3ELNjY2Zm5S27Zw4UKMGTMGXbp0wTPPPAOFQoGjR4/ixIkTWLJkCSIiIuDv74+oqCisWLECt2/fxptvvgngz7M13bp1w5UrV7B161aEhYXh66+/xvbt2w3W4+Pjg4sXLyInJwedO3eGo6Oj0e/M3t4e48ePx4IFC3D69GlMnjxZmte9e3dMmTIFU6dOxapVq/Doo4+iqKgIGRkZCAwMxOjRo5uwp4iIqCUxOWFZvXo1pk+fjujoaADAunXr8PXXX2PDhg2YN29etfpDhw41mJ41axY2b96MH374wSBhsbGxafa/qh988qxOp0NJSQmcnJxa7MsSIyMjsXPnTixatAjLly+HSqVCjx498OKLLwIAlEolduzYgRdffBGPPfYY/Pz88NZbb2Hs2LHSJaMnn3wSr7zyCv75z39Co9Fg9OjRWLBgARISEqT1PP3009i2bRuGDRuG4uJibNy4EdOmTTMa05QpU/CXv/wFgwcPRpcuXQzmbdy4EUuWLMG//vUvXL16Fa6urujXrx/GjBnTJP1DREQtk0kJS0VFBbKzsxEXFyeVKRQKhIeHIysrq87lRVHEt99+i7Nnz2L58uUG8zIzM9GpUye0b98ew4cPx5IlS2oceKnRaAweIlZSUgIA0Gq10rNC9LRaLURRhE6nk8Z51Baf/t+66jaX+2MZPHiwdBlGXxYYGFitLCIiAhEREdXa0s/v3r279BwUANIt0X5+ftDpdBBFEYsWLcLbb79t8IqCf/7zn1IbKpVKumvo/vYfjAW4l0QZKwfuJVDx8fGIj4+vMV59P9T2vejj1mq11cbDmEO/Hz24P7V2gk6o9vn+Mkux1PfQVveDB7Ef2AdA0/SBKW0Jov4oXQ/Xrl2Dl5cX9u/fj/79+0vlc+fOxXfffYeDBw8aXe7WrVvw8vKCRqOBUqnE2rVr8fzzz0vzt27dCjs7O/j6+uLChQuYP38+HBwckJWVZfTgk5CQgMTExGrlKSkpsLOzMyjT33rr7e0Na2vr+m6qLIwZMwaHDh2CtbU1du/ejUceeaRR2t25cyfs7e3h7++PX3/9FXFxcXB2dkZaWlqjtN+Y9u/fjwkTJkCj0Uh3HBlTUVGBy5cvIz8/3+AheUREJF/l5eV47rnncOvWrTrvRG2Wu4QcHR2Rk5OD0tJSZGRkIDY2Fn5+ftLlokmTJkl1e/fujcDAQPj7+yMzMxNPPPFEtfbi4uIQGxsrTZeUlMDb2xsjRoyotsF3797F5cuX4eDgUOddMqIo4vbt23B0dGySlx+a6tNPP8WdO3cAAF26dGm0hKuyshKvv/46Ll26BFdXVzzxxBNYuXKl1Hdy6ochQ4bg8OHDAO7dOVTTDn337l3Y2tpi8ODB9b4bqjZarRbp6emIiIgwuCurtVt/fL30WdAJ8LjqgTyvPIs/OO7F3i9aZL1tdT94EPuBfQA0TR/or5DUh0kJi6urK5RKpcEdJgBQUFBQ6/gThUKBbt26AQCCg4Nx+vRpJCUlVRvfoufn5wdXV1f88ssvRhMWGxsbowM8VSpVtU6sqqqCIAhQKBR1jkvRX27Q17c0b2/vJml32rRpNY43AeTVD/b29tWe02KMQqGAIAhG94GGaOz25M5YYiIqRIsnLJb+DtraflAT9gP7AGjcPjClHZOORtbW1ggJCUFGRoZUptPpkJGRYXCJqC46na7WF9lduXIFN27ckB6KRkRERG2byZeEYmNjERUVhdDQUPTt2xfJyckoKyuT7hqaOnUqvLy8kJSUBABISkpCaGgo/P39odFosGvXLnzyySf44IMPAPz5oLOnn34a7u7uuHDhAubOnYtu3boZ3EVEREREbZfJCcvEiRNRVFSEhQsXIj8/H8HBwUhLS4ObmxsA4NKlSwaXEcrKyjBjxgxcuXIFtra26NGjB/773/9i4sSJAO7dJXLs2DFs3rwZxcXF8PT0xIgRI7B48WI+i4WIiIgAmDnoNiYmBjExMUbnZWZmGkwvWbIES5YsqbEtW1tb7N6925wwiIiIqI2w/MhSIiIiojowYSEiIiLZa9tva96bZDApiCLUGg0EGxugKZ4/Miyu7jr3GTp0KL777jsAwJEjRxAcHNz4MTWDTZs2Yfbs2SguLpam9YO0Z82aheTkZMsFR0RELQLPsMjc9OnTkZeXh169ejXbOjMzM9G+fXspwWhsEydORF5enkm3whMRUdvWts+wtAB2dnbN/lLI+qqoqDDr6bu2trawtbVtca9KICIiy+EZlhYkMzMTgiAgIyMDoaGhsLOzw4ABA3D27FmDel999RX69OkDtVoNPz8/JCYmSu/Xyc3NhSAIyMnJkeoXFxdDEARkZmYiNzdXerpwhw4dIAiC9FTcoUOHIiYmBrNnz4arq6v0nJzVq1ejd+/esLe3h7e3N2bMmIHS0tKm7xAiImozmLC0QG+88QZWrVqFn3/+GVZWVgYvkty3bx+mTp2KWbNm4dSpU/jwww+xadMmLF26tF5te3t74/PPPwcAnD59Gnl5eXjnnXek+Zs3b4a1tTV+/PFHrFu3DsC9x+K/++67OHnyJDZv3oxvv/0Wc+fObcQtJiKito6XhFqgpUuXYsiQIQCAefPmYfTo0bh79y7UajUSExMxb948REVFAbj3XqbFixdj7ty5iI+Pr7NtpVIJFxcXAECnTp2kz3oBAQFYsWKFQdns2bOlzz4+PliyZAlefvllrF27tiGbSUREJGHC0gIFBgZKn/XvWyosLESXLl1w9OhR/PjjjwZnVKqqqnD37l2Ul5c3eN0hISHVyr755hskJSXhzJkzKCkpQWVlpbQ+Ozu7Bq+TiIiICUsLdP/bLYU/br/Wv2FZ/26mp556qtpyarVaem2CKP759l2tVlvvddvb2xtM5+bmYsyYMXjllVewdOlSuLi44IcffsALL7yAiooKJixERNQomLC0Mn369MHZs2fRrVs3o/M7duwIAMjLy8Ojjz4KAAYDcAFId+9UVVXVub7s7GzodDqsWrVKSoY+++wzc8MnIiIyiglLK7Nw4UKMGTMGXbp0wTPPPAOFQoGjR4/ixIkTWLJkCWxtbdGvXz8sW7YMvr6+KCwsxJtvvmnQRteuXSEIAnbu3IkxY8bA1tYWDg4ORtfXrVs3aLVavPfeexg7dqzBYFwiIqLG0rYTlgeePCvqdLhbUgJrJycIipZ5A1VkZCR27tyJRYsWYfny5VCpVOjRowdefPFFqc6GDRvwwgsvICQkBA899BBWrFiBESNGSPO9vLwQFxeH+fPn44UXXsDUqVOxadMmo+sLCgrC6tWrsXz5csTFxWHw4MFISkrC1KlTm3pTiYioDWnbCUsLM3ToUIOxJwAQHBxcrSwyMlJ6RooxDz/8MPbv329Q9mAbc+bMweLFi6XLPED1N3Hrvfrqq3j11VcNyv72t79Jn6dNmyY9y4WIiMgcLfM0Qhuydu1aODg44Pjx45YOpdFs2bIFDg4O2Ldvn6VDISKiFoJnWGRsy5YtuHPnDgCgS5cuFo6m8Tz55JMICwsDALRr186ywRARUYvAhEXGvLy8LB1Ck3B0dISjo6OlwyAiohaEl4SIiIhI9piwEBERkewxYSEiIiLZY8JCREREsseEhYiIiGSPdwkREdXH3iRLR2DcA0/spnoy9fsUFQB6APtWA4KuSUICwO+zFm06YVmbs9ZgWhRFaDQa2NjYSG9BbkwzgmeYVH/o0KH47rvvAABHjhxBcHBwo8dkbJ1BQUFITEyssc6mTZswe/ZsFBcXN9p6p02bhs2bNwMAtm/fjvHjxzda20RE1PLxkpDMTZ8+HXl5eejVq1ezrG/btm1YtGiRNO3j44Pk5GSDOhMnTsS5c+cadb3vvPMO8vLyGrVNIiJqPdr0GZaWwM7ODu7u7s22PhcXF+h0OpSUlNRYx9bWFra2to26XmdnZzg7Ozdqm0RE1HrwDEsLkpmZCUEQ8PXXXyMwMBBqtRr9+vXDiRMnDOp9+eWXeOSRR2BjYwMfHx+sWrXKYP7atWsREBAAtVoNNzc3PPPMM9K8oUOHSi8yHD58OH777Te8+uqrEARBuky2adMm6ZH6586dgyAIOHPmjME63n77bfj7+0vTJ06cwKhRo+Dg4AA3Nzf87W9/w/Xr1xutb4iIqHVjwtICzZkzB6tWrcJPP/2Ejh07YuzYsdBqtQCA7OxsTJgwAZMmTcLx48eRkJCABQsWYNOmTQCAn3/+Gf/85z+xaNEinD17FmlpaRg8eLDR9XzxxRfo3LkzFi1ahLy8PKOXbLp3747Q0FBs2bLFoHzLli147rnnAADFxcUYPnw4Hn30Ufz8889IS0tDQUEBJkyY0Ii9QkRErRkvCbVA8fHxiIiIAABs3rwZnTt3xvbt2zFhwgSsXr0aTzzxBBYsWADgXkJx6tQpvPXWW5g2bRouXboEe3t7jBkzBo6OjujatSseffRRo+txcXGBUqmEo6NjrZelpkyZgvfffx+LFy8GcO+sS3Z2Nv773/8CAN5//308+uij+L//+z9pmQ0bNsDb2xvnzp1D9+7dG6VfiIio9eIZlhaof//+0mcXFxc89NBDOH36NADg9OnTePzxxw3qP/744zh//jyqqqoQERGBrl27ws/PD3/729+wZcsWlJeXNyieSZMmITc3FwcOHABw7+xKnz590KNHDwDA0aNHsXfvXjg4OEg/+nkXLlxo0LqJiKhtYMLSxjg6OuLw4cP49NNP4eHhgYULFyIoKKhBtyi7u7tj+PDhSElJAQCkpKRgypQp0vzS0lKMHTsWOTk5Bj/nz5+v8XIUERHR/ZiwtED6MxkA8Pvvv+PcuXN4+OGHAQAPP/wwfvzxR4P6P/74I7p37w6lUgkAsLKyQnh4OFasWIFjx44hNzcX3377rdF1WVtbo6qqqs6YpkyZgtTUVGRlZeHXX3/FpEmTpHl9+vTByZMn4ePjg27duhn82Nvbm7z9RETU9jBhaYEWLVqEjIwMnDhxAtOmTYOrq6v0oLV//etfyMjIwOLFi3Hu3Dls3rwZ77//Pl577TUAwM6dO/Huu+8iJycHv/32Gz7++GPodDo89NBDRtfl4+OD77//HlevXq31rp6nnnoKt2/fxiuvvIJhw4bB09NTmjdz5kzcvHkTkydPxk8//YQLFy5g9+7diI6OrlcyRERE1KYH3T745Fn980ecnJygUMg3l1u2bBlmzZqF8+fPIzg4GP/v//0/WFtbA7h3NuOzzz7DwoULsXjxYnh4eGDRokWYNm0aAKBdu3bYtm0bEhIScPfuXQQEBODTTz/FI488YnRdixYtwt///nf4+/tDo9FAFEWj9RwdHTF27Fh89tln2LBhg8E8T09P/Pjjj3j99dcxYsQIaDQadO3aFSNHjpR1PxMRkXy06YSlpRo4cGC1Z6/c7+mnn8bTTz9d47KZmZk1LpuZmWnw4Lh+/frh6NGjBnWmTZsmJUD3S01NRWpqqtF2AwICsG3bthrXS0REVBv+eStza9euhYODA44fP27pUJrUyy+/DAcHB0uHQUREMsUzLDK2ZcsW3LlzBwDQpUsX7N+/38IRNZ1FixZJ42w8PDwsHA0REckNExYZ8/LyMpgeOnRojWNIWrpOnTqhU6dOlg6DiIhkyqxLQmvWrIGPjw/UajXCwsJw6NChGutu27YNoaGhaNeuHezt7REcHIxPPvnEoI4oili4cCE8PDxga2uL8PBwnD9/3pzQiIiIqBUyOWFJTU1FbGws4uPjcfjwYQQFBSEyMhKFhYVG67u4uOCNN95AVlYWjh07hujoaERHR2P37t1SnRUrVuDdd9/FunXrcPDgQdjb2yMyMhJ37941f8se0FrPTNA9/H6JiFo3kxOW1atXY/r06YiOjkbPnj2xbt062NnZVbuVVW/o0KH461//iocffhj+/v6YNWsWAgMD8cMPPwC4d6BJTk7Gm2++iXHjxiEwMBAff/wxrl27hh07djRo4wBApVIBQIMfP0/yVlFRAQDSw/GIiKh1MWkMS0VFBbKzsxEXFyeVKRQKhIeHIysrq87lRVHEt99+i7Nnz2L58uUAgIsXLyI/Px/h4eFSPWdnZ4SFhSErK8vgial6Go0GGo1GmtbfgqvVaqW3Ft/P0dERBQUF0Ol0sLOzgyAINcZXUVGBO3fu1FinLWhp/aDT6VBYWAi1Wg1RFI3uA6bSt9EYbbUkgk6o9vn+Mkux1PdgsB+IMr2pshn6plX+Ppj4fWr/qK9t6v1Axn3cFPuBKW2ZlLBcv34dVVVVcHNzMyh3c3PDmTNnalzu1q1b8PLygkajgVKpxNq1a6W3Defn50ttPNimft6DkpKSkJiYWK18z549sLOzM7qMo6MjysrK+KCyVkqr1aKoqAjHjh1r1HbT09MbtT2584RntTKPq5a/a2vX5V0WXf+9/aCHRWOo0a7m65vW9ftg3veZXtrEb5dvxu/TXI25H5hy9aNZ7hJydHRETk4OSktLkZGRgdjYWPj5+WHo0KFmtRcXF4fY2FhpuqSkBN7e3hgxYgScnJxqXK6qqgqVlZU1jneorKzE/v37MWDAAFhZtd0bqFpaPwiCAJVK1ajJqFarRXp6OiIiIqTLim3B+uPrpc+CToDHVQ/keeVBVFh2jNCLvV+0yHoN9oMD71kkhjoNiq27TgO1yt+HfatNqq4VFUgv7Y4Ih3NQCbomCgrN8n2aqyn2A/0Vkvow6Wjk6uoKpVKJgoICg/KCggK4u7vXuJxCoUC3bt0AAMHBwTh9+jSSkpIwdOhQabmCggKD528UFBQgODjYaHs2NjawsbGpVq5SqWrtxLo6WKvVorKyEg4ODq3nl9IM7Ic/1bVPtTbGEhNRIVo8YbH0d6BSqZr2INUQzdg3rer3wczvUyXomnZfaAH925j7gSntmPQnqbW1NUJCQpCRkSGV6XQ6ZGRkoH///vVuR6fTSWNQfH194e7ubtBmSUkJDh48aFKbRERE1HqZfL4/NjYWUVFRCA0NRd++fZGcnIyysjJER0cDAKZOnQovLy8kJSUBuDfeJDQ0VHp53q5du/DJJ5/ggw8+AHDvdP7s2bOxZMkSBAQEwNfXFwsWLICnp6f0BmIiIiJq20xOWCZOnIiioiIsXLgQ+fn5CA4ORlpamjRo9tKlSwZjCcrKyjBjxgxcuXIFtra26NGjB/773/9i4sSJUp25c+eirKwML730EoqLizFw4ECkpaVBrVY3wiYSERFRS2fWiMqYmBjExMQYnffgm4CXLFmCJUuW1NqeIAhYtGgRFi1aZE44RERE1MrxHl8iIiKSPSYsREREJHtMWIiIiEj2mLAQERGR7DFhISIiItljwkJERESyx4SFiIiIZI8JCxEREckeExYiIiKSPSYsREREJHtMWIiIiEj2zHqXEFFr8Xb6OaPlglgFXwBr9v4CUVA2a0yvRnRv1vUREbUEPMNCREREsseEhYiIiGSPCQsRERHJHhMWIiIikj0mLERERCR7TFiIiIhI9piwEBERkewxYSEiIiLZY8JCREREsseEhYiIiGSPCQsRERHJHhMWIiIikj0mLERERCR7TFiIiIhI9piwEBERkewxYSEiIiLZY8JCREREsseEhYiIiGSPCQsRERHJHhMWIiIikj0rSwdARIbeTj9nsXUfLrkhfbaCAuOtPC0WCxHR/XiGhYiIiGSPCQsRERHJHhMWIiIikj0mLERERCR7TFiIiIhI9sxKWNasWQMfHx+o1WqEhYXh0KFDNdb96KOPMGjQILRv3x7t27dHeHh4tfrTpk2DIAgGPyNHjjQnNCIiImqFTE5YUlNTERsbi/j4eBw+fBhBQUGIjIxEYWGh0fqZmZmYPHky9u7di6ysLHh7e2PEiBG4evWqQb2RI0ciLy9P+vn000/N2yIiIiJqdUxOWFavXo3p06cjOjoaPXv2xLp162BnZ4cNGzYYrb9lyxbMmDEDwcHB6NGjB9avXw+dToeMjAyDejY2NnB3d5d+2rdvb94WERERUatj0oPjKioqkJ2djbi4OKlMoVAgPDwcWVlZ9WqjvLwcWq0WLi4uBuWZmZno1KkT2rdvj+HDh2PJkiXo0KGD0TY0Gg00Go00XVJSAgDQarXQarWmbJIB/bINaaM1aEv9IIhVtZbXNL+1srrvbxj9Z0EnWCociaX2RYPfBVGmQ/6aoW9a5f8JJn6f2j/qa5t6P5BxHzfFfmBKW4IoimJ9K1+7dg1eXl7Yv38/+vfvL5XPnTsX3333HQ4ePFhnGzNmzMDu3btx8uRJqNVqAMDWrVthZ2cHX19fXLhwAfPnz4eDgwOysrKgVCqrtZGQkIDExMRq5SkpKbCzs6vv5hAREZEFlZeX47nnnsOtW7fg5ORUa91mfTT/smXLsHXrVmRmZkrJCgBMmjRJ+ty7d28EBgbC398fmZmZeOKJJ6q1ExcXh9jYWGm6pKREGhtT1wbXRqvVIj09HREREVCpVGa309K1pX5Ys/cXo+WCWAWfuxeQq/aHKFRPmluro7e3SZ+toMAYq57I88qDqKj33zVN4sXeL1pkvQa/Cwfes0gMdRoUW3edBmqV/yfsW21Sda2oQHppd0Q4nINK0DVRUPKm7fePRt8P9FdI6sOkhMXV1RVKpRIFBQUG5QUFBXB3d6912ZUrV2LZsmX45ptvEBgYWGtdPz8/uLq64pdffjGasNjY2MDGxqZauUqlapRObKx2Wrq20A91JSOioGxTCUslqv9HLCpEiycslt4PVSqVfA9Szdg3rer/BDO/T5Wgk+++0NT++O4bcz8wpR2TLsZZW1sjJCTEYMCsfgDt/ZeIHrRixQosXrwYaWlpCA0NrXM9V65cwY0bN+Dh4WFKeERERNRKmTx6KDY2Fh999BE2b96M06dP45VXXkFZWRmio6MBAFOnTjUYlLt8+XIsWLAAGzZsgI+PD/Lz85Gfn4/S0lIAQGlpKebMmYMDBw4gNzcXGRkZGDduHLp164bIyMhG2kwiIiJqyUwewzJx4kQUFRVh4cKFyM/PR3BwMNLS0uDm5gYAuHTpEhSKP/OgDz74ABUVFXjmmWcM2omPj0dCQgKUSiWOHTuGzZs3o7i4GJ6enhgxYgQWL15s9LIPERERtT1mDbqNiYlBTEyM0XmZmZkG07m5ubW2ZWtri927d5sTBhEREbURMn2wABEREdGfmLAQERGR7DFhISIiItljwkJERESyx4SFiIiIZI8JCxEREckeExYiIiKSPSYsREREJHtMWIiIiEj2mLAQERGR7DFhISIiItljwkJERESyx4SFiIiIZI8JCxEREckeExYiIiKSPSYsREREJHtMWIiIiEj2mLAQERGR7DFhISIiItmzsnQA1LqszVlr6RCqmRE8w9IhUCu0tviYpUO4pxl+5wSdAE94Yv3x9RAVosnL83eQGgPPsBAREZHsMWEhIiIi2WPCQkRERLLHhIWIiIhkjwkLERERyR4TFiIiIpI93tZMzSrrwo1mX6em6Fyzr7M1OXTxJiqhs2gM93+Hr0Z0t2AkRGQpPMNCREREsseEhYiIiGSPl4SIiKhpXNx379/fb1k2DmoVeIaFiIiIZI8JCxEREckeExYiIiKSPSYsREREJHtMWIiIiEj2mLAQERGR7DFhISIiItljwkJERESyx4SFiIiIZM+shGXNmjXw8fGBWq1GWFgYDh06VGPdjz76CIMGDUL79u3Rvn17hIeHV6sviiIWLlwIDw8P2NraIjw8HOfPnzcnNCIiImqFTE5YUlNTERsbi/j4eBw+fBhBQUGIjIxEYWGh0fqZmZmYPHky9u7di6ysLHh7e2PEiBG4evWqVGfFihV49913sW7dOhw8eBD29vaIjIzE3bt3zd8yIiIiajVMfpfQ6tWrMX36dERHRwMA1q1bh6+//hobNmzAvHnzqtXfsmWLwfT69evx5ZdfIiMjA1OnToUoikhOTsabb76JcePGAQA+/vhjuLm5YceOHZg0aVK1NjUaDTQajTRdUlICANBqtdBqtaZukkS/bEPaaA0a0g+CTqh1vpUFrkIKYpXZy5izbEt2//ej/2yJ7+xB938Pzfn7afC7IBr2gyCXV7HV8TvXGPS/13X9fld3r4+0ouX3oYbSb0Nr2BZzNcUx0pS2BFEUxfpWrqiogJ2dHb744guMHz9eKo+KikJxcTG++uqrOtu4ffs2OnXqhM8//xxjxozBr7/+Cn9/fxw5cgTBwcFSvSFDhiA4OBjvvPNOtTYSEhKQmJhYrTwlJQV2dnb13RwiIiKyoPLycjz33HO4desWnJycaq1r0p8I169fR1VVFdzc3AzK3dzccObMmXq18frrr8PT0xPh4eEAgPz8fKmNB9vUz3tQXFwcYmNjpemSkhLpUlNdG1wbrVaL9PR0REREQKVSmd1OS9eQflh/fH2t8w9dvNmQ0MwS5PiUycsIYhV87l5ArtofoqBsgqjk6ejtbdJnKygwxqondlaeQiV0FozK8DucOaxbs63X4HfhwHsG89bfOtFscdSq64AmX4WgE+Bx1QN5XnkQFfX+Gxf4bT8A4EXnXk0UWfPRigqkl3ZHhMM5qATL/j5YirbfPxr9GKm/QlIfzXpOc9myZdi6dSsyMzOhVqvNbsfGxgY2NjbVylUqVaN0YmO109KZ0w91/WdmiQNfQxIOUVC2qYTF2PdTCZ3FE5b7vwNL/G6qVKpqBykRlc0eh1GmJBANJCpE0xKWP/qoNR3gVYKuVW2PSf743WvMY6Qp7Zh0Mc7V1RVKpRIFBQUG5QUFBXB3d6912ZUrV2LZsmXYs2cPAgMDpXL9cua0SURERG2DSQmLtbU1QkJCkJGRIZXpdDpkZGSgf//+NS63YsUKLF68GGlpaQgNDTWY5+vrC3d3d4M2S0pKcPDgwVrbJCIiorbD5EtCsbGxiIqKQmhoKPr27Yvk5GSUlZVJdw1NnToVXl5eSEpKAgAsX74cCxcuREpKCnx8fKRxKQ4ODnBwcIAgCJg9ezaWLFmCgIAA+Pr6YsGCBfD09DQY2EtERERtl8kJy8SJE1FUVISFCxciPz8fwcHBSEtLkwbNXrp0CQrFnyduPvjgA1RUVOCZZ54xaCc+Ph4JCQkAgLlz56KsrAwvvfQSiouLMXDgQKSlpTVonAsRERG1HmYNuo2JiUFMTIzReZmZmQbTubm5dbYnCAIWLVqERYsWmRMOERERtXJt9wk4RERE1GIwYSEiIiLZY8JCREREsseEhYiIiGSPCQsRERHJHhMWIiIikj0mLERERCR7TFiIiIhI9piwEBERkewxYSEiIiLZY8JCREREsseEhYiIiGSPCQsRERHJnllvayYiak6HS1Klz2tzOjTbegWdAE94Yv3x9RCLjzXbeomoOp5hISIiItljwkJERESyx4SFiIiIZI8JCxEREckeExYiIiKSPd4lRETUAJeL71h0/Vcu3GjydVhBgfFWnjh08SYqoav3cp1L/uibdk0TF7UtPMNCREREsseEhYiIiGSPCQsRERHJHhMWIiIikj0mLERERCR7TFiIiIhI9piwEBERkewxYSEiIiLZY8JCREREsseEhYiIiGSPCQsRERHJHhMWIiIikj0mLERERCR7TFiIiIhI9piwEBERkewxYSEiIiLZY8JCREREsseEhYiIiGTPrIRlzZo18PHxgVqtRlhYGA4dOlRj3ZMnT+Lpp5+Gj48PBEFAcnJytToJCQkQBMHgp0ePHuaERkRERK2QyQlLamoqYmNjER8fj8OHDyMoKAiRkZEoLCw0Wr+8vBx+fn5YtmwZ3N3da2z3kUceQV5envTzww8/mBoaERERtVJWpi6wevVqTJ8+HdHR0QCAdevW4euvv8aGDRswb968avUfe+wxPPbYYwBgdL4UiJVVrQlNS/B2+jlLh1DNqxHdLR0CERFRg5mUsFRUVCA7OxtxcXFSmUKhQHh4OLKyshoUyPnz5+Hp6Qm1Wo3+/fsjKSkJXbp0MVpXo9FAo9FI0yUlJQAArVYLrVZrdgz6Zc1tQxCrzF53UzFnWxrSD4JOqHW+lQWGTZnzveiXkeN32pTu/370ny3xndWmrn2sKdZ171/j/10qoWq2eIxpju/H3H1B3zdaUV77kDn029AatsVcDT1G1tZmfQiiKIr1rXzt2jV4eXlh//796N+/v1Q+d+5cfPfddzh48GCty/v4+GD27NmYPXu2Qfn//vc/lJaW4qGHHkJeXh4SExNx9epVnDhxAo6OjtXaSUhIQGJiYrXylJQU2NnZ1XdziIiIyILKy8vx3HPP4datW3Bycqq1rsmXhJrCqFGjpM+BgYEICwtD165d8dlnn+GFF16oVj8uLg6xsbHSdElJCby9vTFixIg6N7g2Wq0W6enpiIiIgEpl+l9Na/b+Yva6m8rMYd1MXqYh/bD++Ppa5x+6eNPkeBoqyPEpk5cRxCr43L2AXLU/REHZBFHJ09Hb26TPVlBgjFVP7Kw8hUroLBiVob6+Ls22LkEnwOOqB/K88iBe/tFonavFd5stHqPrdwpu8nWYuy94leQAAOK7hjZRZM1HKyqQXtodEQ7noBLk8/vQnLT9/tGgY6Qx+isk9WFSwuLq6gqlUomCggKD8oKCgkYdf9KuXTt0794dv/xiPAGwsbGBjY1NtXKVStUonWhuO3I8sDWkP8zpB1FR+wk7Sxz4GvK9iIJSlt9rUzH2/VRCJ6uEpa59rKnWKaLS6LwqNN7pcXM053dj6r6g75vWdIBXCbpWtT0m+eN40FjHWn1b9WXSxThra2uEhIQgIyNDKtPpdMjIyDC4RNRQpaWluHDhAjw8PBqtTSIiImq5TL4kFBsbi6ioKISGhqJv375ITk5GWVmZdNfQ1KlT4eXlhaSkJAD3BuqeOnVK+nz16lXk5OTAwcEB3brdu1zx2muvYezYsejatSuuXbuG+Ph4KJVKTJ48ubG2k4iIiFowkxOWiRMnoqioCAsXLkR+fj6Cg4ORlpYGNzc3AMClS5egUPx54ubatWt49NFHpemVK1di5cqVGDJkCDIzMwEAV65cweTJk3Hjxg107NgRAwcOxIEDB9CxY8cGbh4RERG1BmYNuo2JiUFMTIzRefokRM/Hxwd13Yi0detWc8IgIiKiNkIWdwkRmatzSXaddfoV3zK5XZ2gxPUOA/DYlU1QtMJnsRzo8pKlQyAiMknbfQIOERERtRhMWIiIiEj2mLAQERGR7DFhISIiItljwkJERESyx4SFiIiIZI8JCxEREcken8NCrd7/pzD9LdpKqPAYBuB/wq+oEprm5XZP6kx/kzYRUVvFMyxEREQke0xYiIiISPaYsBAREZHscQwLERE1qaxfb1g6BAP9/TpYOgQyA8+wEBERkewxYSEiIiLZY8JCREREsseEhYiIiGSPCQsRERHJHhMWIiIikj0mLERERCR7TFiIiIhI9piwEBERkewxYSEiIiLZY8JCREREsseEhYiIiGSPCQsRERHJHhMWIiIikj0mLERERCR7TFiIiIhI9piwEBERkexZWToAorbq/1P8YrF1XylJtdi6qe2x5L5uzJHiq5jRLtDSYZCJeIaFiIiIZI8JCxEREckeExYiIiKSPY5hIaIWJevCjWZblxUUGG/liUMXb8K95E6zrZeIquMZFiIiIpI9JixEREQke0xYiIiISPbMSljWrFkDHx8fqNVqhIWF4dChQzXWPXnyJJ5++mn4+PhAEAQkJyc3uE0iIiJqW0xOWFJTUxEbG4v4+HgcPnwYQUFBiIyMRGFhodH65eXl8PPzw7Jly+Du7t4obRIREVHbYnLCsnr1akyfPh3R0dHo2bMn1q1bBzs7O2zYsMFo/cceewxvvfUWJk2aBBsbm0Zpk4iIiNoWk25rrqioQHZ2NuLi4qQyhUKB8PBwZGVlmRWAOW1qNBpoNBppuqSkBACg1Wqh1WrNikO//P3/mkoQq8xed1MxZ1sa0g+CTqh1vlUjD5tSQtWo7ekp/mhX0UTtW1p9vgd9ncb+zlqS+/ugqfa1hmqO78fcfUGufSbAClrRtG3R1zd1udakocfI2tqsD5MSluvXr6Oqqgpubm4G5W5ubjhz5owpTTWozaSkJCQmJlYr37NnD+zs7MyK437p6elmLefb4DU3vl27zpm9rDn94AnPWuePt6p9vslcejVuew8IcZnYpO1bymMm1B1j1bPJ4mgpxlj1BFzk2Q+mfJcNZfK+0MS/nw2x67Z5y6WXdm/cQFqSP44J5h4jjSkvL6933Rb54Li4uDjExsZK0yUlJfD29saIESPg5ORkdrtarRbp6emIiIiASmX6XwZr9srrBV8AMHNYN5OXaUg/rD++vtb5hy7eNDme2niV5DRqe3oKqBDiMhHZN1OhQ+P9NSEXV52C66xjBQXGWPXEzspTqISu6YOSofv7wK3ksKXDMao+32VDmbsvNNXvZ0N5tVPjRWfTkimtqEB6aXdEOJyDSmibvw/afv9o0DHSGP0VkvowKWFxdXWFUqlEQUGBQXlBQUGNA2qbok0bGxuj42FUKlWjdKK57YiCssHrbmwN6Q9z+kFUiLXOb+wDX1UTJxM6aJt8HZZgyvdQCV2bTVj0KqGT7X7QnN+NqfuCXPtMhJXZSYdK0LXZhAV/HA8a61irb6u+TLoYZ21tjZCQEGRkZEhlOp0OGRkZ6N+/vylNNWmbRERE1LqYfEkoNjYWUVFRCA0NRd++fZGcnIyysjJER0cDAKZOnQovLy8kJSUBuDeo9tSpU9Lnq1evIicnBw4ODujWrVu92iQiIqK2zeSEZeLEiSgqKsLChQuRn5+P4OBgpKWlSYNmL126BIXizxM3165dw6OPPipNr1y5EitXrsSQIUOQmZlZrzaJiIiobTNr0G1MTAxiYmKMztMnIXo+Pj4QxdrHNdTVJhEREbVtbfeGciIiImoxmLAQERGR7DFhISIiItlrkQ+Oa25rc9bWq97hkhtNHInp1uZ0MHkZQSfAE55Yf3x9nc9VISIiag5MWIhINjqXZFs6BANKqACXXvAqyYH83hR2T3P0mWE/yPNhcNT68ZIQERERyR7PsLRyWRdMv0xlBQXGW3ni0MWbbf6R7EREJA88w0JERESyx4SFiIiIZI8JCxEREckeExYiIiKSPSYsREREJHtMWIiIiEj2mLAQERGR7DFhISIiItljwkJERESyx4SFiIiIZI8JCxEREckeExYiIiKSPSYsREREJHtMWIiIiEj2rCwdABE1v84l2XXWUUIFuPSCV0kOqqBthqiIiGrGMyxEREQke0xYiIiISPaYsBAREZHsMWEhIiIi2WPCQkRERLLHhIWIiIhkjwkLERERyR4TFiIiIpI9JixEREQke0xYiIiISPaYsBAREZHsMWEhIiIi2WPCQkRERLLHhIWIiIhkjwkLERERyR4TFiIiIpI9sxKWNWvWwMfHB2q1GmFhYTh06FCt9T///HP06NEDarUavXv3xq5duwzmT5s2DYIgGPyMHDnSnNCIiIioFbIydYHU1FTExsZi3bp1CAsLQ3JyMiIjI3H27Fl06tSpWv39+/dj8uTJSEpKwpgxY5CSkoLx48fj8OHD6NWrl1Rv5MiR2LhxozRtY2Nj5iZRU+hckm3pEIiIqA0z+QzL6tWrMX36dERHR6Nnz55Yt24d7OzssGHDBqP133nnHYwcORJz5szBww8/jMWLF6NPnz54//33DerZ2NjA3d1d+mnfvr15W0REREStjklnWCoqKpCdnY24uDipTKFQIDw8HFlZWUaXycrKQmxsrEFZZGQkduzYYVCWmZmJTp06oX379hg+fDiWLFmCDh06GG1To9FAo9FI0yUlJQAArVYLrVZryiYZ0C/7YBuCTqjX8latZEiQfjvu3x4lVJYKxyIUf2yvoo1t9/3YB+wDvdbWDwKsoBVN+/9aX9/U5VqTmo6RjdFmfZiUsFy/fh1VVVVwc3MzKHdzc8OZM2eMLpOfn2+0fn5+vjQ9cuRIPPXUU/D19cWFCxcwf/58jBo1CllZWVAqldXaTEpKQmJiYrXyPXv2wM7OzpRNMio9Pd1g2hOe9VpuvFX96rUUY6x6/jnh0qvmiq1YiMtES4dgcewD9oFea+qHXbfNWy69tHvjBtKS/HFsfPAY2RDl5eX1rmvyGJamMGnSJOlz7969ERgYCH9/f2RmZuKJJ56oVj8uLs7grE1JSQm8vb0xYsQIODk5mR2HVqtFeno6IiIioFL9+ZfE+uPr67X8oYs3zV63nFhBgTFWPbGz8hQqoQMAeJXkWDaoZqaACiEuE5F9MxU6NN5fEy0J+4B9oNfa+sGrnRovOpv2R5hWVCC9tDsiHM5BJeiaKDJ50/b7h9FjZEPor5DUh0kJi6urK5RKJQoKCgzKCwoK4O7ubnQZd3d3k+oDgJ+fH1xdXfHLL78YTVhsbGyMDspVqVSN0okPtiMqxHotpz+4txaV0EnbVNUK/pMyhw7aNrvteuwD9oFea+kHEVZmJx0qQddmExb8cVxsrGOtvq36MulinLW1NUJCQpCRkSGV6XQ6ZGRkoH///kaX6d+/v0F94N7ppJrqA8CVK1dw48YNeHh4mBIeERERtVImjx6KjY3FRx99hM2bN+P06dN45ZVXUFZWhujoaADA1KlTDQblzpo1C2lpaVi1ahXOnDmDhIQE/Pzzz4iJiQEAlJaWYs6cOThw4AByc3ORkZGBcePGoVu3boiMjGykzSQiIqKWzOQxLBMnTkRRUREWLlyI/Px8BAcHIy0tTRpYe+nSJSgUf+ZBAwYMQEpKCt58803Mnz8fAQEB2LFjh/QMFqVSiWPHjmHz5s0oLi6Gp6cnRowYgcWLF/NZLERERATAzEG3MTEx0hmSB2VmZlYre/bZZ/Hss88arW9ra4vdu3ebEwYRERG1EW33hnIiIiJqMZiwEBERkewxYSEiIiLZY8JCREREsseEhYiIiGSPCQsRERHJHhMWIiIikj0mLERERCR7TFiIiIhI9piwEBERkewxYSEiIiLZY8JCREREsseEhYiIiGSPCQsRERHJHhMWIiIikj0mLERERCR7TFiIiIhI9piwEBERkewxYSEiIiLZY8JCREREsseEhYiIiGSPCQsRERHJHhMWIiIikj0mLERERCR7TFiIiIhI9piwEBERkewxYSEiIiLZY8JCREREsseEhYiIiGSPCQsRERHJHhMWIiIikj0mLERERCR7TFiIiIhI9piwEBERkewxYSEiIiLZY8JCREREsseEhYiIiGSPCQsRERHJnlkJy5o1a+Dj4wO1Wo2wsDAcOnSo1vqff/45evToAbVajd69e2PXrl0G80VRxMKFC+Hh4QFbW1uEh4fj/Pnz5oRGRERErZDJCUtqaipiY2MRHx+Pw4cPIygoCJGRkSgsLDRaf//+/Zg8eTJeeOEFHDlyBOPHj8f48eNx4sQJqc6KFSvw7rvvYt26dTh48CDs7e0RGRmJu3fvmr9lRERE1GqYnLCsXr0a06dPR3R0NHr27Il169bBzs4OGzZsMFr/nXfewciRIzFnzhw8/PDDWLx4Mfr06YP3338fwL2zK8nJyXjzzTcxbtw4BAYG4uOPP8a1a9ewY8eOBm0cERERtQ5WplSuqKhAdnY24uLipDKFQoHw8HBkZWUZXSYrKwuxsbEGZZGRkVIycvHiReTn5yM8PFya7+zsjLCwMGRlZWHSpEnV2tRoNNBoNNL0rVu3AAA3b96EVqs1ZZMMaLValJeX48aNG1CpVFL53ZL6nenRlVeYvW450UGBcqty6CoroIMOAFB5x8JBNTMdgPLycmjv4I8eaHvYB+wDvdbWD3etdbhhZdr/11pRce/4IFRAJbSGXjCd9sYNo8fIhrh9+zaAeycv6mJSwnL9+nVUVVXBzc3NoNzNzQ1nzpwxukx+fr7R+vn5+dJ8fVlNdR6UlJSExMTEauW+vr712xCq08eWDkAWvrB0ADLAPmAf6LWufviXpQNokRKarOXbt2/D2dm51jomJSxyERcXZ3DWRqfT4ebNm+jQoQMEQTC73ZKSEnh7e+Py5ctwcnJqjFBbJPYD+wBgHwDsAz32A/sAaJo+EEURt2/fhqenZ511TUpYXF1doVQqUVBQYFBeUFAAd3d3o8u4u7vXWl//b0FBATw8PAzqBAcHG23TxsYGNjY2BmXt2rUzZVNq5eTk1GZ3yPuxH9gHAPsAYB/osR/YB0Dj90FdZ1b0TBp0a21tjZCQEGRkZEhlOp0OGRkZ6N+/v9Fl+vfvb1AfANLT06X6vr6+cHd3N6hTUlKCgwcP1tgmERERtS0mXxKKjY1FVFQUQkND0bdvXyQnJ6OsrAzR0dEAgKlTp8LLywtJSUkAgFmzZmHIkCFYtWoVRo8eja1bt+Lnn3/Gv//9bwCAIAiYPXs2lixZgoCAAPj6+mLBggXw9PTE+PHjG29LiYiIqMUyOWGZOHEiioqKsHDhQuTn5yM4OBhpaWnSoNlLly5BofjzxM2AAQOQkpKCN998E/Pnz0dAQAB27NiBXr16SXXmzp2LsrIyvPTSSyguLsbAgQORlpYGtVrdCJtYfzY2NoiPj692uamtYT+wDwD2AcA+0GM/sA8Ay/eBINbnXiIiIiIiC+K7hIiIiEj2mLAQERGR7DFhISIiItljwkJERESyx4SFiIiIZI8JSy2efPJJdOnSBWq1Gh4eHvjb3/6Ga9euWTqsZpObm4sXXngBvr6+sLW1hb+/P+Lj41FR0Tpe8lhfS5cuxYABA2BnZ9eoT1SWuzVr1sDHxwdqtRphYWE4dOiQpUNqVt9//z3Gjh0LT09PCILQ5t4en5SUhMceewyOjo7o1KkTxo8fj7Nnz1o6rGb3wQcfIDAwUHq6a//+/fG///3P0mFZ1LJly6RnqDUnJiy1GDZsGD777DOcPXsWX375JS5cuIBnnnnG0mE1mzNnzkCn0+HDDz/EyZMn8fbbb2PdunWYP3++pUNrVhUVFXj22WfxyiuvWDqUZpOamorY2FjEx8fj8OHDCAoKQmRkJAoLCy0dWrMpKytDUFAQ1qxZY+lQLOK7777DzJkzceDAAaSnp0Or1WLEiBEoKyuzdGjNqnPnzli2bBmys7Px888/Y/jw4Rg3bhxOnjxp6dAs4qeffsKHH36IwMDA5l+5SPX21VdfiYIgiBUVFZYOxWJWrFgh+vr6WjoMi9i4caPo7Oxs6TCaRd++fcWZM2dK01VVVaKnp6eYlJRkwagsB4C4fft2S4dhUYWFhSIA8bvvvrN0KBbXvn17cf369ZYOo9ndvn1bDAgIENPT08UhQ4aIs2bNatb18wxLPd28eRNbtmzBgAEDoFKpLB2Oxdy6dQsuLi6WDoOaUEVFBbKzsxEeHi6VKRQKhIeHIysry4KRkSXdunULANr0739VVRW2bt2KsrKyNvmuu5kzZ2L06NEG/zc0JyYsdXj99ddhb2+PDh064NKlS/jqq68sHZLF/PLLL3jvvffw97//3dKhUBO6fv06qqqqpNdt6Lm5uSE/P99CUZEl6XQ6zJ49G48//rjBa1XaiuPHj8PBwQE2NjZ4+eWXsX37dvTs2dPSYTWrrVu34vDhw9J7Ai2hzSUs8+bNgyAItf6cOXNGqj9nzhwcOXIEe/bsgVKpxNSpUyG28LcZmNoHAHD16lWMHDkSzz77LKZPn26hyBuPOX1A1FbNnDkTJ06cwNatWy0dikU89NBDyMnJwcGDB/HKK68gKioKp06dsnRYzeby5cuYNWsWtmzZ0uzv+Ltfm3uXUFFREW7cuFFrHT8/P1hbW1crv3LlCry9vbF///4WfTrQ1D64du0ahg4din79+mHTpk0GL7dsqczZDzZt2oTZs2ejuLi4iaOzrIqKCtjZ2eGLL74weGN6VFQUiouL2+RZRkEQsH379jb5BvmYmBh89dVX+P777+Hr62vpcGQhPDwc/v7++PDDDy0dSrPYsWMH/vrXv0KpVEplVVVVEAQBCoUCGo3GYF5TMfltzS1dx44d0bFjR7OW1el0AACNRtOYITU7U/rg6tWrGDZsGEJCQrBx48ZWkawADdsPWjtra2uEhIQgIyNDOkDrdDpkZGQgJibGssFRsxFFEf/4xz+wfft2ZGZmMlm5j06na/HHAVM88cQTOH78uEFZdHQ0evTogddff71ZkhWgDSYs9XXw4EH89NNPGDhwINq3b48LFy5gwYIF8Pf3b9FnV0xx9epVDB06FF27dsXKlStRVFQkzXN3d7dgZM3r0qVLuHnzJi5duoSqqirk5OQAALp16wYHBwfLBtdEYmNjERUVhdDQUPTt2xfJyckoKytDdHS0pUNrNqWlpfjll1+k6YsXLyInJwcuLi7o0qWLBSNrHjNnzkRKSgq++uorODo6SuOXnJ2dYWtra+Homk9cXBxGjRqFLl264Pbt20hJSUFmZiZ2795t6dCajaOjY7WxS/qxnc06pqlZ70lqQY4dOyYOGzZMdHFxEW1sbEQfHx/x5ZdfFq9cuWLp0JrNxo0bRQBGf9qSqKgoo32wd+9eS4fWpN577z2xS5cuorW1tdi3b1/xwIEDlg6pWe3du9fo9x4VFWXp0JpFTb/7GzdutHRozer5558Xu3btKlpbW4sdO3YUn3jiCXHPnj2WDsviLHFbc5sbw0JEREQtT+sYkEBEREStGhMWIiIikj0mLERERCR7TFiIiIhI9piwEBERkewxYSEiIiLZY8JCREREsseEhYiIiGSPCQsRERHJHhMWIiIikj0mLERERCR7/z9HhY5nYwKkDgAAAABJRU5ErkJggg==",
+      "text/plain": [
+       "<Figure size 640x480 with 1 Axes>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    }
+   ],
+   "source": [
+    "for ctrl_s in ctrl_str:\n",
+    "    plt.hist(\n",
+    "        [r for r, t in zip(logs[\"env/reward_dist\"], task_list) if t == ctrl_s], density=True, alpha=0.5, label=ctrl_s\n",
+    "    )\n",
+    "plt.legend(loc=\"best\")\n",
+    "plt.title(\"reward distribution\")\n",
+    "plt.grid(True)\n",
+    "plt.show()"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Save model\n",
+    "Finally, we save the model to disk for later usage."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "gpt2_model.save_pretrained(\"gpt2-imdb-ctrl\")\n",
+    "gpt2_tokenizer.save_pretrained(\"gpt2-imdb-ctrl\")"
+   ]
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": "trl",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.9.16"
+  },
+  "vscode": {
+   "interpreter": {
+    "hash": "d2cfb53525227c89f8d14fa784301fa46c451cc9223d94ccce9e17956835eea2"
+   }
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 4
+}
diff --git a/with_trlx/bin_exp.py b/with_trlx/bin_exp.py
index bee074a..6663fef 100644
--- a/with_trlx/bin_exp.py
+++ b/with_trlx/bin_exp.py
@@ -11,9 +11,13 @@ There are wrappers for GSM8K and for the ASDiv datasets.
 By default, we support the GPT2 model.
 
 """
-
-print("Doing imports.")
 import os
+
+os.environ["TOKENIZERS_PARALLELISM"] = "false"
+os.environ["NCCL_DEBUG"]             = "WARN"
+os.environ["DATASETS_VERBOSITY"]     = "warning"
+os.environ["TRANSFORMERS_VERBOSITY"] = "warning"
+
 import collections
 import contextlib
 import enum
@@ -27,26 +31,45 @@ import datasets
 import fire
 import general_utils
 import numpy as np
+import peft
 import rich
 import rich.logging
+import rich.status
 import torch
 import transformers
 import trlx
 import yaml
 from trlx.data.configs import TRLConfig
+from trlx.models.modeling_ppo import AutoModelForSeq2SeqLMWithHydraValueHead
 
 import lib_data
 import lib_metric
 import lib_reward
-print("Done with imports")
+import lib_modeling
 
 
+datasets    .logging.set_verbosity_warning()
+transformers.logging.set_verbosity_warning()
+logging.getLogger("datasets"    ).setLevel(logging.WARNING)
+logging.getLogger("transformers").setLevel(logging.WARNING)
+logging.getLogger("deepspeed"   ).setLevel(logging.WARNING)
+
+DEFAULT_MODEL_PRECISION = torch.bfloat16
+DEFAULT_INT8_MODE       = True
+DEFAULT_USE_LORA        = True
 DEFAULT_DETERMINISTIC   = False
 DEFAULT_DO_SINGLE_PROC  = False
 DEFAULT_DATASET_TO_USE  = "gsm8k"
-DEFAULT_REWARD_MODEL    = "google/flan-t5-xl"
+DEFAULT_REWARD_MODEL    = "google/flan-t5-xxl"
 DEFAULT_MAIN_MODEL      = DEFAULT_REWARD_MODEL
 DEFAULT_TOKENIZER_MODEL = DEFAULT_REWARD_MODEL
+DEFAULT_PEFT_CONFIG     = dict(
+    r              = 8,
+    lora_alpha     = 32,
+    task_type      = peft.TaskType.SEQ_2_SEQ_LM,
+    lora_dropout   = 0,
+    inference_mode = False,
+)
 
 
 # -----------------------------------------------------------------------------
@@ -70,6 +93,13 @@ assert Path(DEFAULT_PPO_CONFIG_PATH).exists(), (
     f"{DEFAULT_PPO_CONFIG_PATH = }")
 torch.cuda.set_device(LOCAL_RANK)
 
+@contextlib.contextmanager
+def main_status(msg):
+    if os.environ["RANK"] == "0":
+        with rich.status.Status(msg) as status:
+            yield status
+    else:
+        yield
 
 
 class MergedExtraInfo:
@@ -345,6 +375,43 @@ def _setup_config(
     return config
 
 
+def init_model(
+        *, 
+        model_precision, 
+        model_name_or_path, 
+        accelerator_device,
+    ):
+
+    if model_precision in (torch.float16, torch.bfloat16):
+
+        assert model_precision in (
+            torch.bfloat16, torch.float16
+        ), (model_precision)
+        model = transformers.AutoModelForSeq2SeqLM.from_pretrained(
+            model_name_or_path, torch_dtype=model_precision
+        )
+    elif model_precision == "int8":
+        dmap_keys = ["encoder", "lm_head", "shared", "decoder"]
+        dmap = {k: accelerator_device for k in dmap_keys}
+        model = transformers.AutoModelForSeq2SeqLM.from_pretrained(
+            model_name_or_path, 
+            load_in_8bit = True,
+            torch_dtype  = torch.float16, # Required for 8-bit
+            device_map   = dmap,
+        )
+
+    elif model_precision == torch.float32 or model_precision is None:
+        assert False
+        model = transformers.AutoModelForSeq2SeqLM.from_pretrained(
+            model_name_or_path
+        )
+    else:
+        assert False
+        raise ValueError(f"Unknown model precision: {model_precision}")
+    
+    return model
+
+
 def train(
     *,
     reward_model_hf_name_or_path: Optional[str] = DEFAULT_REWARD_MODEL,
@@ -353,16 +420,22 @@ def train(
     model_class_name: str                       = ModelClassChoices.SEQ2SEQ,
     trlx_config_path: Union[Path, str]          = DEFAULT_PPO_CONFIG_PATH,
     val_subset_size: Optional[int]              = None,
+    model_precision: str                        = DEFAULT_MODEL_PRECISION,
     dataset_to_use: str                         = DEFAULT_DATASET_TO_USE,
     do_single_proc: int                         = DEFAULT_DO_SINGLE_PROC,
     deterministic: bool                         = DEFAULT_DETERMINISTIC,
+    peft_config: bool                           = DEFAULT_PEFT_CONFIG,
+    
     log_level: str                              = "INFO",
+    int8_mode: bool                             = DEFAULT_INT8_MODE,
+    use_lora: bool                              = DEFAULT_USE_LORA,
 ):
     # -------------------------------------------------------------------------
     # Logging stuff.
     # -------------------------------------------------------------------------
     args = locals().copy()
     _logging(args=args, log_level=log_level)
+    logging.getLogger("lib_data").setLevel(logging.WARNING)
 
     if RANK == 0:
         rich.print(
@@ -386,15 +459,16 @@ def train(
     # -------------------------------------------------------------------------
     # Dataset
     # -------------------------------------------------------------------------
-    _sanity_check_model_type(model_class_name, reward_model_hf_name_or_path)
-    _sanity_check_model_type(model_class_name, main_model_hf_name_or_path)
-    config_dict = yaml.safe_load(Path(trlx_config_path).read_text())
-    ds_train_obj, ds_eval_obj, reward_tokenizer = _build_dataset(
-        tokenizer_hf_name_or_path  = tokenizer_hf_name_or_path,
-        val_subset_size            = val_subset_size,
-        dataset_to_use             = dataset_to_use,
-        config_dict                = config_dict, 
-    )
+    with main_status("[bold]Building Dataset..."):
+        _sanity_check_model_type(model_class_name, reward_model_hf_name_or_path)
+        _sanity_check_model_type(model_class_name, main_model_hf_name_or_path)
+        config_dict = yaml.safe_load(Path(trlx_config_path).read_text())
+        ds_train_obj, ds_eval_obj, reward_tokenizer = _build_dataset(
+            tokenizer_hf_name_or_path  = tokenizer_hf_name_or_path,
+            val_subset_size            = val_subset_size,
+            dataset_to_use             = dataset_to_use,
+            config_dict                = config_dict, 
+        )
     
     # -------------------------------------------------------------------------
     # Setup Config.
@@ -416,26 +490,53 @@ def train(
     os.environ["TOKENIZERS_PARALLELISM"] = "false"
 
     # -------------------------------------------------------------------------
-    # Metric and Reward.
+    # Model, Metric and Reward.
     # -------------------------------------------------------------------------
     # The metric and the reward need access to the labe.
     # -------------------------------------------------------------------------
     # The metric needs 
     merger = MergedExtraInfo(
         ds_train_obj = ds_train_obj,
-        ds_eval_obj  = ds_eval_obj,
-    )
-    
+        ds_eval_obj  = ds_eval_obj ,)
     metric_accuracy = lib_metric.ScratchpadAnswerAccuracy(
-        extra_info_engine = merger.merged_get_extra_info,
-    )
+        extra_info_engine = merger.merged_get_extra_info)
+    assert main_model_hf_name_or_path == reward_model_hf_name_or_path, (
+        f"For now, the main and reward model must be the same. "
+        f"{main_model_hf_name_or_path} "
+        f"{reward_model_hf_name_or_path}")
+
+    with main_status("[bold]Loading interior model..."):
+        interior_model = init_model(
+            model_precision    = model_precision,
+            accelerator_device = int(os.environ["LOCAL_RANK"]),
+            model_name_or_path = main_model_hf_name_or_path,
+        )
+        assert int8_mode
+
+    with main_status("[bold]Hydra Value Head..."):
+        if use_lora:
+            model = lib_modeling.AutoModelForSeq2SeqLMWithHydraValueHead(
+                interior_model,
+                peft.LoraConfig(**peft_config),
+                int(os.environ["LOCAL_RANK"]),
+            )
+            reward_model_hf_name_or_path = interior_model
+
+        else:
+            assert False
+            model = AutoModelForSeq2SeqLMWithHydraValueHead(
+                interior_model,
+                num_layers_unfrozen=1,
+            )
 
     # Afaik the eval should not need the extra info engine.
     scratchpad_reward_fn = lib_reward.ScratchpadRewardFn(
         reward_model_hf_name_or_path = reward_model_hf_name_or_path,
         get_extra_info_fn            = merger.merged_get_extra_info,
         reward_tokenizer             = reward_tokenizer,
+        use_frozen_head              = True,
         do_single_proc               = do_single_proc,
+        freeze_model                 = False,
         metric_fn                    = metric_accuracy,
     )
 
@@ -444,7 +545,7 @@ def train(
     # -------------------------------------------------------------------------
     model = trlx.train(
         eval_prompts  = list(ds_eval_obj),
-        model_path    = main_model_hf_name_or_path,
+        model_path    = model,
         metric_fn     = metric_accuracy,
         reward_fn     = scratchpad_reward_fn,
         prompts       = list(ds_train_obj),
@@ -453,4 +554,5 @@ def train(
 
 
 if __name__ == "__main__":
-    fire.Fire(train)
+    with torch.autograd.detect_anomaly():
+        fire.Fire(train)
diff --git a/with_trlx/bin_launch.py b/with_trlx/bin_launch.py
new file mode 100755
index 0000000..5376568
--- /dev/null
+++ b/with_trlx/bin_launch.py
@@ -0,0 +1,186 @@
+#!/usr/bin/env python
+import os
+import itertools
+import shlex
+import sys
+import fire
+import rich
+import subprocess
+import pretty_traceback
+pretty_traceback.install()
+
+
+DEFAULT_ONE                     = False
+DEFAULT_SERVER_PORT             = 29505
+DEFAULT_VAL_SUBSET_SIZE         = 3
+DEFAULT_ACCELERATE_CONFIG       = "accelerate_ddp_no.yaml"
+DEFAULT_MIXED_PRECISION_DEFAULT = "no"
+
+
+def _check_mixed_precision_compatibility(default):
+    """
+    If any of the GPUs is not an A100, disable bf16.
+    """
+
+    # Extract the GPU models
+    gpu_models = subprocess.check_output(
+        "nvidia-smi --query-gpu=name --format=csv,noheader | sort | uniq", 
+        shell=True, universal_newlines=True
+    ).strip().split("\n")
+
+    # If any of the GPUs is not a A100, disable bf16
+    for model in gpu_models:
+        if "a100" not in model:
+            rich.print(f"[bold blue]Disabling bf16 because of GPU model: {model}")
+            return "no"
+
+    return default
+
+def _kill_wandb_servers():
+    subprocess.call(
+        "pgrep wandb | xargs kill -9",
+        shell=True, 
+        # stdout=subprocess.STDOUT, 
+        stderr=subprocess.DEVNULL,
+        universal_newlines=True,
+    )
+
+def _kill_other_python_processes():
+    subprocess.call(
+        f"pgrep python | grep -v {os.getpid()} | xargs kill -9",
+        shell=True,
+        # stdout=subprocess.STDOUT,
+        stderr=subprocess.DEVNULL,
+        universal_newlines=True,
+    )
+
+def dict_to_command_list(d):
+    return itertools.chain.from_iterable(
+        [[f"--{k}", v] for k, v in d.items()]
+    )
+
+def _build_command(
+        *,
+        bin_path, 
+        script_config,
+        accelerate_bin, 
+        accelerate_config, 
+        accel_config,
+    ):
+    if not os.path.exists(bin_path):
+        raise RuntimeError(f"Expected bin path to exist: {bin_path}")
+    
+    command = accelerate_bin + ["launch"]                   # Adds srun if multi node
+    command.extend(dict_to_command_list(accelerate_config)) # Adds nodes & procs info
+    command.extend(["--config_file", accel_config]) # Adds specific args
+    command.append(bin_path)                                # Adds script path 
+    command.extend(dict_to_command_list(script_config))     # Adds script args
+    command = [str(c) for c in command]
+
+    return command
+
+def _build_accelerate_config(one, server_port, mixed_precision):
+
+    accelerate_path = subprocess.check_output(
+        "which accelerate",
+        shell              = True,
+        universal_newlines = True,
+    ).strip()
+
+    srun_path = subprocess.check_output(
+        "which srun",
+        shell              = True, 
+        universal_newlines = True,
+    ).strip()
+
+    total_processes = (
+        int(os.environ["SLURM_NNODES"      ]) * 
+        int(os.environ["SLURM_GPUS_ON_NODE"])
+    )
+    server_hostname = os.environ["SLURMD_NODENAME"]
+    num_nodes       = int(os.environ["SLURM_NNODES"])
+
+    conditional_args_single_node = {}
+    conditional_args_multi_node  = {"deepspeed_multinode_launcher": "standard"}
+    accelerate_bin_single_node   = [accelerate_path]
+    accelerate_bin_multi_node    = [srun_path, accelerate_path]
+
+    if num_nodes > 1 and not one:
+        accelerate_bin              = accelerate_bin_multi_node
+        accelerate_conditional_args = conditional_args_multi_node
+    else:
+        accelerate_bin              = accelerate_bin_single_node
+        accelerate_conditional_args = conditional_args_single_node
+
+    if one:
+        rich.print("[bold yellow]Only using one process, to debug.")
+        num_nodes        = 1
+        accelerate_bin   = accelerate_bin_single_node
+        total_processes  = 1
+        server_hostname  = ""
+        accelerate_conditional_args = conditional_args_single_node
+
+    accelerate_config = {
+        "main_process_port": server_port,
+        "mixed_precision":   mixed_precision,
+        "main_process_ip":   server_hostname,
+        "num_processes":     total_processes,
+        "num_machines":      num_nodes,
+    }
+
+    accelerate_config = dict(
+        **accelerate_config, 
+        **accelerate_conditional_args,
+    )
+    
+    return accelerate_bin, accelerate_config
+
+
+def main(
+        one                     = DEFAULT_ONE,
+        bin_path                = f"{os.getcwd()}/bin_exp.py",
+        server_port             = DEFAULT_SERVER_PORT,
+        val_subset_size         = DEFAULT_VAL_SUBSET_SIZE, 
+        default_accel_config    = DEFAULT_ACCELERATE_CONFIG,
+        mixed_precision_default = DEFAULT_MIXED_PRECISION_DEFAULT,
+    ):
+
+    # Check mixed precision compatibility
+    mixed_precision = _check_mixed_precision_compatibility(
+        mixed_precision_default)
+    del mixed_precision_default
+    
+    # Build accelerate_bin and accelerate_config
+    accelerate_bin, accelerate_config = _build_accelerate_config(
+        one=one, server_port=server_port, mixed_precision=mixed_precision)
+    del one, server_port, mixed_precision
+    
+    # Build script_config
+    if val_subset_size is not None:
+        rich.print(f"[bold red]>>> Using a subset! Of size: {val_subset_size}")
+    script_config = {"val_subset_size": val_subset_size}
+    del val_subset_size
+
+    # Kill all wandb servers
+    _kill_wandb_servers()
+    
+    # Kill all python processes
+    _kill_other_python_processes()
+
+    # Run the training
+    command = _build_command(
+        bin_path             = bin_path,
+        script_config        = script_config,
+        accelerate_bin       = accelerate_bin,
+        accelerate_config    = accelerate_config, 
+        accel_config         = default_accel_config,
+    )
+    del bin_path, accelerate_bin, accelerate_config, script_config
+
+    rich.print("[bold green]>>> Running command:")
+    rich.print(f"[bold]{command}")
+    os.execv(command[0], command)
+
+
+if __name__ == "__main__":
+    fire.Fire(main)
\ No newline at end of file
diff --git a/with_trlx/bin_rl4lms_experimentation.py b/with_trlx/bin_rl4lms_experimentation.py
new file mode 100755
index 0000000..365d544
--- /dev/null
+++ b/with_trlx/bin_rl4lms_experimentation.py
@@ -0,0 +1,138 @@
+#!/usr/bin/env python
+# coding: utf-8
+
+import datetime
+import enum
+import json
+import logging
+import os
+import pickle
+import re
+import sys
+import types
+from pathlib import Path
+from typing import *
+
+import datasets
+import general_utils
+import general_utils as utils
+import numpy as np
+import pretty_traceback
+import rich
+import rich.console
+import torch
+import transformers
+import yaml
+from text2digits import text2digits
+from tqdm import tqdm
+
+import libs_compute_accuracy.dataset_asdiv as dataset_asdiv
+import libs_compute_accuracy.dataset_gsm8k as dataset_gsm8k
+
+pretty_traceback.install()
+datasets.logging.set_verbosity_error()
+transformers.logging.set_verbosity_error()
+
+CONSOLE = rich.console.Console(width=80)
+LOGGER = logging.getLogger(__name__)
+
+class DatasetChoices(str, enum.Enum):
+    gsm8k = "gsm8k"
+    gsm8k_silver = "gsm8k_silver"
+    asdiv = "asdiv"
+
+
+MODEL_PARALLEL = True
+MODEL_HF_NAME = "google/flan-t5-xxl"
+TRAIN_BATCH_SIZE = 1
+EVAL_BATCH_SIZE = 3
+LOG_LEVEL = logging.WARNING
+
+
+###############################################################################
+# Doesn't change
+###############################################################################
+DATASET_CHOICE = DatasetChoices.gsm8k
+MAX_PROMPT_LENGTH = 107
+MAX_EPISODE_LENGTH = 200
+torch.backends.cuda.matmul.allow_tf32 = True
+GENERATION_KWARGS = {
+    "max_new_tokens": MAX_EPISODE_LENGTH,
+    "min_length": 5,
+    "do_sample": True,
+    "top_k": 50,
+}
+
+###############################################################################
+#
+###############################################################################
+
+
+def main(
+    precision = "int8",
+    hf_model_name = MODEL_HF_NAME,
+):
+
+    local_rank = int(os.getenv("LOCAL_RANK", "0"))
+    global_rank = int(os.getenv("RANK", "0"))
+    world_size = int(os.getenv("WORLD_SIZE", "1"))
+
+    logging.basicConfig(
+        level=LOG_LEVEL,
+        format=(
+            f"[{local_rank + 1} / {world_size}]"
+            f"[bold]\[%(name)s]:[/]  %(message)s"
+        ),
+        datefmt="[%X]",
+        handlers=[rich.logging.RichHandler(markup=True, rich_tracebacks=True)],
+    )
+    
+    general_utils.check_contained(precision, ["int8", "fp16", "bf16", "fp32", None])
+
+    if precision == "int8":
+        accelerator_device = os.environ["LOCAL_RANK"]
+        dmap_keys = ["encoder", "lm_head", "shared", "decoder"]
+        device_map = {k: accelerator_device for k in dmap_keys}
+        model_inst = transformers.AutoModelForSeq2SeqLM.from_pretrained(
+            hf_model_name, 
+            device_map=device_map,
+            load_in_8bits=True,
+            torch_dtype=torch.bfloat16, 
+        )
+    elif precision == "fp16":
+        model_inst = transformers.AutoModelForSeq2SeqLM.from_pretrained(
+            hf_model_name, torch_dtype=torch.float16
+        )
+    elif precision == "bf16":
+        model_inst = transformers.AutoModelForSeq2SeqLM.from_pretrained(
+            hf_model_name, torch_dtype=torch.bfloat16
+        )
+    
+    model_tok = transformers.AutoTokenizer.from_pretrained(hf_model_name)
+
+    gsm8k_config = {
+        "args": {"max_sum_squares": 41957, "tokenizer": model_tok},
+        "id": "zero_shot_gsm8k_text_gen_pool",
+    }
+
+    asdiv_config = {
+        "args": {},
+        "id": "zero_shot_asdiv_text_gen_pool",
+    }
+
+    trainer = transformers.Trainer(
+        model=model_inst,
+        train_dataset=dataset_gsm8k.SupervisedGSM8K.prepare(
+            "train", 
+            hf_model_name,
+        ),
+        eval_dataset=
+    )
+
+    # transformers.logging.set_verbosity_error()
+    # datasets.logging.set_verbosity_error()
+    
+
+
+if __name__ == "__main__":
+    main()
diff --git a/with_trlx/config_ppo.yml b/with_trlx/config_ppo.yml
index 8c61dd8..d8917ed 100644
--- a/with_trlx/config_ppo.yml
+++ b/with_trlx/config_ppo.yml
@@ -12,7 +12,7 @@ train:
   trainer:                  "AcceleratePPOTrainer"
 
 model:
-  num_layers_unfrozen: 2
+  num_layers_unfrozen: 1
   model_arch_type:     "seq2seq"
 
 tokenizer:
@@ -57,4 +57,4 @@ method:
   gen_kwargs:
     synced_gpus:    True
     max_new_tokens: 120
-    do_sample:      True
\ No newline at end of file
+    do_sample:      False
\ No newline at end of file
diff --git a/with_trlx/lib_modeling.py b/with_trlx/lib_modeling.py
new file mode 100644
index 0000000..d340121
--- /dev/null
+++ b/with_trlx/lib_modeling.py
@@ -0,0 +1,81 @@
+
+import gc
+import inspect
+from copy import deepcopy
+from dataclasses import dataclass
+from typing import List, Optional, Tuple, Union
+
+import numpy as np
+import peft
+import torch
+import torch.nn as nn
+import transformers
+from torchtyping import TensorType
+from transformers.modeling_outputs import ModelOutput
+from transformers.models.bloom import modeling_bloom
+from transformers.models.opt import modeling_opt
+
+from trlx.models.modeling_base import PreTrainedModelWrapper
+from trlx.utils.modeling import (
+    hf_get_hidden_size,
+    make_head,
+)
+from trlx.models.modeling_ppo import (
+    AutoModelForSeq2SeqLMWithValueHead,
+    Seq2SeqLMOutputWithValue,
+    PreTrainedModelWrapper
+)
+
+
+class AutoModelForSeq2SeqLMWithHydraValueHead(PreTrainedModelWrapper):
+    _supported_modules = ["v_head", "frozen_head"]
+    _supported_args    = ["num_layers_unfrozen"]
+
+    def __init__(
+        self,
+        base_model: transformers.PreTrainedModel,
+        peft_config,
+        device
+    ):
+        super().__init__(base_model)
+        self.v_head = make_head(hf_get_hidden_size(self.base_model.config), 1)
+        
+        self.peft_config = peft_config
+        
+        for parameter in self.base_model.parameters():
+            parameter.requires_grad = False
+        self.frozen_head = base_model
+
+        self.base_model = peft.get_peft_model(base_model, peft_config).bfloat16()
+        type(self.base_model).print_trainable_parameters(self.base_model)
+        
+        self.v_head.to(self.base_model.dtype).to(device)
+    
+    def generate(self, *args, **kwargs) -> Union[ModelOutput, torch.LongTensor]:
+        return self.base_model.generate(*args, **kwargs)
+
+    def forward_hydra(self, **forward_kwargs):
+
+        return_dict = forward_kwargs.get("return_dict", True)
+        
+        forward_kwargs["output_hidden_states"] = True
+        forward_kwargs["return_dict"]          = True
+
+        forward_kwargs["output_attentions"]    = False
+        forward_kwargs["use_cache"]            = False
+
+        hydra_outputs = self.frozen_head(**forward_kwargs)
+        if not return_dict:
+            return hydra_outputs.logits
+        return hydra_outputs
+    
+
+    def forward(self, **forward_kwargs):
+        forward_kwargs["output_hidden_states"] = True
+        forward_kwargs["return_dict"         ] = True
+
+        outputs           = self.base_model(**forward_kwargs)
+        last_hidden_state = outputs.decoder_hidden_states[-1]
+        value             = self.v_head(last_hidden_state).squeeze(-1)
+
+        return Seq2SeqLMOutputWithValue(**outputs, value=value)
\ No newline at end of file
diff --git a/with_trlx/lib_reward.py b/with_trlx/lib_reward.py
index b5c4d83..ab519c7 100644
--- a/with_trlx/lib_reward.py
+++ b/with_trlx/lib_reward.py
@@ -55,6 +55,18 @@ def remove_special_token_ids(
     return filtered_input_ids
 
 
+def _maybe_frozen_head(model, input_dict, use_frozen_head):
+    if use_frozen_head:
+        with torch.no_grad():
+            return model(**input_dict)
+    return model(**input_dict)
+
+def _maybe_autocast(model_or_fn, dtype):
+    if dtype is None:
+        return model_or_fn
+    return torch.cuda.amp.autocast(dtype=dtype)(model_or_fn)
+
+
 def clone_hf_model(
         hf_model: transformers.PreTrainedModel
 ) -> transformers.PreTrainedModel:
@@ -65,10 +77,10 @@ def clone_hf_model(
     return copy
 
 
-class ScratchpadRewardFn(torch.nn.Module):
+class ScratchpadRewardFn:
     def __init__(
         self, *, 
-        reward_model_hf_name_or_path: str,
+        reward_model_hf_name_or_path: typing.Union[str, transformers.PreTrainedModel],
         get_extra_info_fn: typing.Callable[
             [typing.Union[str, typing.List[str]]], 
             typing.Union[typing.Dict, typing.List[typing.Dict]]
@@ -76,15 +88,29 @@ class ScratchpadRewardFn(torch.nn.Module):
         reward_tokenizer: transformers.PreTrainedTokenizerBase, 
         do_single_proc: bool,
         metric_fn,
+        freeze_model,
+        use_frozen_head,
     ):
         super().__init__()
-
+        
+        assert isinstance(freeze_model, bool), type(freeze_model)
+        assert isinstance(use_frozen_head, bool), type(use_frozen_head)
+        assert freeze_model ^ use_frozen_head, (
+            f"{freeze_model = }, {use_frozen_head = }")
+        
         #----------------------------------------------------------------
         # Build Models
         #----------------------------------------------------------------
-        reward_model = transformers.AutoModelForSeq2SeqLM.from_pretrained(
-            reward_model_hf_name_or_path)
-        
+        if isinstance(reward_model_hf_name_or_path, str):
+            assert False
+            reward_model = transformers.AutoModelForSeq2SeqLM.from_pretrained(
+                reward_model_hf_name_or_path)
+        else:
+            assert isinstance(reward_model_hf_name_or_path, transformers.PreTrainedModel), (
+                type(reward_model_hf_name_or_path).mro()
+            )
+            reward_model = reward_model_hf_name_or_path
+
         if do_single_proc:
             non_distributed = [clone_hf_model(
                 reward_model).eval().to(LOCAL_RANK)]
@@ -94,6 +120,7 @@ class ScratchpadRewardFn(torch.nn.Module):
         #----------------------------------------------------------------
         # Set Attributes
         #----------------------------------------------------------------
+        self._use_frozen_head              = use_frozen_head
         self._non_distributed_reward_model = non_distributed
         self._show_answer_replacement      = False
         self._get_extra_info_fn            = get_extra_info_fn
@@ -112,7 +139,9 @@ class ScratchpadRewardFn(torch.nn.Module):
         else:
             raise ValueError(os.environ["ACCELERATE_MIXED_PRECISION"])
 
-        self._prep_models()
+        if freeze_model:
+            assert not use_frozen_head, (freeze_model, use_frozen_head)
+            self._prep_models()
 
 
     def _prep_models(self):
@@ -195,10 +224,10 @@ class ScratchpadRewardFn(torch.nn.Module):
         )
 
         assert model is not None, f"model is None. {is_distributed = }"
-        assert not model.training, f"{model.training = }"
-        assert not any( x.requires_grad for x in model.parameters()), (
-            f"{np.mean([x.requires_grad for x in model.parameters()]):0.1%}"
-        )
+        # assert not model.training, f"{model.training = }"
+        # assert not any( x.requires_grad for x in model.parameters()), (
+        #     f"{np.mean([x.requires_grad for x in model.parameters()]):0.1%}"
+        # )
 
         # Get the answers.
         assert (torch.cuda.current_device() == torch.distributed.get_rank()), (
@@ -328,15 +357,19 @@ class ScratchpadRewardFn(torch.nn.Module):
                     assert not model.training, f"{model.training = }"                    
                     if not is_distributed and not accel_state_mixed == "no":
                         if accel_state_mixed == "bf16":
-                            with torch.cuda.amp.autocast(dtype=torch.bfloat16):
-                                logits = model(**input_dict).logits
+                            dtype = torch.bfloat16
                         elif accel_state_mixed == "fp16": 
-                            with torch.cuda.amp.autocast(dtype=torch.float16):
-                                logits = model(**input_dict).logits
+                            dtype = torch.float16
                         else:
                             raise ValueError(f"{accel_state_mixed = }")
                     else:
-                        logits = model(**input_dict).logits
+                        dtype = None
+                    
+                    logits = _maybe_frozen_head(
+                        _maybe_autocast(model, dtype), 
+                        input_dict, 
+                        self._use_frozen_head
+                    ).logits
 
                     if not is_distributed:
                         if accel_state_mixed == "bf16":
diff --git a/with_trlx/link_modeling_base.py b/with_trlx/link_modeling_base.py
new file mode 120000
index 0000000..155b4f0
--- /dev/null
+++ b/with_trlx/link_modeling_base.py
@@ -0,0 +1 @@
+/home/mila/g/gagnonju/trlx/trlx/models/modeling_base.py
\ No newline at end of file
diff --git a/with_trlx/link_modeling_ppo.py b/with_trlx/link_modeling_ppo.py
new file mode 120000
index 0000000..a62f554
--- /dev/null
+++ b/with_trlx/link_modeling_ppo.py
@@ -0,0 +1 @@
+/home/mila/g/gagnonju/trlx/trlx/models/modeling_ppo.py
\ No newline at end of file
