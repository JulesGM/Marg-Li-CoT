{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import rich.table\n",
    "import rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "config = transformers.AutoConfig.from_pretrained(MODEL_NAME)\n",
    "\n",
    "if config.is_encoder_decoder:\n",
    "    cls = transformers.AutoModelForSeq2SeqLM\n",
    "    if not tokenizer.pad_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "else:\n",
    "    cls = transformers.AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_int8_config\n",
      "model_int8\n",
      "model_float32\n",
      "model_bfloat16\n",
      "model_float16\n"
     ]
    }
   ],
   "source": [
    "print(\"model_int8_config\")\n",
    "model_int8_config = cls.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    quantization_config=transformers.BitsAndBytesConfig(load_in_8bit=True),\n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "print(\"model_int8\")\n",
    "model_int8 = cls.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    load_in_8bit=True, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "print(\"model_float32\")\n",
    "model_float32 = cls.from_pretrained(MODEL_NAME).cuda()\n",
    "print(\"model_bfloat16\")\n",
    "model_bfloat16 = cls.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16).cuda()\n",
    "print(\"model_float16\")\n",
    "model_float16 = cls.from_pretrained(MODEL_NAME, torch_dtype=torch.float16).cuda()\n",
    "\n",
    "models_by_name = dict(\n",
    "    model_int8=model_int8,\n",
    "    model_int8_config=model_int8_config,\n",
    "    model_bfloat16=model_bfloat16,\n",
    "    model_float16=model_float16,\n",
    "    model_float32=model_float32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #af00ff; text-decoration-color: #af00ff; font-weight: bold\">Sample text: </span>Question: What is the color of the moon? Answer: \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;5;129mSample text: \u001b[0mQuestion: What is the color of the moon? Answer: \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                Precision Test                </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> </span><span style=\"color: #af00ff; text-decoration-color: #af00ff; font-weight: bold\">Model name</span><span style=\"font-weight: bold\">        </span>┃<span style=\"font-weight: bold\"> </span><span style=\"color: #af00ff; text-decoration-color: #af00ff; font-weight: bold\">Ce</span><span style=\"font-weight: bold\">    </span>┃<span style=\"font-weight: bold\"> </span><span style=\"color: #af00ff; text-decoration-color: #af00ff; font-weight: bold\">Generation</span><span style=\"font-weight: bold\">     </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━┩\n",
       "│ model_int8        │ 3.193 │ &lt;pad&gt; blue&lt;/s&gt; │\n",
       "├───────────────────┼───────┼────────────────┤\n",
       "│ model_int8_config │ 3.193 │ &lt;pad&gt; blue&lt;/s&gt; │\n",
       "├───────────────────┼───────┼────────────────┤\n",
       "│ model_bfloat16    │ 3.219 │ &lt;pad&gt; blue&lt;/s&gt; │\n",
       "├───────────────────┼───────┼────────────────┤\n",
       "│ model_float16     │ 3.221 │ &lt;pad&gt; blue&lt;/s&gt; │\n",
       "├───────────────────┼───────┼────────────────┤\n",
       "│ model_float32     │ 2.906 │ &lt;pad&gt; blue&lt;/s&gt; │\n",
       "└───────────────────┴───────┴────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                Precision Test                \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1;38;5;129mModel name\u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1;38;5;129mCe\u001b[0m\u001b[1m   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1;38;5;129mGeneration\u001b[0m\u001b[1m    \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━┩\n",
       "│ model_int8        │ 3.193 │ <pad> blue</s> │\n",
       "├───────────────────┼───────┼────────────────┤\n",
       "│ model_int8_config │ 3.193 │ <pad> blue</s> │\n",
       "├───────────────────┼───────┼────────────────┤\n",
       "│ model_bfloat16    │ 3.219 │ <pad> blue</s> │\n",
       "├───────────────────┼───────┼────────────────┤\n",
       "│ model_float16     │ 3.221 │ <pad> blue</s> │\n",
       "├───────────────────┼───────┼────────────────┤\n",
       "│ model_float32     │ 2.906 │ <pad> blue</s> │\n",
       "└───────────────────┴───────┴────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "sample_text = \"Question: What is the color of the moon? Answer: \"\n",
    "sample = tokenizer(sample_text, return_tensors=\"pt\").to(0)\n",
    "\n",
    "table = rich.table.Table(\"[purple]Model name\", \"[purple]Ce\", \"[purple]Generation\", title=\"Precision Test\", show_lines=True)\n",
    "\n",
    "rich.print(f\"[purple bold]Sample text: [/]{sample_text}\")\n",
    "for name, model in models_by_name.items():\n",
    "    ce = model(**sample, labels=sample.input_ids).loss\n",
    "    \n",
    "    if model.config.pad_token_id is None:\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "    \n",
    "    gen_ids = model.generate(**sample, max_new_tokens=20)\n",
    "    \n",
    "    if not model.config.is_encoder_decoder:\n",
    "        # The output of a causal model also includes the input ids\n",
    "        # so we need to remove them.\n",
    "        gen_ids = gen_ids[0, sample.input_ids.shape[-1]:]\n",
    "    generation = tokenizer.decode(gen_ids).strip().replace(\"\\n\", \" \")\n",
    "    \n",
    "    table.add_row(name, f\"{ce:0.3f}\", generation)\n",
    "\n",
    "rich.print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
