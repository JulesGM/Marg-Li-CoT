diff --git a/first_approach/README.md b/archive/first_approach/README.md
similarity index 100%
rename from first_approach/README.md
rename to archive/first_approach/README.md
diff --git a/first_approach/STATE_STEP_2.md b/archive/first_approach/STATE_STEP_2.md
similarity index 100%
rename from first_approach/STATE_STEP_2.md
rename to archive/first_approach/STATE_STEP_2.md
diff --git a/first_approach/STATE_STEP_3.md b/archive/first_approach/STATE_STEP_3.md
similarity index 100%
rename from first_approach/STATE_STEP_3.md
rename to archive/first_approach/STATE_STEP_3.md
diff --git a/first_approach/abstract.md b/archive/first_approach/abstract.md
similarity index 100%
rename from first_approach/abstract.md
rename to archive/first_approach/abstract.md
diff --git a/first_approach/bin_refine.py b/archive/first_approach/bin_refine.py
similarity index 100%
rename from first_approach/bin_refine.py
rename to archive/first_approach/bin_refine.py
diff --git a/first_approach/console.py b/archive/first_approach/console.py
similarity index 100%
rename from first_approach/console.py
rename to archive/first_approach/console.py
diff --git a/first_approach/constants.py b/archive/first_approach/constants.py
similarity index 100%
rename from first_approach/constants.py
rename to archive/first_approach/constants.py
diff --git a/first_approach/data_synthetic_arithmetic_tokenizer.py b/archive/first_approach/data_synthetic_arithmetic_tokenizer.py
similarity index 100%
rename from first_approach/data_synthetic_arithmetic_tokenizer.py
rename to archive/first_approach/data_synthetic_arithmetic_tokenizer.py
diff --git a/first_approach/fast_ckpt_reader.py b/archive/first_approach/fast_ckpt_reader.py
similarity index 100%
rename from first_approach/fast_ckpt_reader.py
rename to archive/first_approach/fast_ckpt_reader.py
diff --git a/first_approach/general_shared_constants.py b/archive/first_approach/general_shared_constants.py
similarity index 100%
rename from first_approach/general_shared_constants.py
rename to archive/first_approach/general_shared_constants.py
diff --git a/first_approach/marginal.py b/archive/first_approach/marginal.py
similarity index 100%
rename from first_approach/marginal.py
rename to archive/first_approach/marginal.py
diff --git a/first_approach/mypy.ini b/archive/first_approach/mypy.ini
similarity index 100%
rename from first_approach/mypy.ini
rename to archive/first_approach/mypy.ini
diff --git a/first_approach/ppo.py b/archive/first_approach/ppo.py
similarity index 100%
rename from first_approach/ppo.py
rename to archive/first_approach/ppo.py
diff --git a/first_approach/pretrain.py b/archive/first_approach/pretrain.py
similarity index 100%
rename from first_approach/pretrain.py
rename to archive/first_approach/pretrain.py
diff --git a/first_approach/rl/__init__.py b/archive/first_approach/rl/__init__.py
similarity index 100%
rename from first_approach/rl/__init__.py
rename to archive/first_approach/rl/__init__.py
diff --git a/first_approach/rl/ppo.py b/archive/first_approach/rl/ppo.py
similarity index 100%
rename from first_approach/rl/ppo.py
rename to archive/first_approach/rl/ppo.py
diff --git a/first_approach/rl_baselines.py b/archive/first_approach/rl_baselines.py
similarity index 100%
rename from first_approach/rl_baselines.py
rename to archive/first_approach/rl_baselines.py
diff --git a/first_approach/script_data_arithmetic_parse.py b/archive/first_approach/script_data_arithmetic_parse.py
similarity index 100%
rename from first_approach/script_data_arithmetic_parse.py
rename to archive/first_approach/script_data_arithmetic_parse.py
diff --git a/first_approach/script_data_to_h5.py b/archive/first_approach/script_data_to_h5.py
similarity index 100%
rename from first_approach/script_data_to_h5.py
rename to archive/first_approach/script_data_to_h5.py
diff --git a/first_approach/train_utils.py b/archive/first_approach/train_utils.py
similarity index 100%
rename from first_approach/train_utils.py
rename to archive/first_approach/train_utils.py
diff --git a/first_approach/wandb_config.json b/archive/first_approach/wandb_config.json
similarity index 100%
rename from first_approach/wandb_config.json
rename to archive/first_approach/wandb_config.json
diff --git a/our_scratchpad/archive/ddps.py b/archive/our_scratchpad/archive/ddps.py
similarity index 100%
rename from our_scratchpad/archive/ddps.py
rename to archive/our_scratchpad/archive/ddps.py
diff --git a/our_scratchpad/archive/length_complexity.ipynb b/archive/our_scratchpad/archive/length_complexity.ipynb
similarity index 100%
rename from our_scratchpad/archive/length_complexity.ipynb
rename to archive/our_scratchpad/archive/length_complexity.ipynb
diff --git a/our_scratchpad/archive/nb_beam_search_speed.ipynb b/archive/our_scratchpad/archive/nb_beam_search_speed.ipynb
similarity index 100%
rename from our_scratchpad/archive/nb_beam_search_speed.ipynb
rename to archive/our_scratchpad/archive/nb_beam_search_speed.ipynb
diff --git a/our_scratchpad/archive/nb_ckpt_pl.ipynb b/archive/our_scratchpad/archive/nb_ckpt_pl.ipynb
similarity index 100%
rename from our_scratchpad/archive/nb_ckpt_pl.ipynb
rename to archive/our_scratchpad/archive/nb_ckpt_pl.ipynb
diff --git a/our_scratchpad/archive/nb_data_lengths.ipynb b/archive/our_scratchpad/archive/nb_data_lengths.ipynb
similarity index 100%
rename from our_scratchpad/archive/nb_data_lengths.ipynb
rename to archive/our_scratchpad/archive/nb_data_lengths.ipynb
diff --git a/our_scratchpad/archive/nb_explore_gsm8k.ipynb b/archive/our_scratchpad/archive/nb_explore_gsm8k.ipynb
similarity index 100%
rename from our_scratchpad/archive/nb_explore_gsm8k.ipynb
rename to archive/our_scratchpad/archive/nb_explore_gsm8k.ipynb
diff --git a/our_scratchpad/archive/nb_generate_experimentation.ipynb b/archive/our_scratchpad/archive/nb_generate_experimentation.ipynb
similarity index 100%
rename from our_scratchpad/archive/nb_generate_experimentation.ipynb
rename to archive/our_scratchpad/archive/nb_generate_experimentation.ipynb
diff --git a/our_scratchpad/archive/nb_group_beam_search.ipynb b/archive/our_scratchpad/archive/nb_group_beam_search.ipynb
similarity index 100%
rename from our_scratchpad/archive/nb_group_beam_search.ipynb
rename to archive/our_scratchpad/archive/nb_group_beam_search.ipynb
diff --git a/our_scratchpad/archive/nb_our_scratchpad.ipynb b/archive/our_scratchpad/archive/nb_our_scratchpad.ipynb
similarity index 100%
rename from our_scratchpad/archive/nb_our_scratchpad.ipynb
rename to archive/our_scratchpad/archive/nb_our_scratchpad.ipynb
diff --git a/our_scratchpad/archive/nb_vmap_transformers.ipynb b/archive/our_scratchpad/archive/nb_vmap_transformers.ipynb
similarity index 100%
rename from our_scratchpad/archive/nb_vmap_transformers.ipynb
rename to archive/our_scratchpad/archive/nb_vmap_transformers.ipynb
diff --git a/our_scratchpad/archive/torch_distributed.py b/archive/our_scratchpad/archive/torch_distributed.py
similarity index 100%
rename from our_scratchpad/archive/torch_distributed.py
rename to archive/our_scratchpad/archive/torch_distributed.py
diff --git a/our_scratchpad/archive/unpicklers/better_reader.py b/archive/our_scratchpad/archive/unpicklers/better_reader.py
similarity index 100%
rename from our_scratchpad/archive/unpicklers/better_reader.py
rename to archive/our_scratchpad/archive/unpicklers/better_reader.py
diff --git a/our_scratchpad/archive/unpicklers/reader.py b/archive/our_scratchpad/archive/unpicklers/reader.py
similarity index 100%
rename from our_scratchpad/archive/unpicklers/reader.py
rename to archive/our_scratchpad/archive/unpicklers/reader.py
diff --git a/our_scratchpad/asdiv_data_exploration.ipynb b/archive/our_scratchpad/asdiv_data_exploration.ipynb
similarity index 100%
rename from our_scratchpad/asdiv_data_exploration.ipynb
rename to archive/our_scratchpad/asdiv_data_exploration.ipynb
diff --git a/our_scratchpad/asdiv_dataset.py b/archive/our_scratchpad/asdiv_dataset.py
similarity index 100%
rename from our_scratchpad/asdiv_dataset.py
rename to archive/our_scratchpad/asdiv_dataset.py
diff --git a/our_scratchpad/configs/ppo_config.yml b/archive/our_scratchpad/configs/ppo_config.yml
similarity index 100%
rename from our_scratchpad/configs/ppo_config.yml
rename to archive/our_scratchpad/configs/ppo_config.yml
diff --git a/our_scratchpad/flan_t5_gsm8k.ipynb b/archive/our_scratchpad/flan_t5_gsm8k.ipynb
similarity index 100%
rename from our_scratchpad/flan_t5_gsm8k.ipynb
rename to archive/our_scratchpad/flan_t5_gsm8k.ipynb
diff --git a/our_scratchpad/flan_t5_xxl_results/asdiv_float32_majority_vote.ipynb b/archive/our_scratchpad/flan_t5_xxl_results/asdiv_float32_majority_vote.ipynb
similarity index 100%
rename from our_scratchpad/flan_t5_xxl_results/asdiv_float32_majority_vote.ipynb
rename to archive/our_scratchpad/flan_t5_xxl_results/asdiv_float32_majority_vote.ipynb
diff --git a/our_scratchpad/flan_t5_xxl_results/asdiv_float32_majority_vote_8_contexts.ipynb b/archive/our_scratchpad/flan_t5_xxl_results/asdiv_float32_majority_vote_8_contexts.ipynb
similarity index 100%
rename from our_scratchpad/flan_t5_xxl_results/asdiv_float32_majority_vote_8_contexts.ipynb
rename to archive/our_scratchpad/flan_t5_xxl_results/asdiv_float32_majority_vote_8_contexts.ipynb
diff --git a/our_scratchpad/little_rocket/.gitignore b/archive/our_scratchpad/little_rocket/.gitignore
similarity index 100%
rename from our_scratchpad/little_rocket/.gitignore
rename to archive/our_scratchpad/little_rocket/.gitignore
diff --git a/our_scratchpad/little_rocket/constants.py b/archive/our_scratchpad/little_rocket/constants.py
similarity index 100%
rename from our_scratchpad/little_rocket/constants.py
rename to archive/our_scratchpad/little_rocket/constants.py
diff --git a/our_scratchpad/little_rocket/jupyter_client.ipynb b/archive/our_scratchpad/little_rocket/jupyter_client.ipynb
similarity index 100%
rename from our_scratchpad/little_rocket/jupyter_client.ipynb
rename to archive/our_scratchpad/little_rocket/jupyter_client.ipynb
diff --git a/our_scratchpad/little_rocket/main.py b/archive/our_scratchpad/little_rocket/main.py
similarity index 100%
rename from our_scratchpad/little_rocket/main.py
rename to archive/our_scratchpad/little_rocket/main.py
diff --git a/our_scratchpad/little_rocket/neural_net.py b/archive/our_scratchpad/little_rocket/neural_net.py
similarity index 100%
rename from our_scratchpad/little_rocket/neural_net.py
rename to archive/our_scratchpad/little_rocket/neural_net.py
diff --git a/our_scratchpad/little_rocket/rl.py b/archive/our_scratchpad/little_rocket/rl.py
similarity index 100%
rename from our_scratchpad/little_rocket/rl.py
rename to archive/our_scratchpad/little_rocket/rl.py
diff --git a/our_scratchpad/little_rocket/rocket_baselines.ipynb b/archive/our_scratchpad/little_rocket/rocket_baselines.ipynb
similarity index 100%
rename from our_scratchpad/little_rocket/rocket_baselines.ipynb
rename to archive/our_scratchpad/little_rocket/rocket_baselines.ipynb
diff --git a/our_scratchpad/little_rocket/utils.py b/archive/our_scratchpad/little_rocket/utils.py
similarity index 100%
rename from our_scratchpad/little_rocket/utils.py
rename to archive/our_scratchpad/little_rocket/utils.py
diff --git a/our_scratchpad/other_run_flan_t5_gsm8k.ipynb b/archive/our_scratchpad/other_run_flan_t5_gsm8k.ipynb
similarity index 100%
rename from our_scratchpad/other_run_flan_t5_gsm8k.ipynb
rename to archive/our_scratchpad/other_run_flan_t5_gsm8k.ipynb
diff --git a/our_scratchpad/save_exp.ipynb b/archive/our_scratchpad/save_exp.ipynb
similarity index 100%
rename from our_scratchpad/save_exp.ipynb
rename to archive/our_scratchpad/save_exp.ipynb
diff --git a/our_scratchpad/text2digits_experimentation.ipynb b/archive/our_scratchpad/text2digits_experimentation.ipynb
similarity index 100%
rename from our_scratchpad/text2digits_experimentation.ipynb
rename to archive/our_scratchpad/text2digits_experimentation.ipynb
diff --git a/with_trlx/bin_exp.py b/with_trlx/bin_exp.py
index bee074a..6663fef 100644
--- a/with_trlx/bin_exp.py
+++ b/with_trlx/bin_exp.py
@@ -11,9 +11,13 @@ There are wrappers for GSM8K and for the ASDiv datasets.
 By default, we support the GPT2 model.
 
 """
-
-print("Doing imports.")
 import os
+
+os.environ["TOKENIZERS_PARALLELISM"] = "false"
+os.environ["NCCL_DEBUG"]             = "WARN"
+os.environ["DATASETS_VERBOSITY"]     = "warning"
+os.environ["TRANSFORMERS_VERBOSITY"] = "warning"
+
 import collections
 import contextlib
 import enum
@@ -27,26 +31,45 @@ import datasets
 import fire
 import general_utils
 import numpy as np
+import peft
 import rich
 import rich.logging
+import rich.status
 import torch
 import transformers
 import trlx
 import yaml
 from trlx.data.configs import TRLConfig
+from trlx.models.modeling_ppo import AutoModelForSeq2SeqLMWithHydraValueHead
 
 import lib_data
 import lib_metric
 import lib_reward
-print("Done with imports")
+import lib_modeling
 
 
+datasets    .logging.set_verbosity_warning()
+transformers.logging.set_verbosity_warning()
+logging.getLogger("datasets"    ).setLevel(logging.WARNING)
+logging.getLogger("transformers").setLevel(logging.WARNING)
+logging.getLogger("deepspeed"   ).setLevel(logging.WARNING)
+
+DEFAULT_MODEL_PRECISION = torch.bfloat16
+DEFAULT_INT8_MODE       = True
+DEFAULT_USE_LORA        = True
 DEFAULT_DETERMINISTIC   = False
 DEFAULT_DO_SINGLE_PROC  = False
 DEFAULT_DATASET_TO_USE  = "gsm8k"
-DEFAULT_REWARD_MODEL    = "google/flan-t5-xl"
+DEFAULT_REWARD_MODEL    = "google/flan-t5-xxl"
 DEFAULT_MAIN_MODEL      = DEFAULT_REWARD_MODEL
 DEFAULT_TOKENIZER_MODEL = DEFAULT_REWARD_MODEL
+DEFAULT_PEFT_CONFIG     = dict(
+    r              = 8,
+    lora_alpha     = 32,
+    task_type      = peft.TaskType.SEQ_2_SEQ_LM,
+    lora_dropout   = 0,
+    inference_mode = False,
+)
 
 
 # -----------------------------------------------------------------------------
@@ -70,6 +93,13 @@ assert Path(DEFAULT_PPO_CONFIG_PATH).exists(), (
     f"{DEFAULT_PPO_CONFIG_PATH = }")
 torch.cuda.set_device(LOCAL_RANK)
 
+@contextlib.contextmanager
+def main_status(msg):
+    if os.environ["RANK"] == "0":
+        with rich.status.Status(msg) as status:
+            yield status
+    else:
+        yield
 
 
 class MergedExtraInfo:
@@ -345,6 +375,43 @@ def _setup_config(
     return config
 
 
+def init_model(
+        *, 
+        model_precision, 
+        model_name_or_path, 
+        accelerator_device,
+    ):
+
+    if model_precision in (torch.float16, torch.bfloat16):
+
+        assert model_precision in (
+            torch.bfloat16, torch.float16
+        ), (model_precision)
+        model = transformers.AutoModelForSeq2SeqLM.from_pretrained(
+            model_name_or_path, torch_dtype=model_precision
+        )
+    elif model_precision == "int8":
+        dmap_keys = ["encoder", "lm_head", "shared", "decoder"]
+        dmap = {k: accelerator_device for k in dmap_keys}
+        model = transformers.AutoModelForSeq2SeqLM.from_pretrained(
+            model_name_or_path, 
+            load_in_8bit = True,
+            torch_dtype  = torch.float16, # Required for 8-bit
+            device_map   = dmap,
+        )
+
+    elif model_precision == torch.float32 or model_precision is None:
+        assert False
+        model = transformers.AutoModelForSeq2SeqLM.from_pretrained(
+            model_name_or_path
+        )
+    else:
+        assert False
+        raise ValueError(f"Unknown model precision: {model_precision}")
+    
+    return model
+
+
 def train(
     *,
     reward_model_hf_name_or_path: Optional[str] = DEFAULT_REWARD_MODEL,
@@ -353,16 +420,22 @@ def train(
     model_class_name: str                       = ModelClassChoices.SEQ2SEQ,
     trlx_config_path: Union[Path, str]          = DEFAULT_PPO_CONFIG_PATH,
     val_subset_size: Optional[int]              = None,
+    model_precision: str                        = DEFAULT_MODEL_PRECISION,
     dataset_to_use: str                         = DEFAULT_DATASET_TO_USE,
     do_single_proc: int                         = DEFAULT_DO_SINGLE_PROC,
     deterministic: bool                         = DEFAULT_DETERMINISTIC,
+    peft_config: bool                           = DEFAULT_PEFT_CONFIG,
+    
     log_level: str                              = "INFO",
+    int8_mode: bool                             = DEFAULT_INT8_MODE,
+    use_lora: bool                              = DEFAULT_USE_LORA,
 ):
     # -------------------------------------------------------------------------
     # Logging stuff.
     # -------------------------------------------------------------------------
     args = locals().copy()
     _logging(args=args, log_level=log_level)
+    logging.getLogger("lib_data").setLevel(logging.WARNING)
 
     if RANK == 0:
         rich.print(
@@ -386,15 +459,16 @@ def train(
     # -------------------------------------------------------------------------
     # Dataset
     # -------------------------------------------------------------------------
-    _sanity_check_model_type(model_class_name, reward_model_hf_name_or_path)
-    _sanity_check_model_type(model_class_name, main_model_hf_name_or_path)
-    config_dict = yaml.safe_load(Path(trlx_config_path).read_text())
-    ds_train_obj, ds_eval_obj, reward_tokenizer = _build_dataset(
-        tokenizer_hf_name_or_path  = tokenizer_hf_name_or_path,
-        val_subset_size            = val_subset_size,
-        dataset_to_use             = dataset_to_use,
-        config_dict                = config_dict, 
-    )
+    with main_status("[bold]Building Dataset..."):
+        _sanity_check_model_type(model_class_name, reward_model_hf_name_or_path)
+        _sanity_check_model_type(model_class_name, main_model_hf_name_or_path)
+        config_dict = yaml.safe_load(Path(trlx_config_path).read_text())
+        ds_train_obj, ds_eval_obj, reward_tokenizer = _build_dataset(
+            tokenizer_hf_name_or_path  = tokenizer_hf_name_or_path,
+            val_subset_size            = val_subset_size,
+            dataset_to_use             = dataset_to_use,
+            config_dict                = config_dict, 
+        )
     
     # -------------------------------------------------------------------------
     # Setup Config.
@@ -416,26 +490,53 @@ def train(
     os.environ["TOKENIZERS_PARALLELISM"] = "false"
 
     # -------------------------------------------------------------------------
-    # Metric and Reward.
+    # Model, Metric and Reward.
     # -------------------------------------------------------------------------
     # The metric and the reward need access to the labe.
     # -------------------------------------------------------------------------
     # The metric needs 
     merger = MergedExtraInfo(
         ds_train_obj = ds_train_obj,
-        ds_eval_obj  = ds_eval_obj,
-    )
-    
+        ds_eval_obj  = ds_eval_obj ,)
     metric_accuracy = lib_metric.ScratchpadAnswerAccuracy(
-        extra_info_engine = merger.merged_get_extra_info,
-    )
+        extra_info_engine = merger.merged_get_extra_info)
+    assert main_model_hf_name_or_path == reward_model_hf_name_or_path, (
+        f"For now, the main and reward model must be the same. "
+        f"{main_model_hf_name_or_path} "
+        f"{reward_model_hf_name_or_path}")
+
+    with main_status("[bold]Loading interior model..."):
+        interior_model = init_model(
+            model_precision    = model_precision,
+            accelerator_device = int(os.environ["LOCAL_RANK"]),
+            model_name_or_path = main_model_hf_name_or_path,
+        )
+        assert int8_mode
+
+    with main_status("[bold]Hydra Value Head..."):
+        if use_lora:
+            model = lib_modeling.AutoModelForSeq2SeqLMWithHydraValueHead(
+                interior_model,
+                peft.LoraConfig(**peft_config),
+                int(os.environ["LOCAL_RANK"]),
+            )
+            reward_model_hf_name_or_path = interior_model
+
+        else:
+            assert False
+            model = AutoModelForSeq2SeqLMWithHydraValueHead(
+                interior_model,
+                num_layers_unfrozen=1,
+            )
 
     # Afaik the eval should not need the extra info engine.
     scratchpad_reward_fn = lib_reward.ScratchpadRewardFn(
         reward_model_hf_name_or_path = reward_model_hf_name_or_path,
         get_extra_info_fn            = merger.merged_get_extra_info,
         reward_tokenizer             = reward_tokenizer,
+        use_frozen_head              = True,
         do_single_proc               = do_single_proc,
+        freeze_model                 = False,
         metric_fn                    = metric_accuracy,
     )
 
@@ -444,7 +545,7 @@ def train(
     # -------------------------------------------------------------------------
     model = trlx.train(
         eval_prompts  = list(ds_eval_obj),
-        model_path    = main_model_hf_name_or_path,
+        model_path    = model,
         metric_fn     = metric_accuracy,
         reward_fn     = scratchpad_reward_fn,
         prompts       = list(ds_train_obj),
@@ -453,4 +554,5 @@ def train(
 
 
 if __name__ == "__main__":
-    fire.Fire(train)
+    with torch.autograd.detect_anomaly():
+        fire.Fire(train)
diff --git a/with_trlx/bin_launch.py b/with_trlx/bin_launch.py
new file mode 100755
index 0000000..5376568
--- /dev/null
+++ b/with_trlx/bin_launch.py
@@ -0,0 +1,186 @@
+#!/usr/bin/env python
+import os
+import itertools
+import shlex
+import sys
+import fire
+import rich
+import subprocess
+import pretty_traceback
+pretty_traceback.install()
+
+
+DEFAULT_ONE                     = False
+DEFAULT_SERVER_PORT             = 29505
+DEFAULT_VAL_SUBSET_SIZE         = 3
+DEFAULT_ACCELERATE_CONFIG       = "accelerate_ddp_no.yaml"
+DEFAULT_MIXED_PRECISION_DEFAULT = "no"
+
+
+def _check_mixed_precision_compatibility(default):
+    """
+    If any of the GPUs is not an A100, disable bf16.
+    """
+
+    # Extract the GPU models
+    gpu_models = subprocess.check_output(
+        "nvidia-smi --query-gpu=name --format=csv,noheader | sort | uniq", 
+        shell=True, universal_newlines=True
+    ).strip().split("\n")
+
+    # If any of the GPUs is not a A100, disable bf16
+    for model in gpu_models:
+        if "a100" not in model:
+            rich.print(f"[bold blue]Disabling bf16 because of GPU model: {model}")
+            return "no"
+
+    return default
+
+def _kill_wandb_servers():
+    subprocess.call(
+        "pgrep wandb | xargs kill -9",
+        shell=True, 
+        # stdout=subprocess.STDOUT, 
+        stderr=subprocess.DEVNULL,
+        universal_newlines=True,
+    )
+
+def _kill_other_python_processes():
+    subprocess.call(
+        f"pgrep python | grep -v {os.getpid()} | xargs kill -9",
+        shell=True,
+        # stdout=subprocess.STDOUT,
+        stderr=subprocess.DEVNULL,
+        universal_newlines=True,
+    )
+
+def dict_to_command_list(d):
+    return itertools.chain.from_iterable(
+        [[f"--{k}", v] for k, v in d.items()]
+    )
+
+def _build_command(
+        *,
+        bin_path, 
+        script_config,
+        accelerate_bin, 
+        accelerate_config, 
+        accel_config,
+    ):
+    if not os.path.exists(bin_path):
+        raise RuntimeError(f"Expected bin path to exist: {bin_path}")
+    
+    command = accelerate_bin + ["launch"]                   # Adds srun if multi node
+    command.extend(dict_to_command_list(accelerate_config)) # Adds nodes & procs info
+    command.extend(["--config_file", accel_config]) # Adds specific args
+    command.append(bin_path)                                # Adds script path 
+    command.extend(dict_to_command_list(script_config))     # Adds script args
+    command = [str(c) for c in command]
+
+    return command
+
+def _build_accelerate_config(one, server_port, mixed_precision):
+
+    accelerate_path = subprocess.check_output(
+        "which accelerate",
+        shell              = True,
+        universal_newlines = True,
+    ).strip()
+
+    srun_path = subprocess.check_output(
+        "which srun",
+        shell              = True, 
+        universal_newlines = True,
+    ).strip()
+
+    total_processes = (
+        int(os.environ["SLURM_NNODES"      ]) * 
+        int(os.environ["SLURM_GPUS_ON_NODE"])
+    )
+    server_hostname = os.environ["SLURMD_NODENAME"]
+    num_nodes       = int(os.environ["SLURM_NNODES"])
+
+    conditional_args_single_node = {}
+    conditional_args_multi_node  = {"deepspeed_multinode_launcher": "standard"}
+    accelerate_bin_single_node   = [accelerate_path]
+    accelerate_bin_multi_node    = [srun_path, accelerate_path]
+
+    if num_nodes > 1 and not one:
+        accelerate_bin              = accelerate_bin_multi_node
+        accelerate_conditional_args = conditional_args_multi_node
+    else:
+        accelerate_bin              = accelerate_bin_single_node
+        accelerate_conditional_args = conditional_args_single_node
+
+    if one:
+        rich.print("[bold yellow]Only using one process, to debug.")
+        num_nodes        = 1
+        accelerate_bin   = accelerate_bin_single_node
+        total_processes  = 1
+        server_hostname  = ""
+        accelerate_conditional_args = conditional_args_single_node
+
+    accelerate_config = {
+        "main_process_port": server_port,
+        "mixed_precision":   mixed_precision,
+        "main_process_ip":   server_hostname,
+        "num_processes":     total_processes,
+        "num_machines":      num_nodes,
+    }
+
+    accelerate_config = dict(
+        **accelerate_config, 
+        **accelerate_conditional_args,
+    )
+    
+    return accelerate_bin, accelerate_config
+
+
+def main(
+        one                     = DEFAULT_ONE,
+        bin_path                = f"{os.getcwd()}/bin_exp.py",
+        server_port             = DEFAULT_SERVER_PORT,
+        val_subset_size         = DEFAULT_VAL_SUBSET_SIZE, 
+        default_accel_config    = DEFAULT_ACCELERATE_CONFIG,
+        mixed_precision_default = DEFAULT_MIXED_PRECISION_DEFAULT,
+    ):
+
+    # Check mixed precision compatibility
+    mixed_precision = _check_mixed_precision_compatibility(
+        mixed_precision_default)
+    del mixed_precision_default
+    
+    # Build accelerate_bin and accelerate_config
+    accelerate_bin, accelerate_config = _build_accelerate_config(
+        one=one, server_port=server_port, mixed_precision=mixed_precision)
+    del one, server_port, mixed_precision
+    
+    # Build script_config
+    if val_subset_size is not None:
+        rich.print(f"[bold red]>>> Using a subset! Of size: {val_subset_size}")
+    script_config = {"val_subset_size": val_subset_size}
+    del val_subset_size
+
+    # Kill all wandb servers
+    _kill_wandb_servers()
+    
+    # Kill all python processes
+    _kill_other_python_processes()
+
+    # Run the training
+    command = _build_command(
+        bin_path             = bin_path,
+        script_config        = script_config,
+        accelerate_bin       = accelerate_bin,
+        accelerate_config    = accelerate_config, 
+        accel_config         = default_accel_config,
+    )
+    del bin_path, accelerate_bin, accelerate_config, script_config
+
+    rich.print("[bold green]>>> Running command:")
+    rich.print(f"[bold]{command}")
+    os.execv(command[0], command)
+
+
+if __name__ == "__main__":
+    fire.Fire(main)
\ No newline at end of file
diff --git a/with_trlx/bin_rl4lms_experimentation.py b/with_trlx/bin_rl4lms_experimentation.py
new file mode 100755
index 0000000..365d544
--- /dev/null
+++ b/with_trlx/bin_rl4lms_experimentation.py
@@ -0,0 +1,138 @@
+#!/usr/bin/env python
+# coding: utf-8
+
+import datetime
+import enum
+import json
+import logging
+import os
+import pickle
+import re
+import sys
+import types
+from pathlib import Path
+from typing import *
+
+import datasets
+import general_utils
+import general_utils as utils
+import numpy as np
+import pretty_traceback
+import rich
+import rich.console
+import torch
+import transformers
+import yaml
+from text2digits import text2digits
+from tqdm import tqdm
+
+import libs_compute_accuracy.dataset_asdiv as dataset_asdiv
+import libs_compute_accuracy.dataset_gsm8k as dataset_gsm8k
+
+pretty_traceback.install()
+datasets.logging.set_verbosity_error()
+transformers.logging.set_verbosity_error()
+
+CONSOLE = rich.console.Console(width=80)
+LOGGER = logging.getLogger(__name__)
+
+class DatasetChoices(str, enum.Enum):
+    gsm8k = "gsm8k"
+    gsm8k_silver = "gsm8k_silver"
+    asdiv = "asdiv"
+
+
+MODEL_PARALLEL = True
+MODEL_HF_NAME = "google/flan-t5-xxl"
+TRAIN_BATCH_SIZE = 1
+EVAL_BATCH_SIZE = 3
+LOG_LEVEL = logging.WARNING
+
+
+###############################################################################
+# Doesn't change
+###############################################################################
+DATASET_CHOICE = DatasetChoices.gsm8k
+MAX_PROMPT_LENGTH = 107
+MAX_EPISODE_LENGTH = 200
+torch.backends.cuda.matmul.allow_tf32 = True
+GENERATION_KWARGS = {
+    "max_new_tokens": MAX_EPISODE_LENGTH,
+    "min_length": 5,
+    "do_sample": True,
+    "top_k": 50,
+}
+
+###############################################################################
+#
+###############################################################################
+
+
+def main(
+    precision = "int8",
+    hf_model_name = MODEL_HF_NAME,
+):
+
+    local_rank = int(os.getenv("LOCAL_RANK", "0"))
+    global_rank = int(os.getenv("RANK", "0"))
+    world_size = int(os.getenv("WORLD_SIZE", "1"))
+
+    logging.basicConfig(
+        level=LOG_LEVEL,
+        format=(
+            f"[{local_rank + 1} / {world_size}]"
+            f"[bold]\[%(name)s]:[/]  %(message)s"
+        ),
+        datefmt="[%X]",
+        handlers=[rich.logging.RichHandler(markup=True, rich_tracebacks=True)],
+    )
+    
+    general_utils.check_contained(precision, ["int8", "fp16", "bf16", "fp32", None])
+
+    if precision == "int8":
+        accelerator_device = os.environ["LOCAL_RANK"]
+        dmap_keys = ["encoder", "lm_head", "shared", "decoder"]
+        device_map = {k: accelerator_device for k in dmap_keys}
+        model_inst = transformers.AutoModelForSeq2SeqLM.from_pretrained(
+            hf_model_name, 
+            device_map=device_map,
+            load_in_8bits=True,
+            torch_dtype=torch.bfloat16, 
+        )
+    elif precision == "fp16":
+        model_inst = transformers.AutoModelForSeq2SeqLM.from_pretrained(
+            hf_model_name, torch_dtype=torch.float16
+        )
+    elif precision == "bf16":
+        model_inst = transformers.AutoModelForSeq2SeqLM.from_pretrained(
+            hf_model_name, torch_dtype=torch.bfloat16
+        )
+    
+    model_tok = transformers.AutoTokenizer.from_pretrained(hf_model_name)
+
+    gsm8k_config = {
+        "args": {"max_sum_squares": 41957, "tokenizer": model_tok},
+        "id": "zero_shot_gsm8k_text_gen_pool",
+    }
+
+    asdiv_config = {
+        "args": {},
+        "id": "zero_shot_asdiv_text_gen_pool",
+    }
+
+    trainer = transformers.Trainer(
+        model=model_inst,
+        train_dataset=dataset_gsm8k.SupervisedGSM8K.prepare(
+            "train", 
+            hf_model_name,
+        ),
+        eval_dataset=
+    )
+
+    # transformers.logging.set_verbosity_error()
+    # datasets.logging.set_verbosity_error()
+    
+
+
+if __name__ == "__main__":
+    main()
diff --git a/with_trlx/config_ppo.yml b/with_trlx/config_ppo.yml
index 8c61dd8..d8917ed 100644
--- a/with_trlx/config_ppo.yml
+++ b/with_trlx/config_ppo.yml
@@ -12,7 +12,7 @@ train:
   trainer:                  "AcceleratePPOTrainer"
 
 model:
-  num_layers_unfrozen: 2
+  num_layers_unfrozen: 1
   model_arch_type:     "seq2seq"
 
 tokenizer:
@@ -57,4 +57,4 @@ method:
   gen_kwargs:
     synced_gpus:    True
     max_new_tokens: 120
-    do_sample:      True
\ No newline at end of file
+    do_sample:      False
\ No newline at end of file
diff --git a/with_trlx/lib_modeling.py b/with_trlx/lib_modeling.py
new file mode 100644
index 0000000..d340121
--- /dev/null
+++ b/with_trlx/lib_modeling.py
@@ -0,0 +1,81 @@
+
+import gc
+import inspect
+from copy import deepcopy
+from dataclasses import dataclass
+from typing import List, Optional, Tuple, Union
+
+import numpy as np
+import peft
+import torch
+import torch.nn as nn
+import transformers
+from torchtyping import TensorType
+from transformers.modeling_outputs import ModelOutput
+from transformers.models.bloom import modeling_bloom
+from transformers.models.opt import modeling_opt
+
+from trlx.models.modeling_base import PreTrainedModelWrapper
+from trlx.utils.modeling import (
+    hf_get_hidden_size,
+    make_head,
+)
+from trlx.models.modeling_ppo import (
+    AutoModelForSeq2SeqLMWithValueHead,
+    Seq2SeqLMOutputWithValue,
+    PreTrainedModelWrapper
+)
+
+
+class AutoModelForSeq2SeqLMWithHydraValueHead(PreTrainedModelWrapper):
+    _supported_modules = ["v_head", "frozen_head"]
+    _supported_args    = ["num_layers_unfrozen"]
+
+    def __init__(
+        self,
+        base_model: transformers.PreTrainedModel,
+        peft_config,
+        device
+    ):
+        super().__init__(base_model)
+        self.v_head = make_head(hf_get_hidden_size(self.base_model.config), 1)
+        
+        self.peft_config = peft_config
+        
+        for parameter in self.base_model.parameters():
+            parameter.requires_grad = False
+        self.frozen_head = base_model
+
+        self.base_model = peft.get_peft_model(base_model, peft_config).bfloat16()
+        type(self.base_model).print_trainable_parameters(self.base_model)
+        
+        self.v_head.to(self.base_model.dtype).to(device)
+    
+    def generate(self, *args, **kwargs) -> Union[ModelOutput, torch.LongTensor]:
+        return self.base_model.generate(*args, **kwargs)
+
+    def forward_hydra(self, **forward_kwargs):
+
+        return_dict = forward_kwargs.get("return_dict", True)
+        
+        forward_kwargs["output_hidden_states"] = True
+        forward_kwargs["return_dict"]          = True
+
+        forward_kwargs["output_attentions"]    = False
+        forward_kwargs["use_cache"]            = False
+
+        hydra_outputs = self.frozen_head(**forward_kwargs)
+        if not return_dict:
+            return hydra_outputs.logits
+        return hydra_outputs
+    
+
+    def forward(self, **forward_kwargs):
+        forward_kwargs["output_hidden_states"] = True
+        forward_kwargs["return_dict"         ] = True
+
+        outputs           = self.base_model(**forward_kwargs)
+        last_hidden_state = outputs.decoder_hidden_states[-1]
+        value             = self.v_head(last_hidden_state).squeeze(-1)
+
+        return Seq2SeqLMOutputWithValue(**outputs, value=value)
\ No newline at end of file
diff --git a/with_trlx/lib_reward.py b/with_trlx/lib_reward.py
index b5c4d83..ab519c7 100644
--- a/with_trlx/lib_reward.py
+++ b/with_trlx/lib_reward.py
@@ -55,6 +55,18 @@ def remove_special_token_ids(
     return filtered_input_ids
 
 
+def _maybe_frozen_head(model, input_dict, use_frozen_head):
+    if use_frozen_head:
+        with torch.no_grad():
+            return model(**input_dict)
+    return model(**input_dict)
+
+def _maybe_autocast(model_or_fn, dtype):
+    if dtype is None:
+        return model_or_fn
+    return torch.cuda.amp.autocast(dtype=dtype)(model_or_fn)
+
+
 def clone_hf_model(
         hf_model: transformers.PreTrainedModel
 ) -> transformers.PreTrainedModel:
@@ -65,10 +77,10 @@ def clone_hf_model(
     return copy
 
 
-class ScratchpadRewardFn(torch.nn.Module):
+class ScratchpadRewardFn:
     def __init__(
         self, *, 
-        reward_model_hf_name_or_path: str,
+        reward_model_hf_name_or_path: typing.Union[str, transformers.PreTrainedModel],
         get_extra_info_fn: typing.Callable[
             [typing.Union[str, typing.List[str]]], 
             typing.Union[typing.Dict, typing.List[typing.Dict]]
@@ -76,15 +88,29 @@ class ScratchpadRewardFn(torch.nn.Module):
         reward_tokenizer: transformers.PreTrainedTokenizerBase, 
         do_single_proc: bool,
         metric_fn,
+        freeze_model,
+        use_frozen_head,
     ):
         super().__init__()
-
+        
+        assert isinstance(freeze_model, bool), type(freeze_model)
+        assert isinstance(use_frozen_head, bool), type(use_frozen_head)
+        assert freeze_model ^ use_frozen_head, (
+            f"{freeze_model = }, {use_frozen_head = }")
+        
         #----------------------------------------------------------------
         # Build Models
         #----------------------------------------------------------------
-        reward_model = transformers.AutoModelForSeq2SeqLM.from_pretrained(
-            reward_model_hf_name_or_path)
-        
+        if isinstance(reward_model_hf_name_or_path, str):
+            assert False
+            reward_model = transformers.AutoModelForSeq2SeqLM.from_pretrained(
+                reward_model_hf_name_or_path)
+        else:
+            assert isinstance(reward_model_hf_name_or_path, transformers.PreTrainedModel), (
+                type(reward_model_hf_name_or_path).mro()
+            )
+            reward_model = reward_model_hf_name_or_path
+
         if do_single_proc:
             non_distributed = [clone_hf_model(
                 reward_model).eval().to(LOCAL_RANK)]
@@ -94,6 +120,7 @@ class ScratchpadRewardFn(torch.nn.Module):
         #----------------------------------------------------------------
         # Set Attributes
         #----------------------------------------------------------------
+        self._use_frozen_head              = use_frozen_head
         self._non_distributed_reward_model = non_distributed
         self._show_answer_replacement      = False
         self._get_extra_info_fn            = get_extra_info_fn
@@ -112,7 +139,9 @@ class ScratchpadRewardFn(torch.nn.Module):
         else:
             raise ValueError(os.environ["ACCELERATE_MIXED_PRECISION"])
 
-        self._prep_models()
+        if freeze_model:
+            assert not use_frozen_head, (freeze_model, use_frozen_head)
+            self._prep_models()
 
 
     def _prep_models(self):
@@ -195,10 +224,10 @@ class ScratchpadRewardFn(torch.nn.Module):
         )
 
         assert model is not None, f"model is None. {is_distributed = }"
-        assert not model.training, f"{model.training = }"
-        assert not any( x.requires_grad for x in model.parameters()), (
-            f"{np.mean([x.requires_grad for x in model.parameters()]):0.1%}"
-        )
+        # assert not model.training, f"{model.training = }"
+        # assert not any( x.requires_grad for x in model.parameters()), (
+        #     f"{np.mean([x.requires_grad for x in model.parameters()]):0.1%}"
+        # )
 
         # Get the answers.
         assert (torch.cuda.current_device() == torch.distributed.get_rank()), (
@@ -328,15 +357,19 @@ class ScratchpadRewardFn(torch.nn.Module):
                     assert not model.training, f"{model.training = }"                    
                     if not is_distributed and not accel_state_mixed == "no":
                         if accel_state_mixed == "bf16":
-                            with torch.cuda.amp.autocast(dtype=torch.bfloat16):
-                                logits = model(**input_dict).logits
+                            dtype = torch.bfloat16
                         elif accel_state_mixed == "fp16": 
-                            with torch.cuda.amp.autocast(dtype=torch.float16):
-                                logits = model(**input_dict).logits
+                            dtype = torch.float16
                         else:
                             raise ValueError(f"{accel_state_mixed = }")
                     else:
-                        logits = model(**input_dict).logits
+                        dtype = None
+                    
+                    logits = _maybe_frozen_head(
+                        _maybe_autocast(model, dtype), 
+                        input_dict, 
+                        self._use_frozen_head
+                    ).logits
 
                     if not is_distributed:
                         if accel_state_mixed == "bf16":
diff --git a/with_trlx/link_modeling_base.py b/with_trlx/link_modeling_base.py
new file mode 120000
index 0000000..155b4f0
--- /dev/null
+++ b/with_trlx/link_modeling_base.py
@@ -0,0 +1 @@
+/home/mila/g/gagnonju/trlx/trlx/models/modeling_base.py
\ No newline at end of file
diff --git a/with_trlx/link_modeling_ppo.py b/with_trlx/link_modeling_ppo.py
new file mode 120000
index 0000000..a62f554
--- /dev/null
+++ b/with_trlx/link_modeling_ppo.py
@@ -0,0 +1 @@
+/home/mila/g/gagnonju/trlx/trlx/models/modeling_ppo.py
\ No newline at end of file
