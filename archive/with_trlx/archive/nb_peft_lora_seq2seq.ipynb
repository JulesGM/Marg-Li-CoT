{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19be6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f93b7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /cvmfs/ai.mila.quebec/apps/arch/common/cuda/11.7/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/mila/g/gagnonju/.main/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "import bitsandbytes\n",
    "import datasets\n",
    "import peft\n",
    "import rich\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "device = 0\n",
    "model_name_or_path     = \"google/flan-t5-xxl\"\n",
    "tokenizer_name_or_path = \"google/flan-t5-xxl\"\n",
    "\n",
    "text_column      = \"sentence\"\n",
    "label_column     = \"text_label\"\n",
    "max_length       = 200\n",
    "lr               = 1e-3\n",
    "num_epochs       = 3\n",
    "train_batch_size = 1\n",
    "eval_batch_size  = 16\n",
    "\n",
    "peft_config = peft.LoraConfig(\n",
    "    lora_alpha     = 32, \n",
    "    r              = 8, \n",
    "    inference_mode = False, \n",
    "    lora_dropout   = 0.1,\n",
    "    task_type      = peft.TaskType.SEQ_2_SEQ_LM, \n",
    ")\n",
    "\n",
    "print_eval_every_x_step = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cc5a1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_acc(preds, split, dataset):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    incorrect = collections.Counter()\n",
    "    for pred, true in zip(preds, dataset[split][\"text_label\"]):\n",
    "        if pred.strip() == true.strip():\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect.update([pred])\n",
    "        total += 1\n",
    "    accuracy = correct / total\n",
    "    rich.print(\n",
    "        f\"{accuracy           = :0.2%} on the evaluation dataset\\n\"\n",
    "        f\"{preds[:10]         = }\\n\"\n",
    "        f\"{dataset[split]['text_label'][:10] = }\\n\"\n",
    "        f\"{incorrect.most_common(10) = }\"\n",
    "    )\n",
    "\n",
    "def eval_epoch(*, fn_model, tokenizer, eval_dataloader):\n",
    "    prev_state = fn_model.training\n",
    "    fn_model.eval()\n",
    "\n",
    "    eval_loss = 0\n",
    "    eval_preds = []\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader, desc=\"Evaluating\")):\n",
    "        fn_model = fn_model.eval()\n",
    "        batch      = {k: v.to(fn_model.device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = fn_model(**batch)\n",
    "        loss       = outputs.loss\n",
    "        eval_loss += loss.detach().float()\n",
    "        eval_preds.extend(\n",
    "            tokenizer.batch_decode(\n",
    "                torch.argmax(outputs.logits, -1).detach().cpu().numpy(), \n",
    "                skip_special_tokens=True,\n",
    "            ))\n",
    "\n",
    "    eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
    "    eval_ppl = torch.exp(eval_epoch_loss)\n",
    "    \n",
    "    if prev_state:\n",
    "        fn_model.train()\n",
    "    return eval_epoch_loss.item(), eval_ppl.item(), eval_preds\n",
    "\n",
    "def train_epoch(\n",
    "        *,\n",
    "        epoch, \n",
    "        fn_model, \n",
    "        optimizer, \n",
    "        tokenizer,\n",
    "        eval_every, \n",
    "        lr_scheduler, \n",
    "        eval_dataloader, \n",
    "        train_dataloader, \n",
    "):\n",
    "\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
    "        fn_model = fn_model.train()\n",
    "        batch       = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs     = fn_model(**batch)\n",
    "        loss        = outputs.loss\n",
    "        total_loss += loss.detach().float()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (\n",
    "            eval_dataloader is not None and \n",
    "            step % eval_every == 0 and \n",
    "            step > 0\n",
    "        ):\n",
    "            eval_epoch_loss, eval_ppl, eval_preds = eval_epoch(\n",
    "                fn_model        = fn_model, \n",
    "                tokenizer       = tokenizer, \n",
    "                eval_dataloader = eval_dataloader,\n",
    "            )\n",
    "            rich.print(\n",
    "                f\"[bold green]{epoch} - {step}:[/] \"\n",
    "                f\"{eval_ppl        = :0.3} \"\n",
    "                f\"{eval_epoch_loss = :0.3}\"\n",
    "            )\n",
    "            calc_acc(eval_preds, \"validation\", eval_dataloader)\n",
    "\n",
    "    return total_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7885d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=torch.bfloat16 with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0073528289794921875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050decdd2c4c4c9fa08e4d496b6f5168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 11135332352 || trainable%: 0.0\n"
     ]
    }
   ],
   "source": [
    "dmap_keys = [\"encoder\", \"lm_head\", \"shared\", \"decoder\"]\n",
    "dmap = {k: os.environ[\"LOCAL_RANK\"] for k in dmap_keys}\n",
    "\n",
    "frozen_model = transformers.T5ForConditionalGeneration.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map   = dmap,\n",
    "    torch_dtype  = torch.bfloat16,\n",
    "    load_in_8bit = True,\n",
    ")\n",
    "\n",
    "for name, param in frozen_model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "peft.PeftModel.print_trainable_parameters(frozen_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d0850ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 11135332352 || trainable%: 0.0\n",
      "trainable params: 9437184 || all params: 11144769536 || trainable%: 0.08467814403443578\n"
     ]
    }
   ],
   "source": [
    "for name, param in frozen_model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "peft.PeftModel.print_trainable_parameters(frozen_model)\n",
    "\n",
    "model = peft.get_peft_model(frozen_model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ee2babf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset financial_phrasebank (/home/mila/g/gagnonju/.cache/huggingface/datasets/financial_phrasebank/sentences_allagree/1.0.0/550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0041849613189697266,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21350ba7adc4a94872dd2db4492d221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004048585891723633,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 2037,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d56fbea5995458ba147879f0401dcdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2037 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003960132598876953,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 227,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df1346b4b6a4c10ac6111a79f01a6b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/227 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'sentence': 'The cranes would be installed onboard two freighters ordered by Singaporean ship owner Masterbulk .',\n",
       " 'label': 1,\n",
       " 'text_label': 'neutral'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading dataset\n",
    "dataset = datasets.load_dataset(\"financial_phrasebank\", \"sentences_allagree\")\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "dataset[\"validation\"] = dataset[\"test\"]\n",
    "del dataset[\"test\"]\n",
    "\n",
    "classes = dataset[\"train\"].features[\"label\"].names\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"text_label\": [classes[label] for label in x[\"label\"]]},\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    ")\n",
    "\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adf9608c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004497528076171875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Running tokenizer on dataset",
       "rate": null,
       "total": 2037,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673804b732624303bcdf69a2ce35f30e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2037 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0041942596435546875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Running tokenizer on dataset",
       "rate": null,
       "total": 227,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14010f235a2e4df98c7cf55bbd6524ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/227 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data preprocessing\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs       = examples[text_column]\n",
    "    targets      = examples[label_column]\n",
    "    prompt = \"Answer if the sentiment of the following sentence is positive, negative or neutral: \"\n",
    "    inputs = [prompt + x for x in inputs]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length     = max_length, \n",
    "        padding        = True, \n",
    "        truncation     = True,\n",
    "        return_tensors = \"pt\",\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        targets, \n",
    "        max_length     = 3, \n",
    "        padding        = True, \n",
    "        truncation     = True,\n",
    "        return_tensors = \"pt\",\n",
    "    )\n",
    "    labels = labels[\"input_ids\"]\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "processed_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    desc     = \"Running tokenizer on dataset\",\n",
    "    batched  = True,\n",
    "    num_proc = 1,\n",
    "    remove_columns       = dataset[\"train\"].column_names,\n",
    "    load_from_cache_file = False,\n",
    ")\n",
    "\n",
    "train_dataset = processed_datasets[\"train\"]\n",
    "eval_dataset  = processed_datasets[\"validation\"]\n",
    "collator = transformers.DataCollatorForSeq2Seq(\n",
    "    tokenizer, \n",
    "    model=model, \n",
    "    padding=True, \n",
    "    max_length=max_length, \n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    shuffle    = True, \n",
    "    collate_fn = collator,\n",
    "    batch_size = train_batch_size, \n",
    "    pin_memory = True,\n",
    ")\n",
    "eval_dataloader = torch.utils.data.DataLoader(\n",
    "    eval_dataset, \n",
    "    shuffle    = False,\n",
    "    collate_fn = collator, \n",
    "    batch_size = eval_batch_size, \n",
    "    pin_memory = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f733a3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer and lr scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b3a4090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭──────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_3872830/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2569830344.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_3872830/2569830344.py'</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰───────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">eval_epoch</span><span style=\"font-weight: bold\">()</span> takes <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> positional arguments but <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> were given\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m─────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m ───────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_3872830/\u001b[0m\u001b[1;33m2569830344.py\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<module>\u001b[0m                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_3872830/2569830344.py'\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰───────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0m\u001b[1;35meval_epoch\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m takes \u001b[1;36m0\u001b[0m positional arguments but \u001b[1;36m3\u001b[0m were given\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_epoch_loss, eval_ppl, eval_preds = eval_epoch(frozen_model, tokenizer, eval_dataloader)\n",
    "rich.print(f\"[bold blue]Zero shot frozen:[/] epoch = -1: {eval_ppl = :0.3} {eval_epoch_loss = :0.3}\")\n",
    "calc_acc(eval_preds, \"validation\", dataset)\n",
    "\n",
    "eval_epoch_loss, eval_ppl, eval_preds = eval_epoch(model, tokenizer, eval_dataloader)\n",
    "rich.print(f\"[bold green]Peft zero-shot:[/] {eval_ppl = :0.3} {eval_epoch_loss = :0.3}\")\n",
    "calc_acc(eval_preds, \"validation\", dataset)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = train_epoch(\n",
    "        epoch            = epoch, \n",
    "        fn_model         = model, \n",
    "        tokenizer        = tokenizer,\n",
    "        optimizer        = optimizer, \n",
    "        eval_every       = print_eval_every_x_step,\n",
    "        lr_scheduler     = lr_scheduler,\n",
    "        eval_dataloader  = eval_dataloader,\n",
    "        train_dataloader = train_dataloader, \n",
    "    )\n",
    "    eval_epoch_loss, eval_ppl, eval_preds = eval_epoch(model, tokenizer, eval_dataloader)\n",
    "\n",
    "    train_epoch_loss = total_loss / len(train_dataloader)\n",
    "    train_ppl = torch.exp(torch.tensor(train_epoch_loss))\n",
    "    rich.print(\n",
    "        f\"[bold blue]{epoch = }:[/] \"\n",
    "        f\"{train_ppl        = :0.3} \"\n",
    "        f\"{train_epoch_loss = :0.3} \"\n",
    "        f\"{eval_ppl         = :0.3} \"\n",
    "        f\"{eval_epoch_loss  = :0.3}\")\n",
    "    calc_acc(eval_preds, \"validation\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c91d3ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭──────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_3872830/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">3613271876.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_3872830/3613271876.py'</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰───────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">eval_epoch</span><span style=\"font-weight: bold\">()</span> takes <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> positional arguments but <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> were given\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m─────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m ───────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_3872830/\u001b[0m\u001b[1;33m3613271876.py\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<module>\u001b[0m                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_3872830/3613271876.py'\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰───────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0m\u001b[1;35meval_epoch\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m takes \u001b[1;36m0\u001b[0m positional arguments but \u001b[1;36m3\u001b[0m were given\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_epoch_loss, eval_ppl, eval_preds = eval_epoch(model, tokenizer, eval_dataloader)\n",
    "rich.print(f\"[bold green]Peft zero-shot:[/] {eval_ppl = :0.3} {eval_epoch_loss = :0.3}\")\n",
    "calc_acc(eval_preds, \"validation\", dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8de6005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model\n",
    "peft_model_id = f\"{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\"\n",
    "model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd20cd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37M\tgoogle/flan-t5-xxl_LORA_SEQ_2_SEQ_LM/adapter_model.bin\n"
     ]
    }
   ],
   "source": [
    "ckpt = f\"{peft_model_id}/adapter_model.bin\"\n",
    "!du -h $ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c2fc29",
   "metadata": {},
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_id = f\"{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model  = transformers.AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\n",
    "model  = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d712ce",
   "metadata": {},
   "source": [
    "model.eval()\n",
    "i = 13\n",
    "inputs = tokenizer(dataset[\"validation\"][text_column][i], return_tensors=\"pt\")\n",
    "print(dataset[\"validation\"][text_column][i])\n",
    "print(inputs)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(model.device), max_new_tokens=10)\n",
    "    print(outputs)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
