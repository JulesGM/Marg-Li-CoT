{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbr9iYfRy4GZ"
      },
      "source": [
        "# HuggingFace meets `bitsandbytes` for lighter models on GPU for inference\n",
        "\n",
        "## Running T5-11b on Google Colab "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32ShdTnhMoKV"
      },
      "source": [
        " <center>\n",
        " <img src=\"https://s3.amazonaws.com/moonup/production/uploads/1659861207959-62441d1d9fdefb55a0b7d12c.png\">\n",
        " </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9uJn2NczFBK"
      },
      "source": [
        "\n",
        "You can run your own 8-bit model on any HuggingFace ðŸ¤— model with just few lines of code. This notebook shows how to do it with a `T5` model that would usually require 12GB of GPU RAM.\n",
        "Install the dependencies below first!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aep1KMF6dqdm",
        "outputId": "11ddcc2d-245e-469c-bc9f-b45999ad4dbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "trlx 0.5.0 requires tritonclient, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "trlx 0.5.0 requires tritonclient, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --quiet bitsandbytes\n",
        "!pip install --quiet --upgrade transformers # Install latest version of transformers\n",
        "!pip install --quiet --upgrade accelerate\n",
        "!pip install --quiet sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6YTOisTFDsR",
        "outputId": "101024e6-60bd-4b7a-8aaa-ee1b8331c78f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.8/dist-packages (0.35.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.8/dist-packages (0.15.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (1.13.0+cu116)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate) (5.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymZwkJUenuV1"
      },
      "source": [
        "## Choose your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3cMSVC-oCyM"
      },
      "source": [
        "Rerun this cell if you want to change the model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HibeFxJnwq7"
      },
      "source": [
        "model_name = \"t5-3b-sharded\" #@param [\"t5-11b-sharded\", \"t5-3b-sharded\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BejbQStU5Eik"
      },
      "source": [
        "## Use 8bit models with `t5-3b-sharded` ðŸ¤—"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YJlldexxwnhM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "CUDA SETUP: CUDA runtime path found: /home/mila/g/gagnonju/.anaconda3/lib/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 117\n",
            "CUDA SETUP: Loading binary /home/mila/g/gagnonju/.anaconda3/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc23141aa1484d3b9ab3b6a23d07ed28",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# T5-3b and T5-11B are supported!\n",
        "# We need sharded weights otherwise we get CPU OOM errors\n",
        "model_id=f\"google/flan-t5-xxl\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model_8bit = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    model_id, \n",
        "    device_map=\"auto\", \n",
        "    load_in_8bit=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtvnO-jtM1is"
      },
      "source": [
        "Let's check the memory footprint of this model! ðŸª¶"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDgDNAmYM1Lx",
        "outputId": "3947d017-06b9-44fa-bb90-bab4db5d5cb9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_8bit.get_memory_footprint() // 1000 ** 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsuYzVoBPMtn"
      },
      "source": [
        "For `t5-3b` the int8 model is about ~2.9GB! whereas the original model has 11GB. For `t5-11b` the int8 model is about ~11GB vs 42GB for the original model.\n",
        "Now let's generate and see the qualitative results of the 8bit model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "j1s0spY4icGK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, mon nom est Younes et je suis un ingÃ©nieur de apprentissage d'applications Ã  Hugging Face.\n"
          ]
        }
      ],
      "source": [
        "max_new_tokens = 50\n",
        "\n",
        "input_ids = tokenizer(\n",
        "    \"translate English to French: Hello my name is Younes \"\n",
        "    \"and I am a Machine Learning Engineer at Hugging Face\", \n",
        "    return_tensors=\"pt\",\n",
        ").input_ids  \n",
        "\n",
        "outputs = model_8bit.generate(\n",
        "    input_ids.to(model_8bit.device), max_new_tokens=max_new_tokens\n",
        ")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
