#abstract #chains-of-thoughts #scratchpads #NLP #iclr

Using Prediction Quality as Reward for Scratchpad Fine-tuning

It is often easier to think of the next steps of a problem than to think of the final solution right away. Scratchpads and chains-of-thought, the fact of learning to decode the steps of a problem with a language model instead of trying to decode the solution write away, have been shown to help massively in problems with multiple discrete reasoning steps such as high school mathematical textual problems, multiplying the accuracy score multiple times in the case of GSM8K, a very hard highschool mathematical dataset. The next steps predicted can be wrong however, and, as confirmed in this paper and elsewhere, this hurts performance. The approach is currently to use a very large LM and to show it a few examples of questions and answers with explanations (chains-of-thoughts) in its context. However, what if we wanted to improve on that? Generating more scratchpads is often long and expensive. In this paper, we adress the setting where we have inputs and labels (such as questions and answers), but we only have very few scratchpad annotations (16 or fewer), and show that one can use the model's confidence on the correct answer as a reward signal to improve the quality of the scratchpads that are generated, and thus improve the final overall performance of the model. We explore different approaches to use this reward signal, and end up with a method that improves the performance of the model on a number of datasets including the very hard GSM8K dataset, and show that the scratchpads generated are indeed better.