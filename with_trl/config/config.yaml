defaults:
  - _self_
  - model: mistral_instruct
  - peft_config: default
  - train_generation_kwargs: default
  - eval_generation_kwargs: default
  - ppo_config: default
  - acc_maintain: default
  - curriculum_schedule: ???
  - experiment: ???

# Dataset options. Limits the lengths of the different parts of the dataset.
tok_max_query_length: null
tok_max_answer_length: null
tok_max_total_length: null

batch_size: ???
mini_batch_size: ???

wandb_project: rl_${dataset_name}
start_eval: True # Whether to start evaluation right away.
use_few_shots: ??? # This depends on whether we are using a fine-tuned model or not
few_shot_qty: 5

inspect_indices: false # Whether to use synchroneous dataloading & print the indices of the elements being loaded. For debugging purposes.
eval_subset_size: ???

use_peft: True
max_epochs: 10
no_training: False

answer_only: False # Whether to train on generating the answer only.
answer_only_path: null 
value_pretrain_epochs: null # Number of epochs to pretrain the value model for. I'm not sure if this actually works.
value_pretrain_steps: null # Number of steps to pretrain the value model for. I'm not sure if this actually works.

precision: bfloat16 # float32 # torch dtype passed to the load_pretrained call.
float32_precision_generation: "highest"
float32_precision_forward_backward: "highest"

disable_adapter_value_pretrain: false
peft_do_all_lin_layers: False # Unlikely that we need to change this.
reward_type: exact_match # Should never change
arithmetic_dataset_root_folder_dir: "/home/mila/g/gagnonju/marglicot/mlc_datasets/arithmetic/"

just_start_metrics: True