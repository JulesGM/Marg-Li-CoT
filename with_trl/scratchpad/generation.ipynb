{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import enum\n",
    "import time\n",
    "import typing\n",
    "\n",
    "\n",
    "import rich\n",
    "import rich.console\n",
    "import rich.table\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "\n",
    "################################################################################################\n",
    "# Generic Utils\n",
    "################################################################################################\n",
    "\n",
    "\n",
    "def add_check_not_exists(dict_, key, value):\n",
    "    assert key not in dict_, f\"Key already exists: {key}\"\n",
    "    dict_[key] = value\n",
    "\n",
    "\n",
    "################################################################################################\n",
    "# Generic HF Utils\n",
    "################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "class Precision(str, enum.Enum):\n",
    "    FP32  = \"fp32\"\n",
    "    FP16  = \"fp16\"\n",
    "    BF16  = \"bf16\"\n",
    "    BITS8 = \"bits8\"\n",
    "    BITS4 = \"bits4\"\n",
    "\n",
    "\n",
    "def create_kwargs(precision, device_map):\n",
    "    if precision == Precision.BITS4:\n",
    "        kwargs = dict(\n",
    "            load_in_4bit=True,\n",
    "        )\n",
    "    elif precision == Precision.BITS8:\n",
    "        kwargs = dict(\n",
    "            load_in_8bit = True, \n",
    "            torch_dtype  = torch.float16,\n",
    "        )\n",
    "    elif precision == Precision.FP16:\n",
    "        kwargs = dict(torch_dtype = torch.float16)\n",
    "    elif precision == Precision.BF16:\n",
    "        kwargs = dict(torch_dtype = torch.bfloat16)\n",
    "    elif precision == Precision.FP32:\n",
    "        kwargs = dict(torch_dtype = torch.float32)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid precision: {precision}\")\n",
    "    \n",
    "    assert \"device_map\" not in kwargs\n",
    "    kwargs[\"device_map\"] = device_map\n",
    "\n",
    "    return kwargs\n",
    "\n",
    "\n",
    "def build_device_map(*, use_device_map, gpu_index, config):\n",
    "    if use_device_map:\n",
    "        if gpu_index is None:\n",
    "            device_map = \"balanced\"\n",
    "        else:\n",
    "            device_map = {\"\": torch.device(gpu_index)}\n",
    "    else:\n",
    "        device_map = None\n",
    "\n",
    "    return device_map\n",
    "\n",
    "\n",
    "def make_model(*, config, model_name, kwargs, gpu_index, device_map):\n",
    "    if config.is_encoder_decoder:\n",
    "        cls = transformers.AutoModelForSeq2SeqLM\n",
    "    else:\n",
    "        cls = transformers.AutoModelForCausalLM\n",
    "\n",
    "    model = cls.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code = True,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    if device_map is None:\n",
    "        model.to(torch.device(gpu_index))\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_model(\n",
    "    *, \n",
    "    config_name: str,\n",
    "    model_name: str, \n",
    "    precision:  Precision, \n",
    "    gpu_index:  typing.Optional[int],\n",
    "    use_device_map: bool,\n",
    "):\n",
    "    rich.print(f\"Loading model: \\\"{model_name}\\\" - \\\"{precision}\\\"\")\n",
    "    if precision in [Precision.BITS4, Precision.BITS8]:\n",
    "        assert use_device_map\n",
    "\n",
    "    precision = Precision(precision)\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "    config    = transformers.AutoConfig   .from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "    device_map = build_device_map(use_device_map=use_device_map, gpu_index=gpu_index, config=config,)\n",
    "    kwargs     = create_kwargs   (precision=precision, device_map=device_map)\n",
    "    model      = make_model      (config=config, model_name=model_name, kwargs=kwargs, gpu_index=gpu_index, device_map=device_map)\n",
    "\n",
    "    print_table(kwargs=kwargs, model=model, config_name=config_name)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "################################################################################################\n",
    "# Project specific\n",
    "################################################################################################\n",
    "def print_table(*, kwargs, model, config_name):\n",
    "    table = rich.table.Table(\"Key\", \"Value\", title=f\"[green]{config_name}\", show_lines=True)\n",
    "    table.add_row(\"Model type\",               model.config.model_type)\n",
    "    table.add_row(\"Loading model with kwargs\", str(kwargs))\n",
    "    table.add_row(\"Device indices\",            str(\n",
    "        collections.Counter(\n",
    "            int(x.device.index) \n",
    "            if x.device.index is not None \n",
    "            else x.device.index \n",
    "            for x in model.parameters()\n",
    "        )))\n",
    "    table.add_row(\"Device types\",              str(collections.Counter(\n",
    "        x.device.type for x in model.parameters())))\n",
    "    rich.print(table)\n",
    "\n",
    "\n",
    "\n",
    "def text_generation(*, s, m, t: transformers.PreTrainedTokenizerBase):\n",
    "    conditional_gen_kwargs = {}\n",
    "    \n",
    "    ###########################\n",
    "    # Pad token id stuff\n",
    "    ###########################\n",
    "    if t.pad_token_id is None:\n",
    "        t.pad_token_id = t.eos_token_id\n",
    "\n",
    "    if m.config.pad_token_id is None:\n",
    "        conditional_gen_kwargs[\"pad_token_id\"] = m.config.eos_token_id\n",
    "\n",
    "    ###########################\n",
    "    # Padding side stuff\n",
    "    ###########################\n",
    "    if not m.config.is_encoder_decoder:\n",
    "        assert hasattr(t       , \"padding_side\"), \"hasattr(t       , 'padding_side') is False\"\n",
    "        t.padding_side = \"left\"\n",
    "\n",
    "    ###########################\n",
    "    # Generation\n",
    "    ###########################\n",
    "    sample_toks = t(s, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    output_toks = m.generate(\n",
    "        input_ids=sample_toks.input_ids,\n",
    "        num_return_sequences = 1, \n",
    "        max_new_tokens       = 200,\n",
    "        num_beams            = 1,\n",
    "        do_sample            = False,\n",
    "        **conditional_gen_kwargs,\n",
    "    )\n",
    "    end = time.perf_counter()\n",
    "    print(f\"\\t- {end - start:.2f} seconds\")\n",
    "    return start - end\n",
    "\n",
    "\n",
    "def print_generations(tokenizer, model, output_toks, sample_toks):\n",
    "    if not model.config.is_encoder_decoder:\n",
    "        output_toks = output_toks[:, sample_toks.input_ids.shape[-1]:]\n",
    "    outputs = tokenizer.batch_decode(output_toks, skip_special_tokens=True)\n",
    "    outputs = [x.replace(\"\\n\", \" \").strip() for x in outputs]\n",
    "\n",
    "    print(f\"\\n{tokenizer.decode(sample_toks.input_ids)}\")\n",
    "    for line in outputs:\n",
    "        line = line.replace(\"  \", \" \")\n",
    "        if line:\n",
    "            print(f\" - {line.strip()}\")\n",
    "\n",
    "\n",
    "def build_configs(*, model_name, big):\n",
    "    configs = {}\n",
    "\n",
    "    if not big:\n",
    "        add_check_not_exists(\n",
    "            configs,\n",
    "            \"m_fp_16_normal\",\n",
    "            dict(\n",
    "                model_name = model_name,\n",
    "                precision  = Precision.BF16, \n",
    "                gpu_index  = 0,\n",
    "                use_device_map = False,\n",
    "            )\n",
    "        )\n",
    "        add_check_not_exists(\n",
    "            configs,\n",
    "            \"m_fp_16_dm_single\",\n",
    "            dict(\n",
    "                model_name = model_name,\n",
    "                precision  = Precision.BF16, \n",
    "                gpu_index  = 0,\n",
    "                use_device_map = True,\n",
    "            ))\n",
    "        add_check_not_exists(\n",
    "            configs,\n",
    "            \"m_b_8_dm_single\",\n",
    "            dict(\n",
    "                model_name = model_name,\n",
    "                precision  = Precision.BITS8, \n",
    "                gpu_index  = 0,\n",
    "                use_device_map = True,\n",
    "            ))\n",
    "        \n",
    "        # add_check_not_exists(\n",
    "        # configs,\n",
    "        # \"m_b_4_dm_single\",\n",
    "        # dict(\n",
    "        #     model_name = model_name,\n",
    "        #     precision  = Precision.BITS4,\n",
    "        #     gpu_index  = 0,\n",
    "        #     use_device_map = True,\n",
    "        # ))\n",
    "\n",
    "    add_check_not_exists(\n",
    "        configs,\n",
    "        \"m_fp_16_dm_auto\",\n",
    "        dict(\n",
    "            model_name = model_name,\n",
    "            precision  = Precision.BF16, \n",
    "            gpu_index  = None,\n",
    "            use_device_map = True,\n",
    "        ))\n",
    "\n",
    "    add_check_not_exists(\n",
    "        configs,\n",
    "        \"m_b_8_dm_auto\", \n",
    "        dict(\n",
    "            model_name = model_name,\n",
    "            precision  = Precision.BITS8, \n",
    "            gpu_index  = None,\n",
    "            use_device_map = True,\n",
    "        ))\n",
    "\n",
    "    # add_check_not_exists(\n",
    "    #     configs,\n",
    "    #     \"m_b_4_dm_auto\", \n",
    "    #     dict(\n",
    "    #         model_name = model_name,\n",
    "    #         precision  = Precision.BITS4,\n",
    "    #         gpu_index  = None,\n",
    "    #         use_device_map = True,\n",
    "    #     ))\n",
    "\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"huggyllama/llama-7b\"\n",
    "BIG = False\n",
    "# MODEL_NAME = \"tiiuae/falcon-40b-instruct\"\n",
    "# MODEL_NAME     = \"google/flan-t5-xl\"\n",
    "###############\n",
    "\n",
    "N_LOOPS    = 5\n",
    "BATCH_SIZE = 8 * 8\n",
    "PRECISION  = Precision.BF16\n",
    "SAMPLE = [\n",
    "    \"Isabella earns $5 an hour babysitting. \"\n",
    "    \"She babysits 5 hours every day, 6 afternoons a week. \"\n",
    "    \"After babysitting for 7 weeks, how much money \"\n",
    "    \"will Isabella have earned?\"\n",
    "] * BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">────────────────────────────────────── </span>m_fp_16_normal<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m────────────────────────────────────── \u001b[0mm_fp_16_normal\u001b[92m ───────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading model: <span style=\"color: #008000; text-decoration-color: #008000\">\"huggyllama/llama-7b\"</span> - <span style=\"color: #008000; text-decoration-color: #008000\">\"bf16\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading model: \u001b[32m\"huggyllama/llama-7b\"\u001b[0m - \u001b[32m\"bf16\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b5e5fcba3b4b9faf794fa94f0e4ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                  </span><span style=\"color: #008000; text-decoration-color: #008000; font-style: italic\">m_fp_16_normal</span><span style=\"font-style: italic\">                                   </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Key                       </span>┃<span style=\"font-weight: bold\"> Value                                               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Model type                │ llama                                               │\n",
       "├───────────────────────────┼─────────────────────────────────────────────────────┤\n",
       "│ Loading model with kwargs │ {'torch_dtype': torch.bfloat16, 'device_map': None} │\n",
       "├───────────────────────────┼─────────────────────────────────────────────────────┤\n",
       "│ Device indices            │ Counter({0: 291})                                   │\n",
       "├───────────────────────────┼─────────────────────────────────────────────────────┤\n",
       "│ Device types              │ Counter({'cuda': 291})                              │\n",
       "└───────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                  \u001b[0m\u001b[3;32mm_fp_16_normal\u001b[0m\u001b[3m                                   \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mKey                      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mValue                                              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Model type                │ llama                                               │\n",
       "├───────────────────────────┼─────────────────────────────────────────────────────┤\n",
       "│ Loading model with kwargs │ {'torch_dtype': torch.bfloat16, 'device_map': None} │\n",
       "├───────────────────────────┼─────────────────────────────────────────────────────┤\n",
       "│ Device indices            │ Counter({0: 291})                                   │\n",
       "├───────────────────────────┼─────────────────────────────────────────────────────┤\n",
       "│ Device types              │ Counter({'cuda': 291})                              │\n",
       "└───────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- 21.53 seconds\n",
      "\t- 8.84 seconds\n",
      "\t- 8.82 seconds\n",
      "\t- 8.84 seconds\n",
      "\t- 8.86 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">───────────────────────────────────── </span>m_fp_16_dm_single<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ─────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m───────────────────────────────────── \u001b[0mm_fp_16_dm_single\u001b[92m ─────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading model: <span style=\"color: #008000; text-decoration-color: #008000\">\"huggyllama/llama-7b\"</span> - <span style=\"color: #008000; text-decoration-color: #008000\">\"bf16\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading model: \u001b[32m\"huggyllama/llama-7b\"\u001b[0m - \u001b[32m\"bf16\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515c994105b84fc89b247627ea4dc2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                      </span><span style=\"color: #008000; text-decoration-color: #008000; font-style: italic\">m_fp_16_dm_single</span><span style=\"font-style: italic\">                                      </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Key                       </span>┃<span style=\"font-weight: bold\"> Value                                                         </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Model type                │ llama                                                         │\n",
       "├───────────────────────────┼───────────────────────────────────────────────────────────────┤\n",
       "│ Loading model with kwargs │ {'torch_dtype': torch.bfloat16, 'device_map': {'':            │\n",
       "│                           │ device(type='cuda', index=0)}}                                │\n",
       "├───────────────────────────┼───────────────────────────────────────────────────────────────┤\n",
       "│ Device indices            │ Counter({0: 291})                                             │\n",
       "├───────────────────────────┼───────────────────────────────────────────────────────────────┤\n",
       "│ Device types              │ Counter({'cuda': 291})                                        │\n",
       "└───────────────────────────┴───────────────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                      \u001b[0m\u001b[3;32mm_fp_16_dm_single\u001b[0m\u001b[3m                                      \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mKey                      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mValue                                                        \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Model type                │ llama                                                         │\n",
       "├───────────────────────────┼───────────────────────────────────────────────────────────────┤\n",
       "│ Loading model with kwargs │ {'torch_dtype': torch.bfloat16, 'device_map': {'':            │\n",
       "│                           │ device(type='cuda', index=0)}}                                │\n",
       "├───────────────────────────┼───────────────────────────────────────────────────────────────┤\n",
       "│ Device indices            │ Counter({0: 291})                                             │\n",
       "├───────────────────────────┼───────────────────────────────────────────────────────────────┤\n",
       "│ Device types              │ Counter({'cuda': 291})                                        │\n",
       "└───────────────────────────┴───────────────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- 10.26 seconds\n",
      "\t- 10.28 seconds\n",
      "\t- 10.31 seconds\n",
      "\t- 10.32 seconds\n",
      "\t- 10.27 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">────────────────────────────────────── </span>m_b_8_dm_single<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ──────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m────────────────────────────────────── \u001b[0mm_b_8_dm_single\u001b[92m ──────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading model: <span style=\"color: #008000; text-decoration-color: #008000\">\"huggyllama/llama-7b\"</span> - <span style=\"color: #008000; text-decoration-color: #008000\">\"bits8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading model: \u001b[32m\"huggyllama/llama-7b\"\u001b[0m - \u001b[32m\"bits8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/mila/g/gagnonju/.main/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /cvmfs/ai.mila.quebec/apps/arch/common/cuda/11.7/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/mila/g/gagnonju/.main/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/g/gagnonju/.main/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/mila/g/gagnonju/local_cudnn/cudnn-linux-x86_64-8.5.0.96_cuda11-archive/lib')}\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3fe4d83da441f3aee16e47bf41cce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                       </span><span style=\"color: #008000; text-decoration-color: #008000; font-style: italic\">m_b_8_dm_single</span><span style=\"font-style: italic\">                                       </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Key                       </span>┃<span style=\"font-weight: bold\"> Value                                                         </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Model type                │ llama                                                         │\n",
       "├───────────────────────────┼───────────────────────────────────────────────────────────────┤\n",
       "│ Loading model with kwargs │ {'load_in_8bit': True, 'torch_dtype': torch.float16,          │\n",
       "│                           │ 'device_map': {'': device(type='cuda', index=0)}}             │\n",
       "├───────────────────────────┼───────────────────────────────────────────────────────────────┤\n",
       "│ Device indices            │ Counter({0: 291})                                             │\n",
       "├───────────────────────────┼───────────────────────────────────────────────────────────────┤\n",
       "│ Device types              │ Counter({'cuda': 291})                                        │\n",
       "└───────────────────────────┴───────────────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                       \u001b[0m\u001b[3;32mm_b_8_dm_single\u001b[0m\u001b[3m                                       \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mKey                      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mValue                                                        \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Model type                │ llama                                                         │\n",
       "├───────────────────────────┼───────────────────────────────────────────────────────────────┤\n",
       "│ Loading model with kwargs │ {'load_in_8bit': True, 'torch_dtype': torch.float16,          │\n",
       "│                           │ 'device_map': {'': device(type='cuda', index=0)}}             │\n",
       "├───────────────────────────┼───────────────────────────────────────────────────────────────┤\n",
       "│ Device indices            │ Counter({0: 291})                                             │\n",
       "├───────────────────────────┼───────────────────────────────────────────────────────────────┤\n",
       "│ Device types              │ Counter({'cuda': 291})                                        │\n",
       "└───────────────────────────┴───────────────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- 28.56 seconds\n",
      "\t- 28.28 seconds\n",
      "\t- 28.36 seconds\n",
      "\t- 28.40 seconds\n",
      "\t- 28.33 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">────────────────────────────────────── </span>m_fp_16_dm_auto<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ──────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m────────────────────────────────────── \u001b[0mm_fp_16_dm_auto\u001b[92m ──────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading model: <span style=\"color: #008000; text-decoration-color: #008000\">\"huggyllama/llama-7b\"</span> - <span style=\"color: #008000; text-decoration-color: #008000\">\"bf16\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading model: \u001b[32m\"huggyllama/llama-7b\"\u001b[0m - \u001b[32m\"bf16\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e563f1be8e4cc8be23bb55f11b9a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                     </span><span style=\"color: #008000; text-decoration-color: #008000; font-style: italic\">m_fp_16_dm_auto</span><span style=\"font-style: italic\">                                     </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Key                       </span>┃<span style=\"font-weight: bold\"> Value                                                     </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Model type                │ llama                                                     │\n",
       "├───────────────────────────┼───────────────────────────────────────────────────────────┤\n",
       "│ Loading model with kwargs │ {'torch_dtype': torch.bfloat16, 'device_map': 'balanced'} │\n",
       "├───────────────────────────┼───────────────────────────────────────────────────────────┤\n",
       "│ Device indices            │ Counter({0: 291})                                         │\n",
       "├───────────────────────────┼───────────────────────────────────────────────────────────┤\n",
       "│ Device types              │ Counter({'cuda': 291})                                    │\n",
       "└───────────────────────────┴───────────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                     \u001b[0m\u001b[3;32mm_fp_16_dm_auto\u001b[0m\u001b[3m                                     \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mKey                      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mValue                                                    \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Model type                │ llama                                                     │\n",
       "├───────────────────────────┼───────────────────────────────────────────────────────────┤\n",
       "│ Loading model with kwargs │ {'torch_dtype': torch.bfloat16, 'device_map': 'balanced'} │\n",
       "├───────────────────────────┼───────────────────────────────────────────────────────────┤\n",
       "│ Device indices            │ Counter({0: 291})                                         │\n",
       "├───────────────────────────┼───────────────────────────────────────────────────────────┤\n",
       "│ Device types              │ Counter({'cuda': 291})                                    │\n",
       "└───────────────────────────┴───────────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- 10.28 seconds\n",
      "\t- 10.33 seconds\n",
      "\t- 10.28 seconds\n",
      "\t- 10.29 seconds\n",
      "\t- 10.29 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────── </span>m_b_8_dm_auto<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────── \u001b[0mm_b_8_dm_auto\u001b[92m ───────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading model: <span style=\"color: #008000; text-decoration-color: #008000\">\"huggyllama/llama-7b\"</span> - <span style=\"color: #008000; text-decoration-color: #008000\">\"bits8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading model: \u001b[32m\"huggyllama/llama-7b\"\u001b[0m - \u001b[32m\"bits8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b31128a1394f4193834c98b97d16fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                        </span><span style=\"color: #008000; text-decoration-color: #008000; font-style: italic\">m_b_8_dm_auto</span><span style=\"font-style: italic\">                                        </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Key                       </span>┃<span style=\"font-weight: bold\"> Value                                                         </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Model type                │ llama                                                         │\n",
       "├───────────────────────────┼───────────────────────────────────────────────────────────────┤\n",
       "│ Loading model with kwargs │ {'load_in_8bit': True, 'torch_dtype': torch.float16,          │\n",
       "│                           │ 'device_map': 'balanced'}                                     │\n",
       "├───────────────────────────┼───────────────────────────────────────────────────────────────┤\n",
       "│ Device indices            │ Counter({0: 291})                                             │\n",
       "├───────────────────────────┼───────────────────────────────────────────────────────────────┤\n",
       "│ Device types              │ Counter({'cuda': 291})                                        │\n",
       "└───────────────────────────┴───────────────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                        \u001b[0m\u001b[3;32mm_b_8_dm_auto\u001b[0m\u001b[3m                                        \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mKey                      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mValue                                                        \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Model type                │ llama                                                         │\n",
       "├───────────────────────────┼───────────────────────────────────────────────────────────────┤\n",
       "│ Loading model with kwargs │ {'load_in_8bit': True, 'torch_dtype': torch.float16,          │\n",
       "│                           │ 'device_map': 'balanced'}                                     │\n",
       "├───────────────────────────┼───────────────────────────────────────────────────────────────┤\n",
       "│ Device indices            │ Counter({0: 291})                                             │\n",
       "├───────────────────────────┼───────────────────────────────────────────────────────────────┤\n",
       "│ Device types              │ Counter({'cuda': 291})                                        │\n",
       "└───────────────────────────┴───────────────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- 28.54 seconds\n",
      "\t- 28.49 seconds\n",
      "\t- 28.56 seconds\n",
      "\t- 28.28 seconds\n",
      "\t- 28.37 seconds\n"
     ]
    }
   ],
   "source": [
    "transformers.logging.set_verbosity_warning()\n",
    "CONSOLE = rich.console.Console()\n",
    "configs = build_configs(model_name=MODEL_NAME, big=BIG)\n",
    "\n",
    "for config_name, config in configs.items():\n",
    "    CONSOLE.rule()\n",
    "    CONSOLE.rule(config_name)\n",
    "    CONSOLE.rule()\n",
    "    model, tokenizer = load_model(config_name=config_name, **config)\n",
    "    for i in range(N_LOOPS):\n",
    "        text_generation(s=SAMPLE, m=model, t=tokenizer)\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Tue Jun  6 20:43:18 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100 80G...  On   | 00000000:43:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    71W / 300W |  53538MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     27331      C   ...gagnonju/.main/bin/python    53535MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
