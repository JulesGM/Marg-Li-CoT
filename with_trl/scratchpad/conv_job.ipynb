{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import torch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMES = {\"SAME\", \"VALID\"}\n",
    "\n",
    "def _apply_conv2d(section, filter):\n",
    "    output_conv2d = torch.conv2d(section, filter)\n",
    "    output_matmul = (section @ filter.transpose(0, 1)).unsqueeze(-1).unsqueeze(-1)\n",
    "    \n",
    "    assert torch.allclose(output_conv2d, output_matmul), (\n",
    "        output_conv2d.shape, output_matmul.shape)\n",
    "    \n",
    "    return section @ filter\n",
    "    \n",
    "\n",
    "def conv_2d(input, filter, stride, overflow_scheme=\"SAME\"):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    assert input.ndim == 4, input.shape\n",
    "    assert filter.ndim == 4, filter.shape\n",
    "    assert stride.ndim == 1, stride.shape\n",
    "    assert len(stride) == 2, stride.shape\n",
    "    assert isinstance(overflow_scheme, str), type(overflow_scheme).mro()\n",
    "    assert input.shape[3] == filter.shape[0], (filter.shape[3], filter.shape[0])\n",
    "    assert overflow_scheme in SCHEMES, (overflow_scheme, SCHEMES)\n",
    "\n",
    "    batch_size = input.shape[0]\n",
    "    x_size = input.shape[1]\n",
    "    y_size = input.shape[2]\n",
    "    x_stride = stride[0]\n",
    "    y_stride = stride[1]\n",
    "    x_filter_size = filter.shape[1]\n",
    "    y_filter_size = filter.shape[2]\n",
    "    num_channels = input.shape[3]\n",
    "    output_channels = filter.shape[4]\n",
    "\n",
    "    if overflow_scheme == \"SAME\":\n",
    "        output = np.empty((\n",
    "            math.ceil(x_size / x_stride), \n",
    "            math.ceil(y_size / y_stride), \n",
    "            output_channels,\n",
    "        ))\n",
    "    elif overflow_scheme == \"VALID\":\n",
    "        output = np.empty((\n",
    "            math.ceil(x_size / x_stride), \n",
    "            math.ceil(y_size / y_stride), \n",
    "            output_channels,\n",
    "        ))\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "\n",
    "    for b_idx in range(batch_size):\n",
    "        for x_idx in range(0, x_size, x_stride):\n",
    "            for y_idx in range(0, y_size, y_stride):\n",
    "                if overflow_scheme == \"SAME\":\n",
    "                    feature_left_boundary_x = x_idx - x_filter_size // 2\n",
    "                    feature_right_boundary_x = x_idx + x_filter_size // 2\n",
    "                    feature_left_boundary_y = y_idx - y_filter_size // 2\n",
    "                    feature_right_boundary_y = y_idx - y_filter_size // 2\n",
    "                \n",
    "                elif overflow_scheme == \"VALID\":\n",
    "                    feature_left_boundary_x = x_idx\n",
    "                    feature_right_boundary_x = x_idx + x_filter_size\n",
    "                    feature_left_boundary_y = y_idx\n",
    "                    feature_right_boundary_y = y_idx + y_filter_size\n",
    "                \n",
    "                else:\n",
    "                    raise ValueError(f\"`overflow_scheme` must be one of {SCHEMES}, got `{overflow_scheme}`\")\n",
    "\n",
    "\n",
    "                output[b_idx, x_idx:x_idx + x_filter_size, y_idx + y_filter_size] = _apply_conv2d(\n",
    "                    input[:, feature_left_boundary_x: feature_right_boundary_x, feature_left_boundary_y:feature_right_boundary_y], \n",
    "                    filter,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 3\n",
    "w = 5\n",
    "b = 1\n",
    "i = 4\n",
    "o = 8\n",
    "\n",
    "image = torch.rand(b, i, h, w)\n",
    "filter = torch.rand(o, i, h, w)\n",
    "\n",
    "output_conv2d = torch.conv2d(image, filter)\n",
    "\n",
    "image = image.reshape(b, -1)\n",
    "filter = filter.reshape(o, -1)\n",
    "\n",
    "###############################################################################\n",
    "output_einsum = torch.einsum(\"bz,oz->bo\", image, filter).unsqueeze(-1).unsqueeze(-1)\n",
    "print(f\"{torch.allclose(output_conv2d, output_einsum) = }\")\n",
    "\n",
    "###############################################################################\n",
    "output_matmul = (image @ filter.transpose(0, 1)).unsqueeze(-1).unsqueeze(-1)\n",
    "print(f\"{torch.allclose(output_conv2d, output_matmul) = }\")\n",
    "\n",
    "###############################################################################\n",
    "output_iter = torch.empty((b, o, 1, 1))\n",
    "for b_idx, o_idx in itertools.product(range(b), range(o)):\n",
    "    output_iter[b_idx, o_idx] = torch.dot(\n",
    "        image[b_idx].reshape(-1), \n",
    "        filter[o_idx].reshape(-1),\n",
    "    )\n",
    "\n",
    "print(f\"{torch.allclose(output_conv2d, output_iter) = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
