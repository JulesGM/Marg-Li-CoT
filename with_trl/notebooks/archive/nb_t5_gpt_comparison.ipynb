{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /cvmfs/ai.mila.quebec/apps/arch/common/cuda/11.7/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/mila/g/gagnonju/.main/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/g/gagnonju/.main/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/mila/g/gagnonju/local_cudnn/cudnn-linux-x86_64-8.5.0.96_cuda11-archive/lib')}\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "import os\n",
    "import peft\n",
    "import rich\n",
    "import torch\n",
    "import transformers\n",
    "import trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTypes(str, enum.Enum):\n",
    "    CAUSAL_LM = \"causal_lm\"\n",
    "    SEQ_2_SEQ_LM = \"seq_2_seq_lm\"\n",
    "\n",
    "class ModelTokenizerPair:\n",
    "    def __init__(\n",
    "        self, \n",
    "        hf_name=None, \n",
    "        model_cls=None, \n",
    "        default_gen_kwargs=None, \n",
    "        device=int(os.getenv(\"LOCAL_RANK\", \"0\")), \n",
    "        init_kwargs=None,\n",
    "    ):\n",
    "\n",
    "        if default_gen_kwargs is None:\n",
    "            default_gen_kwargs = dict(\n",
    "                max_new_tokens=100,\n",
    "            )\n",
    "        \n",
    "        self._init_kwargs = init_kwargs\n",
    "        self._peft_initialized = False\n",
    "        self._trl_initialized = False\n",
    "        self._default_gen_kwargs = default_gen_kwargs\n",
    "        self.device = device\n",
    "\n",
    "        if hf_name is not None:\n",
    "            self.model = model_cls.from_pretrained(hf_name, **init_kwargs)\n",
    "            self.tokenizer = transformers.AutoTokenizer.from_pretrained(hf_name)\n",
    "\n",
    "            if \"gpt\" in hf_name.lower():\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "                self._default_gen_kwargs[\"pad_token_id\"] = self.model.config.eos_token_id\n",
    "                self._model_type = ModelTypes.CAUSAL_LM\n",
    "\n",
    "            else:\n",
    "                assert \"t5\" in hf_name.lower()\n",
    "                self._model_type = ModelTypes.SEQ_2_SEQ_LM\n",
    "        \n",
    "        self.m = self.model\n",
    "        self.t = self.tokenizer\n",
    "        \n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        self.model.to(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def cuda(self, *args, **kwargs):\n",
    "        self.model.cuda(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def cpu(self, *args, **kwargs):\n",
    "        self.model.cpu(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def gen_from_text(self, text, gen_kwargs, to_text=True):\n",
    "        if gen_kwargs is None:\n",
    "            gen_kwargs = {}\n",
    "\n",
    "        tokenized = self.tokenizer(\n",
    "            text, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True,\n",
    "        ).to(self.device)\n",
    "        \n",
    "        output = self.generate(\n",
    "            **tokenized,\n",
    "            **gen_kwargs,\n",
    "        )\n",
    "        if self._model_type == ModelTypes.CAUSAL_LM:\n",
    "            output = output[:, tokenized[\"input_ids\"].shape[-1]:]\n",
    "        \n",
    "        if to_text:\n",
    "            return self.tokenizer.batch_decode(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def generate(self, *args, **gen_kwargs):\n",
    "        if gen_kwargs is None:\n",
    "            gen_kwargs = self._default_gen_kwargs\n",
    "        else:\n",
    "            gen_kwargs = self._default_gen_kwargs | gen_kwargs\n",
    "        return self.model.generate(*args, **gen_kwargs)\n",
    "\n",
    "    def text_to_text(self, text, gen_kwargs=None):\n",
    "        return self.gen_from_text(text, gen_kwargs, to_text=True)\n",
    "\n",
    "    def text_to_ids(self, text, gen_kwargs=None):\n",
    "        return self.gen_from_text(text, gen_kwargs, to_text=False)\n",
    "\n",
    "    def ids_to_text(self, model_inputs, gen_kwargs=None):\n",
    "        if gen_kwargs is None:\n",
    "            gen_kwargs = {}\n",
    "        generated = self.generate(**model_inputs, **gen_kwargs)\n",
    "        return self.tokenizer.batch_decode(generated)\n",
    "\n",
    "    def init_peft(self, peft_config):\n",
    "        self._peft_initialized = True\n",
    "        rich.print(\n",
    "            f\"[red bold]init_trl: \"\n",
    "            f\"{self._peft_initialized = } \"\n",
    "            f\"{self._trl_initialized = }\"\n",
    "        )\n",
    "        self.model = peft.get_peft_model(model=self.model, peft_config=peft_config)\n",
    "\n",
    "    def init_trl(self):\n",
    "        self._trl_initialized = True\n",
    "        rich.print(\n",
    "            f\"[red bold]init_trl: \"\n",
    "            f\"{self._peft_initialized = } \"\n",
    "            f\"{self._trl_initialized = }\"\n",
    "        )\n",
    "        if self._model_type == ModelTypes.CAUSAL_LM:\n",
    "            trl_cls = trl.models.AutoModelForCausalLMWithValueHead\n",
    "\n",
    "        elif self._model_type == ModelTypes.SEQ_2_SEQ_LM:\n",
    "            trl_cls = trl.models.AutoModelForSeq2SeqLMWithValueHead\n",
    "\n",
    "        self.model = trl_cls.from_pretrained(self.model)\n",
    "\n",
    "    def __call__(self, *args, **kwds) -> torch.Tensor:\n",
    "        return self.model(*args, **kwds)\n",
    "\n",
    "    def forward_from_text(self, text: str, decoder_text = None) -> torch.Tensor:\n",
    "        inputs = self.t(\n",
    "            text, \n",
    "            padding        = True, \n",
    "            truncation     = True,\n",
    "            return_tensors = \"pt\", \n",
    "        ).to(self.device)\n",
    "        \n",
    "        if not decoder_text is None:\n",
    "            assert self._model_type == ModelTypes.SEQ_2_SEQ_LM, self._model_type\n",
    "\n",
    "            decoder_inputs = self.t(\n",
    "                decoder_text,\n",
    "                padding        = True,\n",
    "                truncation     = True,\n",
    "                return_tensors = \"pt\",\n",
    "            ).to(self.device)\n",
    "\n",
    "            inputs = dict(\n",
    "                input_ids              = inputs[\"input_ids\"],\n",
    "                attention_mask         = inputs[\"attention_mask\"],\n",
    "                decoder_input_ids      = decoder_inputs[\"input_ids\"],\n",
    "                decoder_attention_mask = decoder_inputs[\"attention_mask\"],\n",
    "            )\n",
    "        else:\n",
    "            assert self._model_type == ModelTypes.CAUSAL_LM, self._model_type\n",
    "\n",
    "        for v in inputs.values():\n",
    "            assert v.device.type == \"cuda\", v.device\n",
    "            \n",
    "        return self.model(**inputs)\n",
    "    \n",
    "    def train(self, *args, **kwargs):\n",
    "        return self.model.train(*args, **kwargs)\n",
    "    \n",
    "    def training(self, *args, **kwargs):\n",
    "        return self.model.training(*args, **kwargs)\n",
    "    \n",
    "    def eval(self, *args, **kwargs):\n",
    "        return  self.model.eval(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE = torch.bfloat16\n",
    "\n",
    "if DTYPE in (torch.float16, torch.bfloat16):\n",
    "    init_kwargs = dict(\n",
    "        torch_dtype=DTYPE,\n",
    "    )\n",
    "elif DTYPE is None:\n",
    "    init_kwargs = {}\n",
    "else: \n",
    "    raise ValueError(f\"Invalid DTYPE: {DTYPE}\")\n",
    "\n",
    "\n",
    "t5  = ModelTokenizerPair(\n",
    "    hf_name=\"google/flan-t5-small\", \n",
    "    model_cls=transformers.AutoModelForSeq2SeqLM, \n",
    "    init_kwargs=init_kwargs,\n",
    ").cuda()\n",
    "\n",
    "gpt = ModelTokenizerPair(\n",
    "    hf_name=\"edbeeching/gpt-neo-125M-imdb-lora-adapter-merged\", \n",
    "    model_cls=transformers.AutoModelForCausalLM,\n",
    "    init_kwargs=init_kwargs,\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Nothing Applied</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mNothing Applied\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know.\"]\n",
      "['<pad> blue</s>']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">init_trl: self._peft_initialized = </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">True</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> self._trl_initialized = </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">False</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31minit_trl: self._peft_initialized = \u001b[0m\u001b[1;3;31mTrue\u001b[0m\u001b[1;31m self._trl_initialized = \u001b[0m\u001b[1;3;31mFalse\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">              </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold; font-style: italic\">INNERMOST PEFT-CONFIG:</span><span style=\"font-style: italic\">              </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Key                     </span>┃<span style=\"font-weight: bold\"> Value                </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000\">base_model_name_or_path</span> │ google/flan-t5-small │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000\">bias</span>                    │ none                 │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000\">enable_lora</span>             │ None                 │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000\">fan_in_fan_out</span>          │ False                │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000\">inference_mode</span>          │ False                │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000\">lora_alpha</span>              │ 32                   │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000\">lora_dropout</span>            │ 0.05                 │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000\">merge_weights</span>           │ False                │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000\">modules_to_save</span>         │ None                 │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000\">peft_type</span>               │ LORA                 │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000\">r</span>                       │ 16                   │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000\">target_modules</span>          │ ['q', 'v']           │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000\">task_type</span>               │ SEQ_2_SEQ_LM         │\n",
       "└─────────────────────────┴──────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m              \u001b[0m\u001b[1;3;34mINNERMOST PEFT-CONFIG:\u001b[0m\u001b[3m              \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mKey                    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mValue               \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ \u001b[32mbase_model_name_or_path\u001b[0m │ google/flan-t5-small │\n",
       "│ \u001b[32mbias\u001b[0m                    │ none                 │\n",
       "│ \u001b[32menable_lora\u001b[0m             │ None                 │\n",
       "│ \u001b[32mfan_in_fan_out\u001b[0m          │ False                │\n",
       "│ \u001b[32minference_mode\u001b[0m          │ False                │\n",
       "│ \u001b[32mlora_alpha\u001b[0m              │ 32                   │\n",
       "│ \u001b[32mlora_dropout\u001b[0m            │ 0.05                 │\n",
       "│ \u001b[32mmerge_weights\u001b[0m           │ False                │\n",
       "│ \u001b[32mmodules_to_save\u001b[0m         │ None                 │\n",
       "│ \u001b[32mpeft_type\u001b[0m               │ LORA                 │\n",
       "│ \u001b[32mr\u001b[0m                       │ 16                   │\n",
       "│ \u001b[32mtarget_modules\u001b[0m          │ ['q', 'v']           │\n",
       "│ \u001b[32mtask_type\u001b[0m               │ SEQ_2_SEQ_LM         │\n",
       "└─────────────────────────┴──────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">init_trl: self._peft_initialized = </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">True</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> self._trl_initialized = </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">False</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31minit_trl: self._peft_initialized = \u001b[0m\u001b[1;3;31mTrue\u001b[0m\u001b[1;31m self._trl_initialized = \u001b[0m\u001b[1;3;31mFalse\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                            </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold; font-style: italic\">INNERMOST PEFT-CONFIG:</span><span style=\"font-style: italic\">                            </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Key                     </span>┃<span style=\"font-weight: bold\"> Value                                            </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000\">base_model_name_or_path</span> │ edbeeching/gpt-neo-125M-imdb-lora-adapter-merged │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000\">bias</span>                    │ none                                             │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000\">enable_lora</span>             │ None                                             │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000\">fan_in_fan_out</span>          │ False                                            │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000\">inference_mode</span>          │ False                                            │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000\">lora_alpha</span>              │ 32                                               │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000\">lora_dropout</span>            │ 0.05                                             │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000\">merge_weights</span>           │ False                                            │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000\">modules_to_save</span>         │ None                                             │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000\">peft_type</span>               │ LORA                                             │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000\">r</span>                       │ 16                                               │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000\">target_modules</span>          │ ['q_proj', 'v_proj']                             │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000\">task_type</span>               │ CAUSAL_LM                                        │\n",
       "└─────────────────────────┴──────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                            \u001b[0m\u001b[1;3;34mINNERMOST PEFT-CONFIG:\u001b[0m\u001b[3m                            \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mKey                    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mValue                                           \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ \u001b[32mbase_model_name_or_path\u001b[0m │ edbeeching/gpt-neo-125M-imdb-lora-adapter-merged │\n",
       "│ \u001b[32mbias\u001b[0m                    │ none                                             │\n",
       "│ \u001b[32menable_lora\u001b[0m             │ None                                             │\n",
       "│ \u001b[32mfan_in_fan_out\u001b[0m          │ False                                            │\n",
       "│ \u001b[32minference_mode\u001b[0m          │ False                                            │\n",
       "│ \u001b[32mlora_alpha\u001b[0m              │ 32                                               │\n",
       "│ \u001b[32mlora_dropout\u001b[0m            │ 0.05                                             │\n",
       "│ \u001b[32mmerge_weights\u001b[0m           │ False                                            │\n",
       "│ \u001b[32mmodules_to_save\u001b[0m         │ None                                             │\n",
       "│ \u001b[32mpeft_type\u001b[0m               │ LORA                                             │\n",
       "│ \u001b[32mr\u001b[0m                       │ 16                                               │\n",
       "│ \u001b[32mtarget_modules\u001b[0m          │ ['q_proj', 'v_proj']                             │\n",
       "│ \u001b[32mtask_type\u001b[0m               │ CAUSAL_LM                                        │\n",
       "└─────────────────────────┴──────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Peft Applied</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mPeft Applied\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know.\"]\n",
      "['<pad> blue</s>']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">init_trl: self._peft_initialized = </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">True</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> self._trl_initialized = </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">True</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31minit_trl: self._peft_initialized = \u001b[0m\u001b[1;3;31mTrue\u001b[0m\u001b[1;31m self._trl_initialized = \u001b[0m\u001b[1;3;31mTrue\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">init_trl: self._peft_initialized = </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">True</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> self._trl_initialized = </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">True</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31minit_trl: self._peft_initialized = \u001b[0m\u001b[1;3;31mTrue\u001b[0m\u001b[1;31m self._trl_initialized = \u001b[0m\u001b[1;3;31mTrue\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Peft &amp; Trl Applied</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mPeft & Trl Applied\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_generated = [\" I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know.\"]\n",
      "t5_generated  = ['<pad> blue</s>']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Forward Pass</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mForward Pass\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">hidden_states      .device:</span>  cu<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">da:0</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">hidden_states      .dtype :</span>  torch.bfloat16\n",
       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">self.summary.weight.device:</span>  cu<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">da:0</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">self.summary.weight.dtype :</span>  torch.bfloat16\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mhidden_states      .device:\u001b[0m  cu\u001b[1;92mda:0\u001b[0m\n",
       "\u001b[1;34mhidden_states      .dtype :\u001b[0m  torch.bfloat16\n",
       "\u001b[1;34mself.summary.weight.device:\u001b[0m  cu\u001b[1;92mda:0\u001b[0m\n",
       "\u001b[1;34mself.summary.weight.dtype :\u001b[0m  torch.bfloat16\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">hidden_states      .device:</span>  cu<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">da:0</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">hidden_states      .dtype :</span>  torch.bfloat16\n",
       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">self.summary.weight.device:</span>  cu<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">da:0</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">self.summary.weight.dtype :</span>  torch.bfloat16\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mhidden_states      .device:\u001b[0m  cu\u001b[1;92mda:0\u001b[0m\n",
       "\u001b[1;34mhidden_states      .dtype :\u001b[0m  torch.bfloat16\n",
       "\u001b[1;34mself.summary.weight.device:\u001b[0m  cu\u001b[1;92mda:0\u001b[0m\n",
       "\u001b[1;34mself.summary.weight.dtype :\u001b[0m  torch.bfloat16\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-43.0000,  -3.6406,  -9.0625,  ..., -43.0000, -43.0000, -43.0000],\n",
       "          [-32.0000,   5.6562,  -0.4766,  ..., -31.7500, -32.0000, -31.5000],\n",
       "          [-60.2500,  -6.2188,  -9.5625,  ..., -60.2500, -60.2500, -60.2500],\n",
       "          ...,\n",
       "          [-52.0000,   0.8984,  -4.6875,  ..., -52.0000, -52.2500, -51.7500],\n",
       "          [-56.5000,  -0.6602, -10.0000,  ..., -56.5000, -56.2500, -56.2500],\n",
       "          [-50.2500,   1.5234,  -8.8750,  ..., -50.0000, -50.2500, -49.7500]]],\n",
       "        device='cuda:0', grad_fn=<ToCopyBackward0>),\n",
       " None,\n",
       " tensor([[ 0.3145,  0.2656, -0.4961,  0.0092,  0.0928,  0.1562, -0.0118]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_message = \"What is the color of the sky?\"\n",
    "\n",
    "shared_peft_config = dict(\n",
    "    lora_dropout=0.05,\n",
    "    lora_alpha=32,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "############################################################\n",
    "############################################################\n",
    "\n",
    "causal_lm = peft.LoraConfig(\n",
    "    task_type=peft.TaskType.CAUSAL_LM,\n",
    "    **shared_peft_config,\n",
    ")\n",
    "\n",
    "seq2seq = peft.LoraConfig(\n",
    "    task_type=peft.TaskType.SEQ_2_SEQ_LM,\n",
    "    **shared_peft_config,\n",
    ")\n",
    "\n",
    "\n",
    "rich.print(\"[bold green]Nothing Applied\")\n",
    "print(gpt.text_to_text(shared_message))\n",
    "print(t5 .text_to_text(shared_message))\n",
    "\n",
    "t5 .init_peft(seq2seq)\n",
    "gpt.init_peft(causal_lm)\n",
    "\n",
    "\n",
    "t5 .to(t5.device)\n",
    "gpt.to(t5.device)\n",
    "\n",
    "if DTYPE in (torch.float16, torch.bfloat16):\n",
    "    t5 .to(DTYPE)\n",
    "    gpt.to(DTYPE)\n",
    "\n",
    "rich.print(\"[bold green]Peft Applied\")\n",
    "print(gpt.text_to_text(shared_message))\n",
    "print(t5 .text_to_text(shared_message))\n",
    "\n",
    "t5 .init_trl()\n",
    "t5.model.v_head.to(t5.device)\n",
    "gpt.init_trl()\n",
    "gpt.model.v_head.to(gpt.device)\n",
    "\n",
    "if DTYPE in (torch.float16, torch.bfloat16):\n",
    "    t5 .to(DTYPE)\n",
    "    gpt.to(DTYPE)\n",
    "\n",
    "rich.print(\"[bold green]Peft & Trl Applied\")\n",
    "gpt_generated = gpt.text_to_text(shared_message)\n",
    "print(f\"{gpt_generated = }\")\n",
    "t5_generated = t5.text_to_text(shared_message)\n",
    "print(f\"{t5_generated  = }\")\n",
    "\n",
    "rich.print(\"[bold green]Forward Pass\")\n",
    "gpt.train()\n",
    "gpt.forward_from_text(shared_message)\n",
    "t5 .train()\n",
    "t5 .forward_from_text(shared_message, t5.text_to_text(shared_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
