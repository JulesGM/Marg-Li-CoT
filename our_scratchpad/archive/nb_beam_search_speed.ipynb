{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import rich\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import general_utils as gu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36, 29, 36, 50, 40, 41, 48, 50, 30, 44, 50, 50, 35, 40, 42, 41, 39, 36, 37, 50, 43, 33, 45, 44, 50, 43, 50, 42, 39, 41, 44, 42, 32, 50, 41, 19]\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "data = \"\"\"\"The task of building general agents \n",
    "that perform well over a wide\n",
    " range of tasks has been an important \n",
    " goal in reinforcement learning since its inception. \n",
    "The problem has been subject of research\n",
    " of a large body of work, with performance \n",
    "frequently measured by observing scores over the\n",
    " wide range of environments contained in the Atari 57 benchmark. \n",
    "Agent57 was the first agent to \n",
    "surpass the human benchmark on all 57 games, \n",
    "but this came at the cost of poor data-efficiency, \n",
    "requiring nearly 80 billion frames of experience to achieve. \n",
    "Taking Agent57 as a starting point,\n",
    " we employ a diverse set of strategies to \n",
    "achieve a 200-fold reduction of experience \n",
    "needed to out perform the human baseline. \n",
    "We investigate a range of instabilities\n",
    " and bottlenecks we encountered while \n",
    "reducing the data regime, and propose\n",
    "effective solutions to build a more robust and efficient agent. \n",
    "We also demonstrate competitive performance\n",
    " with high-performing methods such\n",
    " as Muesli and MuZero. The four key components \n",
    " to our approach are (1) an approximate trust \n",
    "region method which enables stable bootstrapping from the online network, \n",
    "(2) a normalisation scheme for the loss and \n",
    "priorities which improves robustness when learning \n",
    "a set of value functions with a wide range\n",
    " of scales, (3) an improved architecture\n",
    " employing techniques from NFNets in order \n",
    " to leverage deeper networks without the need \n",
    "for normalization layers, and (4) a policy \n",
    "distillation method which serves\n",
    " to smooth out the instantaneous greedy policy overtime.\n",
    "https://doi.org/10.48550/arXiv.2209.07550\n",
    "Focus to learn more\"\"\".strip().split(\"\\n\")\n",
    "max_len = 50\n",
    "data = [x.strip()[:max_len] for x in data]\n",
    "print([len(x) for x in data])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"distilgpt2\", \"gpt2\", \"gpt2-large\"]\n",
    "models = {name: transformers.GPT2LMHeadModel.from_pretrained(name).to(\"cuda\") for name in model_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Key           </span>┃<span style=\"font-weight: bold\"> distilgpt2 </span>┃<span style=\"font-weight: bold\"> gpt2 </span>┃<span style=\"font-weight: bold\"> gpt2-large </span>┃\n",
       "┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━━━━━┩\n",
       "│ _name_or_path │ distilgpt2 │ gpt2 │ gpt2-large │\n",
       "│ n_head        │ 12         │ 12   │ 20         │\n",
       "│ n_layer       │ 6          │ 12   │ 36         │\n",
       "│ n_embd        │ 768        │ 768  │ 1280       │\n",
       "└───────────────┴────────────┴──────┴────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mKey          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mdistilgpt2\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mgpt2\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mgpt2-large\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━━━━━┩\n",
       "│ _name_or_path │ distilgpt2 │ gpt2 │ gpt2-large │\n",
       "│ n_head        │ 12         │ 12   │ 20         │\n",
       "│ n_layer       │ 6          │ 12   │ 36         │\n",
       "│ n_embd        │ 768        │ 768  │ 1280       │\n",
       "└───────────────┴────────────┴──────┴────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import rich.table as table\n",
    "\n",
    "table_ = table.Table(\"Key\", *model_names)\n",
    "all_keys = functools.reduce(lambda a, b: a | b, [vars(x.config).keys() for x in models.values()], set())\n",
    "to_ignore = {\"id2label\", \"label2id\", \"_num_labels\"}\n",
    "\n",
    "for k in all_keys - to_ignore:\n",
    "    \n",
    "    for model_name in models:\n",
    "        if k not in vars(models[model_name].config):\n",
    "            print(k, \"not in\", model_name)\n",
    "\n",
    "    if not all([\n",
    "        vars(models[model_name].config).get(k, object()) ==   # Iterating over the values of the models\n",
    "        vars(models[model_names[0]].config).get(k, object())  # Value for the first model\n",
    "        for model_name in models\n",
    "    ]):\n",
    "        table_.add_row(k, *[str(vars(models[model_name].config).get(k, \"<Not present>\")) for model_name in models])\n",
    "\n",
    "rich.print(table_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\", padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized = tokenizer(data, padding=True, return_tensors=\"pt\", truncation=True, max_length=max_len, add_special_tokens=False)\n",
    "tokenized = {k: (v.half() if v.dtype == torch.float else v).to(\"cuda\") for k, v in tokenized.items()}\n",
    "for model in models.values():\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([36, 20])\n",
      "attention_mask torch.Size([36, 20])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">generation with distilgpt2 and warmup_sampled</span> took <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.86029</span> seconds\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mgeneration with distilgpt2 and warmup_sampled\u001b[0m took \u001b[1;36m0.86029\u001b[0m seconds\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">generation with gpt2 and warmup_sampled</span> took <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.10851</span> seconds\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mgeneration with gpt2 and warmup_sampled\u001b[0m took \u001b[1;36m0.10851\u001b[0m seconds\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">generation with gpt2-large and warmup_sampled</span> took <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.34532</span> seconds\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mgeneration with gpt2-large and warmup_sampled\u001b[0m took \u001b[1;36m0.34532\u001b[0m seconds\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">generation with distilgpt2 and sampled</span> took <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.12457</span> seconds\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mgeneration with distilgpt2 and sampled\u001b[0m took \u001b[1;36m0.12457\u001b[0m seconds\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">generation with gpt2 and sampled</span> took <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.18161</span> seconds\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mgeneration with gpt2 and sampled\u001b[0m took \u001b[1;36m0.18161\u001b[0m seconds\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">generation with gpt2-large and sampled</span> took <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.85905</span> seconds\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mgeneration with gpt2-large and sampled\u001b[0m took \u001b[1;36m0.85905\u001b[0m seconds\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">generation with distilgpt2 and beam_search</span> took <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.26921</span> seconds\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mgeneration with distilgpt2 and beam_search\u001b[0m took \u001b[1;36m0.26921\u001b[0m seconds\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">generation with gpt2 and beam_search</span> took <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.33204</span> seconds\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mgeneration with gpt2 and beam_search\u001b[0m took \u001b[1;36m0.33204\u001b[0m seconds\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">generation with gpt2-large and beam_search</span> took <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.06966</span> seconds\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mgeneration with gpt2-large and beam_search\u001b[0m took \u001b[1;36m1.06966\u001b[0m seconds\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">generation with distilgpt2 and sampled_beam_search</span> took <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.15276</span> seconds\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mgeneration with distilgpt2 and sampled_beam_search\u001b[0m took \u001b[1;36m1.15276\u001b[0m seconds\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">generation with gpt2 and sampled_beam_search</span> took <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.38684</span> seconds\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mgeneration with gpt2 and sampled_beam_search\u001b[0m took \u001b[1;36m1.38684\u001b[0m seconds\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">generation with gpt2-large and sampled_beam_search</span> took <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.99596</span> seconds\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mgeneration with gpt2-large and sampled_beam_search\u001b[0m took \u001b[1;36m4.99596\u001b[0m seconds\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/g/gagnonju/.main/lib/python3.9/site-packages/transformers/generation_beam_search.py:197: UserWarning: Passing `max_length` to BeamSearchScorer is deprecated and has no effect. `max_length` should be passed directly to `beam_search(...)`, `beam_sample(...)`, or `group_beam_search(...)`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">generation with distilgpt2 and group_beam_search</span> took <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.85563</span> seconds\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mgeneration with distilgpt2 and group_beam_search\u001b[0m took \u001b[1;36m0.85563\u001b[0m seconds\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">generation with gpt2 and group_beam_search</span> took <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.84449</span> seconds\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mgeneration with gpt2 and group_beam_search\u001b[0m took \u001b[1;36m0.84449\u001b[0m seconds\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">generation with gpt2-large and group_beam_search</span> took <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.43745</span> seconds\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mgeneration with gpt2-large and group_beam_search\u001b[0m took \u001b[1;36m1.43745\u001b[0m seconds\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_beams = 5\n",
    "max_gen_len = 10\n",
    "\n",
    "configs = dict(\n",
    "    warmup_sampled=dict(\n",
    "        do_sample=True,\n",
    "    ),\n",
    "\n",
    "    sampled=dict(\n",
    "        do_sample=True,\n",
    "    ),\n",
    "\n",
    "    beam_search=dict(\n",
    "        do_sample=False,\n",
    "        num_beams=num_beams, \n",
    "        num_return_sequences=num_beams,\n",
    "    ),\n",
    "\n",
    "    sampled_beam_search=dict(\n",
    "        do_sample=True,\n",
    "        num_beams=num_beams,\n",
    "        num_return_sequences=num_beams,\n",
    "    ),\n",
    "\n",
    "    group_beam_search=dict(\n",
    "        do_sample=False,\n",
    "        num_beams=num_beams,\n",
    "        num_beam_groups=num_beams,\n",
    "        num_return_sequences=num_beams,\n",
    "        diversity_penalty=0.25,\n",
    "    ),\n",
    ")\n",
    "\n",
    "for k, v in tokenized.items():\n",
    "    print(k, v.shape)\n",
    "\n",
    "for config_name, kwargs in configs.items():\n",
    "    for name, model in models.items():\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            inputs = {k: v for k, v in tokenized.items()}\n",
    "            if config_name == \"sampled\":\n",
    "                inputs[\"input_ids\"]      = inputs[\"input_ids\"     ].repeat_interleave(num_beams, dim=0)\n",
    "                inputs[\"attention_mask\"] = inputs[\"attention_mask\"].repeat_interleave(num_beams, dim=0)\n",
    "\n",
    "            with gu.cuda_timeit(f\"generation with {name} and {config_name}\"):\n",
    "                output = model.generate(\n",
    "                    **inputs, \n",
    "                    **kwargs, \n",
    "                    cache=True, \n",
    "                    constraints=None, \n",
    "                    max_new_tokens=max_gen_len,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep 19 17:58:10 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:47:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    78W / 400W |  39302MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM...  On   | 00000000:BD:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    54W / 400W |      3MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   4075954      C   ...gagnonju/.main/bin/python    39299MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46daadc73974f0324ecc1592e5131128499dc93a3a1cbadf14a4773500af3ac4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
