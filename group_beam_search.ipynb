{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd50c390-b3b2-4144-a205-39a41f42b9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import openai\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers.generation_beam_search import BeamSearchScorer\n",
    "from transformers.generation_logits_process import (\n",
    "    LogitsProcessorList,\n",
    "    HammingDiversityLogitsProcessor,\n",
    "    MinLengthLogitsProcessor,\n",
    ")\n",
    "from transformers.generation_stopping_criteria import (\n",
    "    MaxLengthCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")\n",
    "\n",
    "from group_bs import group_beam_search, expand_inputs_for_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d27c01c-53df-4122-9b9a-7bfd8e625e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRATCH = os.environ['SCRATCH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f82f1ade-eb67-48aa-9b5b-da483a4601b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9be5ba6-44cd-4ffe-a8b1-e38b6b70eae6",
   "metadata": {},
   "source": [
    "#### Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a689115f-4891-428b-8c7a-fbc47a0bec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = os.path.join(SCRATCH, 'huggingface/gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e3f9e6-0b3a-4a63-8701-057adaf097b8",
   "metadata": {},
   "source": [
    "#### Test on One Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac821050-07b9-460d-9b32-9a58fc23cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = [\n",
    "\"\"\"Question: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "\n",
    "Let's think step by step.\n",
    "\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4271b04a-c90f-4c01-b7c1-bd50107bab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_prompt = tokenizer(prompt_text, padding='longest', add_special_tokens=False, return_tensors=\"pt\")\n",
    "encoded_prompt = encoded_prompt.to(device)\n",
    "\n",
    "input_ids = encoded_prompt['input_ids']\n",
    "attention_mask = encoded_prompt['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5415a100-aa7a-4750-b5b5-8994cf43955b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instantiate beam scorer\n",
    "NUM_BEAM, NUM_GROUP = 6, 6\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size=1,\n",
    "    num_beams=NUM_BEAM,\n",
    "    device=device,\n",
    "    num_beam_groups=6,\n",
    "    num_beam_hyps_to_keep=NUM_BEAM,\n",
    ")\n",
    "\n",
    "# instantiate logits processors\n",
    "logits_processor = LogitsProcessorList(\n",
    "    [\n",
    "        HammingDiversityLogitsProcessor(3.0, num_beams=NUM_BEAM, num_beam_groups=NUM_GROUP),\n",
    "        MinLengthLogitsProcessor(5, eos_token_id=tokenizer.eos_token_id),\n",
    "    ]\n",
    ")\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=30 + input_ids.shape[1])])\n",
    "\n",
    "input_ids_expanded, model_kwargs = expand_inputs_for_generation(\n",
    "    input_ids, expand_size=NUM_BEAM\n",
    ")\n",
    "with torch.no_grad():\n",
    "    outputs = group_beam_search(\n",
    "        tokenizer,\n",
    "        input_ids_expanded,\n",
    "        beam_scorer,\n",
    "        logits_processor=logits_processor,\n",
    "        stopping_criteria=stopping_criteria,\n",
    "        **model_kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ad0a624-dd64-4085-9991-89838f8a1533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Question: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\n\\nLet's think step by step.\\n\\n\\n\\n\\nThere are 3 cars in the parking lot.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\n",
       " \"Question: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\n\\nLet's think step by step.\\n\\n\\nThere are 3 cars in the parking lot.\\n\\n\\n2 more cars arrive.\\n\\n\\nNow, there are 5 cars in the parking\",\n",
       " \"Question: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\n\\nLet's think step by step.\\nThere are 3 cars in the parking lot. 2 more cars arrive. So now,\\n\\n\\nThere are 5 cars in the parking lot.\",\n",
       " \"Question: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\n\\nLet's think step by step.\\nFirst, we have three cars. Two more cars arrive, so now we have five cars in the parking lot.\",\n",
       " \"Question: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\n\\nLet's think step by step.\\nFirst, we have three cars. Two more cars arrive, so now we have five cars in the parking lot. Easy, right?\",\n",
       " \"Question: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\n\\nLet's think step by step.\\nThere are 3 cars in the parking lot. 2 more cars arrive. So now,\\n\\n\\nThere are 5 cars in the parking lot. 3\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
