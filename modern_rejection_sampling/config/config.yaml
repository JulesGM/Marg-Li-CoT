defaults:
  - dataset: gsm8k         # default dataset config (gsm8k or hendrycks_math)
  - experiment: gsm8k      # default experiment config

model:
  name: "HuggingFaceTB/SmolLM2-1.7B-Instruct"

training:
  max_length: 768
  batch_size: 12
  learning_rate: 3e-5
  num_epochs: 30

vllm_sampling:
  temperature: 1
  num_candidates: 48
  top_p: null            # if null, top_p will not be used

accelerate:
  seed: 42
  gpu_ids: [0]

vllm:
  gpu_id: 1

wandb:
  project: "expert_iteration"
  entity: "julesgm"
  log_interval: 10

evaluation:
  eval_batch_size: 1500
  eval_percentage: 0.05