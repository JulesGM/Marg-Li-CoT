defaults:
  - _self_
  - dataset: ???         # default dataset config (gsm8k or hendrycks_math)
  - experiment: ???      # default experiment config
  - override hydra/hydra_logging: disabled  
  - override hydra/job_logging: disabled  

model:
  name: "HuggingFaceTB/SmolLM2-1.7B-Instruct"

training:
  batch_size: 6
  learning_rate: 0.0001
  generation_max_length: 1024
  forward_max_length: 4092
  num_epochs: 100
  gradient_accumulation_steps: 1

vllm_sampling:
  temperature: 1
  num_candidates: 48
  top_p: null            # if null, top_p will not be used

accelerate:
  seed: 42
  gpu_ids: [0]

vllm:
  gpu_id: 1


wandb:
  project: "expert_iteration"
  entity: "julesgm"
  log_interval: 10

evaluation:
  eval_percentage: 0.05
  eval_batch_size: 1500
  eval_subset: null

output_dir: ???

master_port: 25123
internal_master_port: 25124


max_retries_vllm_crash: 5

few_shot_qty: ???


hydra:
  output_subdir: null